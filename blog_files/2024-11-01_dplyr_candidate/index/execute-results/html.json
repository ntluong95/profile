{
  "hash": "156f8c08c34816f634c3e7834c250c36",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Data frame wars: Choosing a Python dataframe library as a dplyr user\"\nexcerpt: \"A comparison of pandas, siuba, pydatatable, polars and duckdb from the perspective of a dplyr user\"\ndate: \"2021-12-20\"\ncategories: [\"R\", \"Python\"]\nimage: \"image.jpg\"\n---\n\n\n\n\n\nI'm a long time R user and lately I've seen more and more [signals](https://www.tiobe.com/tiobe-index/python/) that it's worth investing into Python. I use it for NLP with [spaCy](https://spacy.io) and to build functions on [AWS Lambda](https://aws.amazon.com/lambda/features/). Further, there are many more data API libraries and machine learning libraries for Python than for R.\n\n::: {.callout-note}\nThis article was written at the end of 2021 with the latest versions of the libraries and the number of Github stars at that time.\n:::\n\nAdopting Python means making choices on which libraries to invest time into learning. Manipulating data frames is one of the most common data science activities, so choosing the right library for it is key.\n\nMichael Chow, developer of [siuba](https://github.com/machow/siuba), a Python port of dplyr on top of pandas [wrote](https://mchow.com/posts/pandas-has-a-hard-job/) describes the situation well:\n\n> It seems like there’s been a lot of frustration surfacing on twitter lately from people coming from R—especially if they’ve used dplyr and ggplot—towards pandas and matplotlib. I can relate. I’m developing a port of dplyr to python. But in the end, it’s probably helpful to view these libraries as foundational to a lot of other, higher-level libraries (some of which will hopefully get things right for you!).\n\nThe higher-level libraries he mentions come with a problem : There's no universal standard.\n\nIn a discussion of the polars library on Hacker News the user \"civilized\" put the dplyr user perspective more bluntly:\n\n> In my world, anything that isn't \"identical to R's dplyr API but faster\" just isn't quite worth switching for. There's absolutely no contest: dplyr has the most productive API and that matters to me more than anything else.\n\nI'm more willing to compromise though, so here's a comparison of the strongest contenders.\n\n## The contenders\n\nThe [database-like ops benchmark on H2Oai](https://h2oai.github.io/db-benchmark/) is a helpful performance comparison.\n\nI'm considering these libraries:\n\n1. [Pandas](https://pandas.pydata.org): The most commonly used library and the one with the most tutorials and Stack Overflow answers available.\n2. [siuba](https://github.com/machow/siuba): A port of dplyr to Python, built on top of pandas. Not in the benchmark. Performance probably similar to pandas or worse due to translation.\n3. [Polars](https://www.pola.rs): The fastest library available. According to the benchmark, it runs 3-10x faster than Pandas.\n4. [Duckdb](https://www.pola.rs): Use an in-memory OLAP database instead of a dataframe and write SQL. In R, this can also be queried via dbplyr.\n5. [ibis](https://ibis-project.org/docs/index.html). Backend-agnostic wrapper for pandas and SQL engines.\n\nThere are more options. I excluded the others for these reasons:\n\n- Slower than polars and not with a readability focus (dask, Arrow, Modin, pydatatable)\n- Requires or is optmized for running on a remote server (Spark, ClickHouse and most other SQL databases).\n- Not meant for OLAP (sqlite)\n- Not in Python (DataFrames.jl)\n- Meant for GPU (cuDF)\n\n## Github stars as a proxy for popularity\n\nThe benchmark provides a comparison of performance, but another important factor is popularity and maturity. A more mature library has a more stable API, better test coverage and there is more help available online, such as on StackOverflow. One way to measure popularity is the number of stars that the package repository has on Github.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibs <- data.frame(\n    library = c(\"pandas\", \"siuba\", \"polars\", \"duckdb\", \"dplyr\", \"data.table\", \"pydatatable\", \"dtplyr\", \"tidytable\", \"ibis\"),\n    language = c(\"Python\", \"Python\", \"Python\", \"SQL\", \"R\", \"R\", \"Python\", \"R\", \"R\", \"Python\"),\n    stars = c(32100, 732, 3900, 4100, 3900, 2900, 1400, 542, 285, 1600)\n)\n\nggplot(libs, aes(x = reorder(library, -stars), y = stars, fill = language)) +\n    geom_col() +\n    labs(\n        title = \"Pandas is by far the most popular choice\",\n        subtitle = \"Comparison of Github stars on 2021-12-25\",\n        fill = \"Language\",\n        x = \"Library\",\n        y = \"Github stars\"\n    )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/github_stars-1.png){width=672}\n:::\n:::\n\n\n\n\n\nGithub stars are not a perfect proxy. For instance, dplyr is more mature than its star count suggests. Comparing the completeness of the documentation and tutorials for dplyr and polars reveals that it's a day and night difference.\n\nWith the quantitative comparison out of the way, here's a qualitative comparison of the Python packages. I'm speaking of my personal opinion of these packages - not a general comparison. My reference is my current use of [dplyr](https://dplyr.tidyverse.org) in R. When I need more performance, I use [tidytable](https://github.com/markfairbanks/tidytable) to get most of the speed of data.table with the grammar of dplyr and eager evaluation. Another alternative is [dtplyr](https://github.com/tidyverse/dtplyr), which translates dplyr to data.table with lazy evaluation. I also use [dbplyr](https://dbplyr.tidyverse.org), which translates dplyr to SQL. \n\nI'll compare the libraries by running a data transformation pipeline involving import from CSV, mutate, filter, sort, join, group by and summarize. I'll use the nycflights13 dataset, which is featured in Hadley Wickham's [R for Data Science](https://r4ds.had.co.nz/transform.html).\n\n## dplyr: Reference in R\n\nLet's start with a reference implementation in dplyr. The dataset is available as a package, so I skip the CSV import.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(nycflights13)\nlibrary(reactable)\n\n# Take a look at the tables\nreactable(head(flights, 10))\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"reactable html-widget html-fill-item\" id=\"htmlwidget-0e1501413d1bdc423ecc\" style=\"width:auto;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-0e1501413d1bdc423ecc\">{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"year\":[2013,2013,2013,2013,2013,2013,2013,2013,2013,2013],\"month\":[1,1,1,1,1,1,1,1,1,1],\"day\":[1,1,1,1,1,1,1,1,1,1],\"dep_time\":[517,533,542,544,554,554,555,557,557,558],\"sched_dep_time\":[515,529,540,545,600,558,600,600,600,600],\"dep_delay\":[2,4,2,-1,-6,-4,-5,-3,-3,-2],\"arr_time\":[830,850,923,1004,812,740,913,709,838,753],\"sched_arr_time\":[819,830,850,1022,837,728,854,723,846,745],\"arr_delay\":[11,20,33,-18,-25,12,19,-14,-8,8],\"carrier\":[\"UA\",\"UA\",\"AA\",\"B6\",\"DL\",\"UA\",\"B6\",\"EV\",\"B6\",\"AA\"],\"flight\":[1545,1714,1141,725,461,1696,507,5708,79,301],\"tailnum\":[\"N14228\",\"N24211\",\"N619AA\",\"N804JB\",\"N668DN\",\"N39463\",\"N516JB\",\"N829AS\",\"N593JB\",\"N3ALAA\"],\"origin\":[\"EWR\",\"LGA\",\"JFK\",\"JFK\",\"LGA\",\"EWR\",\"EWR\",\"LGA\",\"JFK\",\"LGA\"],\"dest\":[\"IAH\",\"IAH\",\"MIA\",\"BQN\",\"ATL\",\"ORD\",\"FLL\",\"IAD\",\"MCO\",\"ORD\"],\"air_time\":[227,227,160,183,116,150,158,53,140,138],\"distance\":[1400,1416,1089,1576,762,719,1065,229,944,733],\"hour\":[5,5,5,5,6,5,6,6,6,6],\"minute\":[15,29,40,45,0,58,0,0,0,0],\"time_hour\":[\"2013-01-01T10:00:00Z\",\"2013-01-01T10:00:00Z\",\"2013-01-01T10:00:00Z\",\"2013-01-01T10:00:00Z\",\"2013-01-01T11:00:00Z\",\"2013-01-01T10:00:00Z\",\"2013-01-01T11:00:00Z\",\"2013-01-01T11:00:00Z\",\"2013-01-01T11:00:00Z\",\"2013-01-01T11:00:00Z\"]},\"columns\":[{\"id\":\"year\",\"name\":\"year\",\"type\":\"numeric\"},{\"id\":\"month\",\"name\":\"month\",\"type\":\"numeric\"},{\"id\":\"day\",\"name\":\"day\",\"type\":\"numeric\"},{\"id\":\"dep_time\",\"name\":\"dep_time\",\"type\":\"numeric\"},{\"id\":\"sched_dep_time\",\"name\":\"sched_dep_time\",\"type\":\"numeric\"},{\"id\":\"dep_delay\",\"name\":\"dep_delay\",\"type\":\"numeric\"},{\"id\":\"arr_time\",\"name\":\"arr_time\",\"type\":\"numeric\"},{\"id\":\"sched_arr_time\",\"name\":\"sched_arr_time\",\"type\":\"numeric\"},{\"id\":\"arr_delay\",\"name\":\"arr_delay\",\"type\":\"numeric\"},{\"id\":\"carrier\",\"name\":\"carrier\",\"type\":\"character\"},{\"id\":\"flight\",\"name\":\"flight\",\"type\":\"numeric\"},{\"id\":\"tailnum\",\"name\":\"tailnum\",\"type\":\"character\"},{\"id\":\"origin\",\"name\":\"origin\",\"type\":\"character\"},{\"id\":\"dest\",\"name\":\"dest\",\"type\":\"character\"},{\"id\":\"air_time\",\"name\":\"air_time\",\"type\":\"numeric\"},{\"id\":\"distance\",\"name\":\"distance\",\"type\":\"numeric\"},{\"id\":\"hour\",\"name\":\"hour\",\"type\":\"numeric\"},{\"id\":\"minute\",\"name\":\"minute\",\"type\":\"numeric\"},{\"id\":\"time_hour\",\"name\":\"time_hour\",\"type\":\"Date\"}],\"dataKey\":\"efb7087d9e2b5bd5b3155062bf174300\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n\n```{.r .cell-code}\nreactable(head(airlines, 10))\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"reactable html-widget html-fill-item\" id=\"htmlwidget-b01b9f9a81e585d5f53a\" style=\"width:auto;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-b01b9f9a81e585d5f53a\">{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"carrier\":[\"9E\",\"AA\",\"AS\",\"B6\",\"DL\",\"EV\",\"F9\",\"FL\",\"HA\",\"MQ\"],\"name\":[\"Endeavor Air Inc.\",\"American Airlines Inc.\",\"Alaska Airlines Inc.\",\"JetBlue Airways\",\"Delta Air Lines Inc.\",\"ExpressJet Airlines Inc.\",\"Frontier Airlines Inc.\",\"AirTran Airways Corporation\",\"Hawaiian Airlines Inc.\",\"Envoy Air\"]},\"columns\":[{\"id\":\"carrier\",\"name\":\"carrier\",\"type\":\"character\"},{\"id\":\"name\",\"name\":\"name\",\"type\":\"character\"}],\"dataKey\":\"f306daa08a2136046ead72a665ae9011\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n\n\nThe `flights` tables has 336776 rows, one for each flight of an airplane. The `airlines` table has 16 rows, one for each airline mapping the full name of the company to a code.\n\nLet's find the airline with the highest arrival delays in January 2013. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights |>\n    filter(year == 2013, month == 1, !is.na(arr_delay)) |>\n    mutate(arr_delay = replace(arr_delay, arr_delay < 0, 0)) |>\n    left_join(airlines, by = \"carrier\") |>\n    group_by(airline = name) |>\n    summarise(flights = n(), mean_delay = mean(arr_delay)) |>\n    arrange(desc(mean_delay))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 16 × 3\n   airline                     flights mean_delay\n   <chr>                         <int>      <dbl>\n 1 SkyWest Airlines Inc.             1     107   \n 2 Hawaiian Airlines Inc.           31      48.8 \n 3 ExpressJet Airlines Inc.       3964      29.6 \n 4 Frontier Airlines Inc.           59      23.9 \n 5 Mesa Airlines Inc.               39      20.4 \n 6 Endeavor Air Inc.              1480      19.3 \n 7 Alaska Airlines Inc.             62      17.6 \n 8 Envoy Air                      2203      14.3 \n 9 Southwest Airlines Co.          985      13.0 \n10 JetBlue Airways                4413      12.9 \n11 United Air Lines Inc.          4590      11.9 \n12 American Airlines Inc.         2724      11.0 \n13 AirTran Airways Corporation     324       9.95\n14 US Airways Inc.                1554       9.11\n15 Delta Air Lines Inc.           3655       8.07\n16 Virgin America                  314       3.17\n```\n\n\n:::\n:::\n\n\n\n\n\nSome values in `arr_delay` are negative, indicating that the flight was faster than expected. I replaced these values with 0 because I don't want them to cancel out delays of other flights. I joined to the airlines table to get the full names of the airlines.\n\nI export the flights and airlines tables to CSV to hand them over to Python.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Write to temporary files\nflights_path <- tempfile(fileext = \".csv\")\nairlines_path <- tempfile(fileext = \".csv\")\n\ndata.table::fwrite(flights, flights_path, row.names = FALSE)\ndata.table::fwrite(airlines, airlines_path, row.names = FALSE)\n```\n:::\n\n\n\n\n\nTo access the file from Python, the path is handed over:\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\n#| eval: false\n# Hand over the path from R\nflights_path = r[\"flights_path\"]\nairlines_path = r[\"airlines_path\"]\n```\n:::\n\n\n\n\n\nFor more details on how this works with the reticulate package, check this [documentation](# See https://rstudio.github.io/reticulate/articles/calling_python.html).\n\n## Pandas: Most popular\n\nThe following sections follow a pattern: read in from CSV, then build a query.\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n\n# Import from CSV\nflights_pd = pd.read_csv(flights_path)\nairlines_pd = pd.read_csv(airlines_path)\n```\n:::\n\n\n\n\n\n`pandas.read_csv` reads the header and conveniently infers the column types.\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(\n    flights_pd.query(\"year == 2013 & month == 1 & arr_delay.notnull()\")\n    .assign(arr_delay=flights_pd.arr_delay.clip(lower=0))\n    .merge(airlines_pd, how=\"left\", on=\"carrier\")\n    .rename(columns={\"name\": \"airline\"})\n    .groupby(\"airline\")\n    .agg(flights=(\"airline\", \"count\"), mean_delay=(\"arr_delay\", \"mean\"))\n    .sort_values(by=\"mean_delay\", ascending=False)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                             flights  mean_delay\nairline                                         \nSkyWest Airlines Inc.              1  107.000000\nHawaiian Airlines Inc.            31   48.774194\nExpressJet Airlines Inc.        3964   29.642785\nFrontier Airlines Inc.            59   23.881356\nMesa Airlines Inc.                39   20.410256\nEndeavor Air Inc.               1480   19.321622\nAlaska Airlines Inc.              62   17.645161\nEnvoy Air                       2203   14.303677\nSouthwest Airlines Co.           985   12.964467\nJetBlue Airways                 4413   12.919329\nUnited Air Lines Inc.           4590   11.851852\nAmerican Airlines Inc.          2724   10.953377\nAirTran Airways Corporation      324    9.953704\nUS Airways Inc.                 1554    9.111326\nDelta Air Lines Inc.            3655    8.070315\nVirgin America                   314    3.165605\n```\n\n\n:::\n:::\n\n\n\n\n\nI chose to use the pipeline syntax from pandas - another option is to modify the dataset in place. That has a lower memory footprint, but can't be run repeatedly for the same result, such as in interactive use in a notebook.\n\nHere, the `query()` function is slightly awkward with the long string argument. The `groupby` doesn't allow renaming on the fly like dplyr, though I don't consider that a real drawback. Perhaps it's clearer to rename explicitly anyway.\n\nPandas has the widest API, offering hundreds of functions for every conceivable manipulation. The `clip` function used here is one such example. One difference to dplyr is that pandas uses its own methods `.mean()`, rather than using external ones such as `base::mean()`. That means using custom functions instead carries a [performance penalty](https://stackoverflow.com/a/26812998).\n\nAs we'll see later, pandas is the backend for siuba and ibis, which boil down to pandas code.\n\nOne difference to all other discussed solutions is that pandas uses a [row index](https://www.sharpsightlabs.com/blog/pandas-index/). Base R also has this with row names, but the tidyverse and tibbles have largely removed them from common use. I never missed row names. At the times I had to work with them in pandas they were more confusing than helpful. The documentation of polars puts it more bluntly:\n\n> No index. They are not needed. Not having them makes things easier. Convince me otherwise\n\nThat's quite passive aggressive, but I do agree and wish pandas didn't have it.\n\n## siuba: dplyr in Python\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport siuba as si\n\n# Import from CSV\nflights_si = pd.read_csv(r[\"flights_path\"])\nairlines_si = pd.read_csv(r[\"airlines_path\"])\n```\n:::\n\n\n\n\n\nAs siuba is just an alternative way of writing some pandas commands, we read the data just like in the pandas implementation.\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(\n    flights_si\n    >> si.filter(si._.year == 2013, si._.month == 1, si._.arr_delay.notnull())\n    >> si.mutate(arr_delay=si._.arr_delay.clip(lower=0))\n    >> si.left_join(si._, airlines_si, on=\"carrier\")\n    >> si.rename(airline=si._.name)\n    >> si.group_by(si._.airline)\n    >> si.summarize(flights=si._.airline.count(), mean_delay=si._.arr_delay.mean())\n    >> si.arrange(-si._.mean_delay)\n)\n```\n:::\n\n\n\n\n\nI found siuba the easiest to work with. Once I understood the `_` placeholder for a table of data, I could write it almost as fast as dplyr. Out of all the ways to refer to a column in a data frame, I found it to be the most convenient, because it doesn't require me to spell out the name of the data frame over and over. While not as elegant as dplyr's [tidy evaluation](https://www.tidyverse.org/blog/2019/06/rlang-0-4-0/#a-simpler-interpolation-pattern-with) (discussed at the end of the article), it avoids the ambivalence in dplyr where it can be unclear whether a name refers to a column or an outside object.\n\nIt's always possible to drop into pandas, such as for the aggregation functions which use the `mean()` and `count()` methods of the pandas series. The `>>` is an easy replacement for the `%>%` magrittr pipe or `|>` base pipe in R.\n\nThe author advertises siuba like this (from the [docs](https://siuba.readthedocs.io/en/latest/)):\n\n> Siuba is a library for quick, scrappy data analysis in Python. It is a port of dplyr, tidyr, and other R Tidyverse libraries.\n\nA way for dplyr users to quickly hack away at data analysis in Python, but not meant for unsupervised production use.\n\n## Polars: Fastest\n\nPolars is written in Rust and also offers a Python API. It comes in two flavors: eager and lazy. Lazy evaluation is similar to how dbplyr and dtplyr work: until asked, nothing is evaluated. This enables performance gains by reordering the commands being executed. But it's a little less convenient for interactive analysis. I'll use the eager API here.\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport polars as pl\n\n# Import from CSV\nflights_pl = pl.read_csv(flights_path)\nairlines_pl = pl.read_csv(airlines_path)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n(\n    flights_pl.filter((pl.col(\"year\") == 2013) & (pl.col(\"month\") == 1))\n    .drop_nulls(\"arr_delay\")\n    .join(airlines_pl, on=\"carrier\", how=\"left\")\n    .with_columns(\n        [\n            pl.when(pl.col(\"arr_delay\") > 0)\n            .then(pl.col(\"arr_delay\"))\n            .otherwise(0)\n            .alias(\"arr_delay\"),\n            pl.col(\"name\").alias(\"airline\"),\n        ]\n    )\n    .groupby(\"airline\")\n    .agg(\n        [pl.count(\"airline\").alias(\"flights\"), pl.mean(\"arr_delay\").alias(\"mean_delay\")]\n    )\n    .sort(\"mean_delay\", descending=True)\n)\n```\n:::\n\n\n\n\n\nThe API is leaner than pandas, requiring to memorize fewer functions and patterns. Though this can also be seen as less feature-complete. Pandas, for example has a dedicated `clip` function.\n\nThere isn't nearly as much help available for problems with polars as for with pandas. While the documentation is good, it can't answer every question and lots of trial and error is needed.\n\nA comparison of polars and pandas is available in the [polars documentation](https://pola-rs.github.io/polars-book/user-guide/coming_from_pandas.html?highlight=assign#column-assignment).\n\n## DuckDB: Highly compatible and easy for SQL users\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport duckdb\n\ncon_duckdb = duckdb.connect(database=\":memory:\")\n\n# Import from CSV\ncon_duckdb.execute(\n    \"CREATE TABLE 'flights' AS \"\n    f\"SELECT * FROM read_csv_auto('{flights_path}', header = True);\"\n    \"CREATE TABLE 'airlines' AS \"\n    f\"SELECT * FROM read_csv_auto('{airlines_path}', header = True);\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<duckdb.duckdb.DuckDBPyConnection object at 0x00000205F9A4F2B0>\n```\n\n\n:::\n:::\n\n\n\n\n\nDuckDB's `read_csv_auto()` works just like the csv readers in Python.\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncon_duckdb.execute(\n    \"WITH flights_clipped AS ( \"\n    \"SELECT carrier, CASE WHEN arr_delay > 0 THEN arr_delay ELSE 0 END AS arr_delay \"\n    \"FROM flights \"\n    \"WHERE year = 2013 AND month = 1 AND arr_delay IS NOT NULL\"\n    \")\"\n    \"SELECT name AS airline, COUNT(*) AS flights, AVG(arr_delay) AS mean_delay \"\n    \"FROM flights_clipped \"\n    \"LEFT JOIN airlines ON flights_clipped.carrier = airlines.carrier \"\n    \"GROUP BY name \"\n    \"ORDER BY mean_delay DESC \"\n).fetchdf()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                        airline  flights  mean_delay\n0         SkyWest Airlines Inc.        1  107.000000\n1        Hawaiian Airlines Inc.       31   48.774194\n2      ExpressJet Airlines Inc.     3964   29.642785\n3        Frontier Airlines Inc.       59   23.881356\n4            Mesa Airlines Inc.       39   20.410256\n5             Endeavor Air Inc.     1480   19.321622\n6          Alaska Airlines Inc.       62   17.645161\n7                     Envoy Air     2203   14.303677\n8        Southwest Airlines Co.      985   12.964467\n9               JetBlue Airways     4413   12.919329\n10        United Air Lines Inc.     4590   11.851852\n11       American Airlines Inc.     2724   10.953377\n12  AirTran Airways Corporation      324    9.953704\n13              US Airways Inc.     1554    9.111326\n14         Delta Air Lines Inc.     3655    8.070315\n15               Virgin America      314    3.165605\n```\n\n\n:::\n:::\n\n\n\n\n\nThe performance is closer to polars than to pandas. A big plus is the ability to handle larger than memory data.\n\nDuckDB can also operate directly on a pandas dataframe. The SQL code is portable to R, C, C++, Java and other programming languages the duckdb has [APIs](https://duckdb.org/docs/api/overview). It's also portable when the logic is taken to a DB like [Postgres](https://www.postgresql.org), or [Clickhouse](https://clickhouse.com), or is ported to an ETL framework like [DBT](https://github.com/dbt-labs/dbt-core).\n\nThis stands in contrast to polars and pandas code, which has to be rewritten from scratch. It also means that the skill gained in manipulating SQL translates well to other situations. SQL has been around for more than 50 years -  learning SQL is future-proofing a career.\n\nWhile these are big plusses, duckdb isn't so convenient for interactive data exploration. SQL isn't as composeable. Composing SQL queries requires many common table expressions (CTEs, `WITH x AS (SELECT ...)`). Reusing them for other queries is not as easy as with Python. SQL is typically less expressive than Python. It lacks shorthands and it's awkward when there are many columns. It's also harder to write custom functions in SQL than in R or Python. This is the motivation for using libraries like pandas and dplyr. But SQL can actually do a surprising amount of things, as database expert Haki Benita explained in a [detailed article](https://hakibenita.com/sql-for-data-analysis).\n\nOr in short, from the [documentation](https://ibis-project.org) of ibis:\n\n> SQL is widely used and very convenient when writing simple queries. But as the complexity of operations grow, SQL can become very difficult to deal with.\n\nThen, there's the issue of how to actually write the SQL code. Writing strings rather than actual Python is awkward and many editors don't provide syntax highlighting within the strings (Jetbrains editors like [PyCharm](https://www.jetbrains.com/pycharm/) and [DataSpell](https://www.jetbrains.com/dataspell/) do). The other option is writing `.sql` that have placeholders for parameters. That's cleaner and allows using a linter, but is inconvenient for interactive use.\n\nSQL is inherently lazily executed, because the query planner needs to take the whole query into account before starting computation. This enables performance gains. For interactive use, lazy evaluation is less convenient, because one can't see the intermediate results at each step. Speed of iteration is critical: the faster one can iterate, the more hypotheses about the data can be tested.\n\nThere is a [programmatic way to construct queries](https://github.com/duckdb/duckdb/blob/master/examples/python/duckdb-python.py) for duckdb, designed to provide a [dbplyr alternative](https://github.com/duckdb/duckdb/issues/302) in Python. Unfortunately its documentation is sparse.\n\nUsing duckdb without pandas doesn't seem feasible for exploratory data analysis, because graphing packages like seaborn and plotly expect a pandas data frame or similar as an input.\n\n## ibis: Lingua franca in Python\n\nThe goal of ibis is to provide a universal language for working with data frames in Python, regardless of the backend that is used. It's tagline is: *Write your analytics code once, run in everywhere*. This is similar to how dplyr can use SQL as a backend with dbplyr and data.table with dtplyr.\n\nAmong others, Ibis supports pandas, PostgreSQL and SQLite as backends. Unfortunately duckdb is not an available backend, because the authors of duckdb have [decided against](https://github.com/duckdb/duckdb/issues/302) building on ibis.\n\nThe ibis project aims to bridge the gap between the needs of interactive data analysis and the capabilities of SQL, which I have detailed in the previous section on duckdb.\n\n::: {.callout-note}\n**UPDATE October 2023**\n\n- Duckdb is now a supported backend (along with many more). So performance is going to be very similar to duckdb.\n- Directly load/save data\n- `join()`, `clip()`, and `case()` are well-supported\n- Ibis is much more popular and now very actively maintained. There are more examples, better documentation, and community. Still definitely less than pandas, but perhaps comparable to polars.\n\nThanks to [NickCrews](https://github.com/psimm/website/issues/10#issuecomment-1767099439) for providing this update, including the following code example.\n:::\n\nFor the test drive, I'll use the [duckdb backend](https://ibis-project.org/docs/backends/duckdb.html), meaning that the ibis code is translated to duckdb operations, similar to how siuba is translated to pandas. This gives ibis the blazing speed of duckdb.\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport ibis\nfrom ibis import _\n\nflights_ib_csv = pd.read_csv(flights_path)\nairlines_ib_csv = pd.read_csv(airlines_path)\n\nibis.options.interactive = True\n\nflights_ib = ibis.read_csv(flights_path)\nairlines_ib = ibis.read_csv(airlines_path)\nflights_ib\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n┌───────┬───────┬───────┬──────────┬────────────────┬───────────┬──────────┬──┐\n│ year  │ month │ day   │ dep_time │ sched_dep_time │ dep_delay │ arr_time │  │\n├───────┼───────┼───────┼──────────┼────────────────┼───────────┼──────────┼──┤\n│ int64 │ int64 │ int64 │ int64    │ int64          │ int64     │ int64    │  │\n├───────┼───────┼───────┼──────────┼────────────────┼───────────┼──────────┼──┤\n│  2013 │     1 │     1 │      517 │            515 │         2 │      830 │  │\n│  2013 │     1 │     1 │      533 │            529 │         4 │      850 │  │\n│  2013 │     1 │     1 │      542 │            540 │         2 │      923 │  │\n│  2013 │     1 │     1 │      544 │            545 │        -1 │     1004 │  │\n│  2013 │     1 │     1 │      554 │            600 │        -6 │      812 │  │\n│  2013 │     1 │     1 │      554 │            558 │        -4 │      740 │  │\n│  2013 │     1 │     1 │      555 │            600 │        -5 │      913 │  │\n│  2013 │     1 │     1 │      557 │            600 │        -3 │      709 │  │\n│  2013 │     1 │     1 │      557 │            600 │        -3 │      838 │  │\n│  2013 │     1 │     1 │      558 │            600 │        -2 │      753 │  │\n│     … │     … │     … │        … │              … │         … │        … │  │\n└───────┴───────┴───────┴──────────┴────────────────┴───────────┴──────────┴──┘\n```\n\n\n:::\n:::\n\n\n\n\n\nNon-interactive ibis means that queries are evaluated lazily.\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(\n    flights_ib.filter(\n        [\n            _.year == 2013,\n            _.month == 1,\n            _.arr_delay.notnull(),\n        ]\n    )\n    .join(airlines_ib, \"carrier\", how=\"left\")\n    .select(arr_delay=_.arr_delay.clip(lower=0), airline=_.name)\n    .group_by(\"airline\")\n    .agg(flights=_.count(), mean_delay=_.arr_delay.mean())\n    .order_by(_.mean_delay.desc())\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n┌──────────────────────────┬─────────┬────────────┐\n│ airline                  │ flights │ mean_delay │\n├──────────────────────────┼─────────┼────────────┤\n│ string                   │ int64   │ float64    │\n├──────────────────────────┼─────────┼────────────┤\n│ SkyWest Airlines Inc.    │       1 │ 107.000000 │\n│ Hawaiian Airlines Inc.   │      31 │  48.774194 │\n│ ExpressJet Airlines Inc. │    3964 │  29.642785 │\n│ Frontier Airlines Inc.   │      59 │  23.881356 │\n│ Mesa Airlines Inc.       │      39 │  20.410256 │\n│ Endeavor Air Inc.        │    1480 │  19.321622 │\n│ Alaska Airlines Inc.     │      62 │  17.645161 │\n│ Envoy Air                │    2203 │  14.303677 │\n│ Southwest Airlines Co.   │     985 │  12.964467 │\n│ JetBlue Airways          │    4413 │  12.919329 │\n│ …                        │       … │          … │\n└──────────────────────────┴─────────┴────────────┘\n```\n\n\n:::\n:::\n\n\n\n\n\nThe syntax looks quite similar to dplyr and the versatility of interchangeable backends is remarkable. In the first version of this article, ibis was lacking in documentation and had some rough edges in the API, but these were improved in the meantime.\n\n## Conclusion\n\nIt's not a clear-cut choice. None of the options offer a syntax that is as convenient for interactive analysis as dplyr. siuba is the closest to it, but dplyr still has an edge with [tidy evaluation](https://www.tidyverse.org/blog/2019/06/rlang-0-4-0/#a-simpler-interpolation-pattern-with), letting users refer to columns in a data frame by their names (`colname`) directly, without any wrappers. But I've also seen it be confusing for newbies to R that mix it up with base R's syntax. It's also harder to program with, where it's necessary to use operators like `{{ }}` and `:=`.\n\nMy appreciation for dplyr (and the closely associated tidyr) grew during this research. Not only is it a widely accepted standard like pandas, it can also be used as a translation layer for backends like SQL databases (including duckdb), data.table, and Spark. All while having the most elegant and flexible syntax available.\n\nPersonally, I'll primarily leverage SQL and a OLAP database (such as Clickhouse or Snowflake) running on a server to do the heavy lifting. For steps that are better done locally, I'll use pandas for maximum compatibility. I find the use of an index inconvenient, but there's so much online help available on StackOverflow. Github Copilot also deserves a mention for making it easier to pick up. Other use cases can be very different, so I don't mean to say that my way is the best. For instance, if the data is not already on a server, fast local processing with polars may be best.\n\nMost data science work happens in a team. Choosing a library that all team members are familiar with is critical for collaboration. That is typically SQL, pandas or dplyr. The performance gains from using a less common library like polars have to be weighed against the effort spent learning the syntax as well as the increased likelihood of bugs, when beginners write in a new syntax.\n\nRelated articles:\n\n- [Polars: the fastest DataFrame library you've never heard of](https://www.analyticsvidhya.com/blog/2021/06/polars-the-fastest-dataframe-library-youve-never-heard-of/)\n- [What would it take to recreate dplyr in python?](https://mchow.com/posts/2020-02-11-dplyr-in-python/)\n- [Pandas has a hard job (and does it well)](https://mchow.com/posts/pandas-has-a-hard-job/)\n- [dplyr in Python? First impressions of the siuba module](https://bensstats.wordpress.com/2021/09/14/pythonmusings-6-dplyr-in-python-first-impressions-of-the-siuba-小巴-module/)\n- [An Overview of Python's Datatable package](https://towardsdatascience.com/an-overview-of-pythons-datatable-package-5d3a97394ee9)\n- [Discussion of DuckDB on Hacker News](https://news.ycombinator.com/item?id=24531085)\n- [Discussion of Polars on Hacker News](https://news.ycombinator.com/item?id=29584698)\n- [Practical SQL for Data Analysis](https://hakibenita.com/sql-for-data-analysis)\n\nPhoto by <a href=\"https://unsplash.com/@hharritt?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Hunter Harritt</a> on <a href=\"https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/core-js-2.5.3/shim.min.js\"></script>\n<script src=\"../../site_libs/react-18.2.0/react.min.js\"></script>\n<script src=\"../../site_libs/react-18.2.0/react-dom.min.js\"></script>\n<script src=\"../../site_libs/reactwidget-2.0.0/react-tools.js\"></script>\n<link href=\"../../site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<link href=\"../../site_libs/reactable-0.4.4/reactable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/reactable-binding-0.4.4/reactable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}