[
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Upcoming\nNone at this time.\n\n\nPast\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to keep information ‘fresh’\n\n\n\n\n\n\nCommunication\n\n\nTechnology\n\n\n\nThe talk will introduce the concept of the ‘age of information’—a metric used to assess the freshness of information—and analyze this metric within a random multi-access system involving numerous devices\n\n\n\n\n\nNov 17, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "{aetranslations}\n\n\n\nR package\n\n\n\nThis package hosts functions and guidelines for translating Applied Epi courses and resources\n\n\n\nOct 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAMR Global Trend\n\n\n\nShiny app\n\n\n\nAMR Global Trend: A shiny app for exploring the global trend of AMR indicators from 73 countries\n\n\n\nAug 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAMR-SDGs interactions\n\n\n\nShiny app\n\n\n\nAMR-SDGs Explorer: A shiny app for exploring the interaction between AMR and SDGs agenda in national policies\n\n\n\nJun 20, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/amr_sdg.html",
    "href": "projects/amr_sdg.html",
    "title": "AMR-SDGs interactions",
    "section": "",
    "text": "App  Code\nThe ShinyApp is the supplementary materials for the article:\n“When Global Health meets Global Goals”: A comparative analysis the alignment between National Action Plans for AMR and Sustainable Development\nLuong Nguyen-Thanh1,2,4, Didier Wernli3, Mats Målqvist1, Peter Søgaard Jørgensen1,4,5\nAffiliations:\n1 SWEDESD – Sustainability Learning and Research Center, Department of Women’s and Children’s Health, Uppsala University, Uppsala, Sweden\n2 Uppsala Antibiotic Centre (UAC), Uppsala University\n3 Global Studies Institute and Faculty of Science, University of Geneva, Geneva, Switzerland\n4 Global Economic Dynamics and the Biosphere, Royal Swedish Academy of Sciences, Stockholm, Sweden\n5 Stockholm Resilience Centre, Stockholm University, Stockholm, Sweden\n\n\n\nScreenshot from the App"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Luong Nguyen Thanh",
    "section": "",
    "text": "GitHub\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Research Profile"
  },
  {
    "objectID": "index.html#hi-there",
    "href": "index.html#hi-there",
    "title": "Luong Nguyen Thanh",
    "section": "Hi there!",
    "text": "Hi there!\nWelcome to my website — I’m glad you’re here!\nI’m currently a PhD student at Uppsala University, following an academic path rooted in health and sustainability sciences, with degrees in Public Health (BPH) and One Health and Global Health (MSc).\nBeyond academia, I am passionate about programming, especially in R, with experience building packages, dashboards, Shiny apps, and working with large language models. My technical skills also extend to intermediate Python and foundational knowledge of SQL, HTML, CSS, and JavaScript. I’m especially interested in reproducibility and open science, and finding ways to streamline workflow.\nMy guiding principles in life are kindness, discipline, and integrity. I have found genuine fulfillment in working within the health and sustainability data science field, where I have the opportunity to engage with an inspiring community of like-minded professionals.\nIn my spare time, I love exploring history and diving into subjects related to religion and culture."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "The Data Alchemist",
    "section": "",
    "text": "“Welcome, adventurer! I’m thrilled you’ve joined me on this journey through the world of data science and code. Here, I’ll share the discoveries I’m making, the projects I’m building, and the curiosities I’m exploring—sprinkled with a little feline charm along the way. Let’s learn and uncover insights together!”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSwitching from R to Python: A Beginner’s Guide to Equivalent Tools\n\n\n\n\n\n\nPython\n\n\nR\n\n\nData Science\n\n\n\nLearn how to transition from R to Python seamlessly with tools that match your R favorites like dplyr, ggplot2, Shiny, and more.\n\n\n\n\n\nNov 28, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nData frame wars: Choosing a Python dataframe library as a dplyr user\n\n\n\n\n\n\nR\n\n\nPython\n\n\n\n\n\n\n\n\n\nJan 25, 2023\n\n\n17 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html",
    "href": "blog/2024-11-01_dplyr_candidate/index.html",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "",
    "text": "I’m a long time R user and lately I’ve seen more and more signals that it’s worth investing into Python. I use it for NLP with spaCy and to build functions on AWS Lambda. Further, there are many more data API libraries and machine learning libraries for Python than for R.\nAdopting Python means making choices on which libraries to invest time into learning. Manipulating data frames is one of the most common data science activities, so choosing the right library for it is key.\nMichael Chow, developer of siuba, a Python port of dplyr on top of pandas wrote describes the situation well:\nThe higher-level libraries he mentions come with a problem : There’s no universal standard.\nIn a discussion of the polars library on Hacker News the user “civilized” put the dplyr user perspective more bluntly:\nI’m more willing to compromise though, so here’s a comparison of the strongest contenders."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#the-contenders",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#the-contenders",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "The contenders",
    "text": "The contenders\nThe database-like ops benchmark on H2Oai is a helpful performance comparison.\nI’m considering these libraries:\n\nPandas: The most commonly used library and the one with the most tutorials and Stack Overflow answers available.\nsiuba: A port of dplyr to Python, built on top of pandas. Not in the benchmark. Performance probably similar to pandas or worse due to translation.\nPolars: The fastest library available. According to the benchmark, it runs 3-10x faster than Pandas.\nDuckdb: Use an in-memory OLAP database instead of a dataframe and write SQL. In R, this can also be queried via dbplyr.\nibis. Backend-agnostic wrapper for pandas and SQL engines.\n\nThere are more options. I excluded the others for these reasons:\n\nSlower than polars and not with a readability focus (dask, Arrow, Modin, pydatatable)\nRequires or is optmized for running on a remote server (Spark, ClickHouse and most other SQL databases).\nNot meant for OLAP (sqlite)\nNot in Python (DataFrames.jl)\nMeant for GPU (cuDF)"
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#github-stars-as-a-proxy-for-popularity",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#github-stars-as-a-proxy-for-popularity",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Github stars as a proxy for popularity",
    "text": "Github stars as a proxy for popularity\nThe benchmark provides a comparison of performance, but another important factor is popularity and maturity. A more mature library has a more stable API, better test coverage and there is more help available online, such as on StackOverflow. One way to measure popularity is the number of stars that the package repository has on Github.\n\nlibrary(ggplot2)\nlibs &lt;- data.frame(\n    library = c(\"pandas\", \"siuba\", \"polars\", \"duckdb\", \"dplyr\", \"data.table\", \"pydatatable\", \"dtplyr\", \"tidytable\", \"ibis\"),\n    language = c(\"Python\", \"Python\", \"Python\", \"SQL\", \"R\", \"R\", \"Python\", \"R\", \"R\", \"Python\"),\n    stars = c(32100, 732, 3900, 4100, 3900, 2900, 1400, 542, 285, 1600)\n)\n\nggplot(libs, aes(x = reorder(library, -stars), y = stars, fill = language)) +\n    geom_col() +\n    labs(\n        title = \"Pandas is by far the most popular choice\",\n        subtitle = \"Comparison of Github stars\",\n        fill = \"Language\",\n        x = \"Library\",\n        y = \"Github stars\"\n    )\n\n\n\n\n\n\n\n\nGithub stars are not a perfect proxy. For instance, dplyr is more mature than its star count suggests. Comparing the completeness of the documentation and tutorials for dplyr and polars reveals that it’s a day and night difference.\nWith the quantitative comparison out of the way, here’s a qualitative comparison of the Python packages. I’m speaking of my personal opinion of these packages - not a general comparison. My reference is my current use of dplyr in R. When I need more performance, I use tidytable to get most of the speed of data.table with the grammar of dplyr and eager evaluation. Another alternative is dtplyr, which translates dplyr to data.table with lazy evaluation. I also use dbplyr, which translates dplyr to SQL.\nI’ll compare the libraries by running a data transformation pipeline involving import from CSV, mutate, filter, sort, join, group by and summarize. I’ll use the nycflights13 dataset, which is featured in Hadley Wickham’s R for Data Science."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#dplyr-reference-in-r",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#dplyr-reference-in-r",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "dplyr: Reference in R",
    "text": "dplyr: Reference in R\nLet’s start with a reference implementation in dplyr. The dataset is available as a package, so I skip the CSV import.\n\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(nycflights13)\nlibrary(reactable)\n\n# Take a look at the tables\nreactable(head(flights, 10))\n\n\n\n\nreactable(head(airlines, 10))\n\n\n\n\n\nThe flights tables has 336776 rows, one for each flight of an airplane. The airlines table has 16 rows, one for each airline mapping the full name of the company to a code.\nLet’s find the airline with the highest arrival delays in January 2013.\n\nflights |&gt;\n    filter(year == 2013, month == 1, !is.na(arr_delay)) |&gt;\n    mutate(arr_delay = replace(arr_delay, arr_delay &lt; 0, 0)) |&gt;\n    left_join(airlines, by = \"carrier\") |&gt;\n    group_by(airline = name) |&gt;\n    summarise(flights = n(), mean_delay = mean(arr_delay)) |&gt;\n    arrange(desc(mean_delay))\n\n# A tibble: 16 × 3\n   airline                     flights mean_delay\n   &lt;chr&gt;                         &lt;int&gt;      &lt;dbl&gt;\n 1 SkyWest Airlines Inc.             1     107   \n 2 Hawaiian Airlines Inc.           31      48.8 \n 3 ExpressJet Airlines Inc.       3964      29.6 \n 4 Frontier Airlines Inc.           59      23.9 \n 5 Mesa Airlines Inc.               39      20.4 \n 6 Endeavor Air Inc.              1480      19.3 \n 7 Alaska Airlines Inc.             62      17.6 \n 8 Envoy Air                      2203      14.3 \n 9 Southwest Airlines Co.          985      13.0 \n10 JetBlue Airways                4413      12.9 \n11 United Air Lines Inc.          4590      11.9 \n12 American Airlines Inc.         2724      11.0 \n13 AirTran Airways Corporation     324       9.95\n14 US Airways Inc.                1554       9.11\n15 Delta Air Lines Inc.           3655       8.07\n16 Virgin America                  314       3.17\n\n\nSome values in arr_delay are negative, indicating that the flight was faster than expected. I replaced these values with 0 because I don’t want them to cancel out delays of other flights. I joined to the airlines table to get the full names of the airlines.\nI export the flights and airlines tables to CSV to hand them over to Python.\n\n# Write to temporary files\nflights_path &lt;- tempfile(fileext = \".csv\")\nairlines_path &lt;- tempfile(fileext = \".csv\")\n\ndata.table::fwrite(flights, flights_path, row.names = FALSE)\ndata.table::fwrite(airlines, airlines_path, row.names = FALSE)\n\nTo access the file from Python, the path is handed over:\n\n\n#| eval: false\n# Hand over the path from R\nflights_path = r[\"flights_path\"]\nairlines_path = r[\"airlines_path\"]\n\nFor more details on how this works with the reticulate package, check this documentation."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#pandas-most-popular",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#pandas-most-popular",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Pandas: Most popular",
    "text": "Pandas: Most popular\nThe following sections follow a pattern: read in from CSV, then build a query.\n\nimport pandas as pd\n\n# Import from CSV\nflights_pd = pd.read_csv(flights_path)\nairlines_pd = pd.read_csv(airlines_path)\n\npandas.read_csv reads the header and conveniently infers the column types.\n\n(\n    flights_pd.query(\"year == 2013 & month == 1 & arr_delay.notnull()\")\n    .assign(arr_delay=flights_pd.arr_delay.clip(lower=0))\n    .merge(airlines_pd, how=\"left\", on=\"carrier\")\n    .rename(columns={\"name\": \"airline\"})\n    .groupby(\"airline\")\n    .agg(flights=(\"airline\", \"count\"), mean_delay=(\"arr_delay\", \"mean\"))\n    .sort_values(by=\"mean_delay\", ascending=False)\n)\n\n                             flights  mean_delay\nairline                                         \nSkyWest Airlines Inc.              1  107.000000\nHawaiian Airlines Inc.            31   48.774194\nExpressJet Airlines Inc.        3964   29.642785\nFrontier Airlines Inc.            59   23.881356\nMesa Airlines Inc.                39   20.410256\nEndeavor Air Inc.               1480   19.321622\nAlaska Airlines Inc.              62   17.645161\nEnvoy Air                       2203   14.303677\nSouthwest Airlines Co.           985   12.964467\nJetBlue Airways                 4413   12.919329\nUnited Air Lines Inc.           4590   11.851852\nAmerican Airlines Inc.          2724   10.953377\nAirTran Airways Corporation      324    9.953704\nUS Airways Inc.                 1554    9.111326\nDelta Air Lines Inc.            3655    8.070315\nVirgin America                   314    3.165605\n\n\nI chose to use the pipeline syntax from pandas - another option is to modify the dataset in place. That has a lower memory footprint, but can’t be run repeatedly for the same result, such as in interactive use in a notebook.\nHere, the query() function is slightly awkward with the long string argument. The groupby doesn’t allow renaming on the fly like dplyr, though I don’t consider that a real drawback. Perhaps it’s clearer to rename explicitly anyway.\nPandas has the widest API, offering hundreds of functions for every conceivable manipulation. The clip function used here is one such example. One difference to dplyr is that pandas uses its own methods .mean(), rather than using external ones such as base::mean(). That means using custom functions instead carries a performance penalty.\nAs we’ll see later, pandas is the backend for siuba and ibis, which boil down to pandas code.\nOne difference to all other discussed solutions is that pandas uses a row index. Base R also has this with row names, but the tidyverse and tibbles have largely removed them from common use. I never missed row names. At the times I had to work with them in pandas they were more confusing than helpful. The documentation of polars puts it more bluntly:\n\nNo index. They are not needed. Not having them makes things easier. Convince me otherwise\n\nThat’s quite passive aggressive, but I do agree and wish pandas didn’t have it."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#siuba-dplyr-in-python",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#siuba-dplyr-in-python",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "siuba: dplyr in Python",
    "text": "siuba: dplyr in Python\n\nimport siuba as si\n\n# Import from CSV\nflights_si = pd.read_csv(r[\"flights_path\"])\nairlines_si = pd.read_csv(r[\"airlines_path\"])\n\nAs siuba is just an alternative way of writing some pandas commands, we read the data just like in the pandas implementation.\n\n(\n    flights_si\n    &gt;&gt; si.filter(si._.year == 2013, si._.month == 1, si._.arr_delay.notnull())\n    &gt;&gt; si.mutate(arr_delay=si._.arr_delay.clip(lower=0))\n    &gt;&gt; si.left_join(si._, airlines_si, on=\"carrier\")\n    &gt;&gt; si.rename(airline=si._.name)\n    &gt;&gt; si.group_by(si._.airline)\n    &gt;&gt; si.summarize(flights=si._.airline.count(), mean_delay=si._.arr_delay.mean())\n    &gt;&gt; si.arrange(-si._.mean_delay)\n)\n\nI found siuba the easiest to work with. Once I understood the _ placeholder for a table of data, I could write it almost as fast as dplyr. Out of all the ways to refer to a column in a data frame, I found it to be the most convenient, because it doesn’t require me to spell out the name of the data frame over and over. While not as elegant as dplyr’s tidy evaluation (discussed at the end of the article), it avoids the ambivalence in dplyr where it can be unclear whether a name refers to a column or an outside object.\nIt’s always possible to drop into pandas, such as for the aggregation functions which use the mean() and count() methods of the pandas series. The &gt;&gt; is an easy replacement for the %&gt;% magrittr pipe or |&gt; base pipe in R.\nThe author advertises siuba like this (from the docs):\n\nSiuba is a library for quick, scrappy data analysis in Python. It is a port of dplyr, tidyr, and other R Tidyverse libraries.\n\nA way for dplyr users to quickly hack away at data analysis in Python, but not meant for unsupervised production use."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#polars-fastest",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#polars-fastest",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Polars: Fastest",
    "text": "Polars: Fastest\nPolars is written in Rust and also offers a Python API. It comes in two flavors: eager and lazy. Lazy evaluation is similar to how dbplyr and dtplyr work: until asked, nothing is evaluated. This enables performance gains by reordering the commands being executed. But it’s a little less convenient for interactive analysis. I’ll use the eager API here.\n\nimport polars as pl\n\n# Import from CSV\nflights_pl = pl.read_csv(flights_path)\nairlines_pl = pl.read_csv(airlines_path)\n\n\n(\n    flights_pl.filter((pl.col(\"year\") == 2013) & (pl.col(\"month\") == 1))\n    .drop_nulls(\"arr_delay\")\n    .join(airlines_pl, on=\"carrier\", how=\"left\")\n    .with_columns(\n        [\n            pl.when(pl.col(\"arr_delay\") &gt; 0)\n            .then(pl.col(\"arr_delay\"))\n            .otherwise(0)\n            .alias(\"arr_delay\"),\n            pl.col(\"name\").alias(\"airline\"),\n        ]\n    )\n    .groupby(\"airline\")\n    .agg(\n        [pl.count(\"airline\").alias(\"flights\"), pl.mean(\"arr_delay\").alias(\"mean_delay\")]\n    )\n    .sort(\"mean_delay\", descending=True)\n)\n\nThe API is leaner than pandas, requiring to memorize fewer functions and patterns. Though this can also be seen as less feature-complete. Pandas, for example has a dedicated clip function.\nThere isn’t nearly as much help available for problems with polars as for with pandas. While the documentation is good, it can’t answer every question and lots of trial and error is needed.\nA comparison of polars and pandas is available in the polars documentation."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#duckdb-highly-compatible-and-easy-for-sql-users",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#duckdb-highly-compatible-and-easy-for-sql-users",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "DuckDB: Highly compatible and easy for SQL users",
    "text": "DuckDB: Highly compatible and easy for SQL users\n\nimport duckdb\n\ncon_duckdb = duckdb.connect(database=\":memory:\")\n\n# Import from CSV\ncon_duckdb.execute(\n    \"CREATE TABLE 'flights' AS \"\n    f\"SELECT * FROM read_csv_auto('{flights_path}', header = True);\"\n    \"CREATE TABLE 'airlines' AS \"\n    f\"SELECT * FROM read_csv_auto('{airlines_path}', header = True);\"\n)\n\n&lt;duckdb.duckdb.DuckDBPyConnection object at 0x0000014F23081730&gt;\n\n\nDuckDB’s read_csv_auto() works just like the csv readers in Python.\n\ncon_duckdb.execute(\n    \"WITH flights_clipped AS ( \"\n    \"SELECT carrier, CASE WHEN arr_delay &gt; 0 THEN arr_delay ELSE 0 END AS arr_delay \"\n    \"FROM flights \"\n    \"WHERE year = 2013 AND month = 1 AND arr_delay IS NOT NULL\"\n    \")\"\n    \"SELECT name AS airline, COUNT(*) AS flights, AVG(arr_delay) AS mean_delay \"\n    \"FROM flights_clipped \"\n    \"LEFT JOIN airlines ON flights_clipped.carrier = airlines.carrier \"\n    \"GROUP BY name \"\n    \"ORDER BY mean_delay DESC \"\n).fetchdf()\n\n                        airline  flights  mean_delay\n0         SkyWest Airlines Inc.        1  107.000000\n1        Hawaiian Airlines Inc.       31   48.774194\n2      ExpressJet Airlines Inc.     3964   29.642785\n3        Frontier Airlines Inc.       59   23.881356\n4            Mesa Airlines Inc.       39   20.410256\n5             Endeavor Air Inc.     1480   19.321622\n6          Alaska Airlines Inc.       62   17.645161\n7                     Envoy Air     2203   14.303677\n8        Southwest Airlines Co.      985   12.964467\n9               JetBlue Airways     4413   12.919329\n10        United Air Lines Inc.     4590   11.851852\n11       American Airlines Inc.     2724   10.953377\n12  AirTran Airways Corporation      324    9.953704\n13              US Airways Inc.     1554    9.111326\n14         Delta Air Lines Inc.     3655    8.070315\n15               Virgin America      314    3.165605\n\n\nThe performance is closer to polars than to pandas. A big plus is the ability to handle larger than memory data.\nDuckDB can also operate directly on a pandas dataframe. The SQL code is portable to R, C, C++, Java and other programming languages the duckdb has APIs. It’s also portable when the logic is taken to a DB like Postgres, or Clickhouse, or is ported to an ETL framework like DBT.\nThis stands in contrast to polars and pandas code, which has to be rewritten from scratch. It also means that the skill gained in manipulating SQL translates well to other situations. SQL has been around for more than 50 years - learning SQL is future-proofing a career.\nWhile these are big plusses, duckdb isn’t so convenient for interactive data exploration. SQL isn’t as composeable. Composing SQL queries requires many common table expressions (CTEs, WITH x AS (SELECT ...)). Reusing them for other queries is not as easy as with Python. SQL is typically less expressive than Python. It lacks shorthands and it’s awkward when there are many columns. It’s also harder to write custom functions in SQL than in R or Python. This is the motivation for using libraries like pandas and dplyr. But SQL can actually do a surprising amount of things, as database expert Haki Benita explained in a detailed article.\nOr in short, from the documentation of ibis:\n\nSQL is widely used and very convenient when writing simple queries. But as the complexity of operations grow, SQL can become very difficult to deal with.\n\nThen, there’s the issue of how to actually write the SQL code. Writing strings rather than actual Python is awkward and many editors don’t provide syntax highlighting within the strings (Jetbrains editors like PyCharm and DataSpell do). The other option is writing .sql that have placeholders for parameters. That’s cleaner and allows using a linter, but is inconvenient for interactive use.\nSQL is inherently lazily executed, because the query planner needs to take the whole query into account before starting computation. This enables performance gains. For interactive use, lazy evaluation is less convenient, because one can’t see the intermediate results at each step. Speed of iteration is critical: the faster one can iterate, the more hypotheses about the data can be tested.\nThere is a programmatic way to construct queries for duckdb, designed to provide a dbplyr alternative in Python. Unfortunately its documentation is sparse.\nUsing duckdb without pandas doesn’t seem feasible for exploratory data analysis, because graphing packages like seaborn and plotly expect a pandas data frame or similar as an input."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#ibis-lingua-franca-in-python",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#ibis-lingua-franca-in-python",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "ibis: Lingua franca in Python",
    "text": "ibis: Lingua franca in Python\nThe goal of ibis is to provide a universal language for working with data frames in Python, regardless of the backend that is used. It’s tagline is: Write your analytics code once, run in everywhere. This is similar to how dplyr can use SQL as a backend with dbplyr and data.table with dtplyr.\nAmong others, Ibis supports pandas, PostgreSQL and SQLite as backends. Unfortunately duckdb is not an available backend, because the authors of duckdb have decided against building on ibis.\nThe ibis project aims to bridge the gap between the needs of interactive data analysis and the capabilities of SQL, which I have detailed in the previous section on duckdb.\n\n\n\n\n\n\nNote\n\n\n\nUPDATE October 2023\n\nDuckdb is now a supported backend (along with many more). So performance is going to be very similar to duckdb.\nDirectly load/save data\njoin(), clip(), and case() are well-supported\nIbis is much more popular and now very actively maintained. There are more examples, better documentation, and community. Still definitely less than pandas, but perhaps comparable to polars.\n\nThanks to NickCrews for providing this update, including the following code example.\n\n\nFor the test drive, I’ll use the duckdb backend, meaning that the ibis code is translated to duckdb operations, similar to how siuba is translated to pandas. This gives ibis the blazing speed of duckdb.\n\nimport ibis\nfrom ibis import _\n\nflights_ib_csv = pd.read_csv(flights_path)\nairlines_ib_csv = pd.read_csv(airlines_path)\n\nibis.options.interactive = True\n\nflights_ib = ibis.read_csv(flights_path)\nairlines_ib = ibis.read_csv(airlines_path)\nflights_ib\n\n┌───────┬───────┬───────┬──────────┬────────────────┬───────────┬──────────┬──┐\n│ year  │ month │ day   │ dep_time │ sched_dep_time │ dep_delay │ arr_time │  │\n├───────┼───────┼───────┼──────────┼────────────────┼───────────┼──────────┼──┤\n│ int64 │ int64 │ int64 │ int64    │ int64          │ int64     │ int64    │  │\n├───────┼───────┼───────┼──────────┼────────────────┼───────────┼──────────┼──┤\n│  2013 │     1 │     1 │      517 │            515 │         2 │      830 │  │\n│  2013 │     1 │     1 │      533 │            529 │         4 │      850 │  │\n│  2013 │     1 │     1 │      542 │            540 │         2 │      923 │  │\n│  2013 │     1 │     1 │      544 │            545 │        -1 │     1004 │  │\n│  2013 │     1 │     1 │      554 │            600 │        -6 │      812 │  │\n│  2013 │     1 │     1 │      554 │            558 │        -4 │      740 │  │\n│  2013 │     1 │     1 │      555 │            600 │        -5 │      913 │  │\n│  2013 │     1 │     1 │      557 │            600 │        -3 │      709 │  │\n│  2013 │     1 │     1 │      557 │            600 │        -3 │      838 │  │\n│  2013 │     1 │     1 │      558 │            600 │        -2 │      753 │  │\n│     … │     … │     … │        … │              … │         … │        … │  │\n└───────┴───────┴───────┴──────────┴────────────────┴───────────┴──────────┴──┘\n\n\nNon-interactive ibis means that queries are evaluated lazily.\n\n(\n    flights_ib.filter(\n        [\n            _.year == 2013,\n            _.month == 1,\n            _.arr_delay.notnull(),\n        ]\n    )\n    .join(airlines_ib, \"carrier\", how=\"left\")\n    .select(arr_delay=_.arr_delay.clip(lower=0), airline=_.name)\n    .group_by(\"airline\")\n    .agg(flights=_.count(), mean_delay=_.arr_delay.mean())\n    .order_by(_.mean_delay.desc())\n)\n\n┌──────────────────────────┬─────────┬────────────┐\n│ airline                  │ flights │ mean_delay │\n├──────────────────────────┼─────────┼────────────┤\n│ string                   │ int64   │ float64    │\n├──────────────────────────┼─────────┼────────────┤\n│ SkyWest Airlines Inc.    │       1 │ 107.000000 │\n│ Hawaiian Airlines Inc.   │      31 │  48.774194 │\n│ ExpressJet Airlines Inc. │    3964 │  29.642785 │\n│ Frontier Airlines Inc.   │      59 │  23.881356 │\n│ Mesa Airlines Inc.       │      39 │  20.410256 │\n│ Endeavor Air Inc.        │    1480 │  19.321622 │\n│ Alaska Airlines Inc.     │      62 │  17.645161 │\n│ Envoy Air                │    2203 │  14.303677 │\n│ Southwest Airlines Co.   │     985 │  12.964467 │\n│ JetBlue Airways          │    4413 │  12.919329 │\n│ …                        │       … │          … │\n└──────────────────────────┴─────────┴────────────┘\n\n\nThe syntax looks quite similar to dplyr and the versatility of interchangeable backends is remarkable. In the first version of this article, ibis was lacking in documentation and had some rough edges in the API, but these were improved in the meantime."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#conclusion",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#conclusion",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Conclusion",
    "text": "Conclusion\nIt’s not a clear-cut choice. None of the options offer a syntax that is as convenient for interactive analysis as dplyr. siuba is the closest to it, but dplyr still has an edge with tidy evaluation, letting users refer to columns in a data frame by their names (colname) directly, without any wrappers. But I’ve also seen it be confusing for newbies to R that mix it up with base R’s syntax. It’s also harder to program with, where it’s necessary to use operators like { } and :=.\nMy appreciation for dplyr (and the closely associated tidyr) grew during this research. Not only is it a widely accepted standard like pandas, it can also be used as a translation layer for backends like SQL databases (including duckdb), data.table, and Spark. All while having the most elegant and flexible syntax available.\nPersonally, I’ll primarily leverage SQL and a OLAP database (such as Clickhouse or Snowflake) running on a server to do the heavy lifting. For steps that are better done locally, I’ll use pandas for maximum compatibility. I find the use of an index inconvenient, but there’s so much online help available on StackOverflow. Github Copilot also deserves a mention for making it easier to pick up. Other use cases can be very different, so I don’t mean to say that my way is the best. For instance, if the data is not already on a server, fast local processing with polars may be best.\nMost data science work happens in a team. Choosing a library that all team members are familiar with is critical for collaboration. That is typically SQL, pandas or dplyr. The performance gains from using a less common library like polars have to be weighed against the effort spent learning the syntax as well as the increased likelihood of bugs, when beginners write in a new syntax.\nRelated articles:\n\nPolars: the fastest DataFrame library you’ve never heard of\nWhat would it take to recreate dplyr in python?\nPandas has a hard job (and does it well)\ndplyr in Python? First impressions of the siuba module\nAn Overview of Python’s Datatable package\nDiscussion of DuckDB on Hacker News\nDiscussion of Polars on Hacker News\nPractical SQL for Data Analysis"
  },
  {
    "objectID": "blog/2024-11-28_transition_to_python/index.html",
    "href": "blog/2024-11-28_transition_to_python/index.html",
    "title": "Switching from R to Python: A Beginner’s Guide to Equivalent Tools",
    "section": "",
    "text": "Transitioning from R to Python can feel like a daunting leap, especially if you’ve grown comfortable with R’s ecosystem. The good news? Python offers several tools and libraries that mimic the syntax and functionality of your favorite R packages. Let’s explore these equivalents to ease your journey."
  },
  {
    "objectID": "blog/2024-11-28_transition_to_python/index.html#data-manipulation-ibis",
    "href": "blog/2024-11-28_transition_to_python/index.html#data-manipulation-ibis",
    "title": "Switching from R to Python: A Beginner’s Guide to Equivalent Tools",
    "section": "2.1 Data Manipulation: ibis",
    "text": "2.1 Data Manipulation: ibis\nIn R, dplyr and dbplyr are go-to packages for data manipulation, offering a clean, declarative syntax to filter, mutate, summarize, and join datasets. Python’s ibis serves as an excellent alternative, providing a similar experience for working with structured data.\nWhat sets ibis apart is its performance optimization. It abstracts SQL-like operations and enables you to specify a backend engine, such as DuckDB, Polars, or Pandas. This allows for efficient in-memory data processing or seamless database querying without switching languages. Whether you’re dealing with small data or large-scale analytics, ibis scales beautifully.\nTo get started, explore this dplyr-to-ibis tutorial, which maps your familiar R syntax to ibis equivalents."
  },
  {
    "objectID": "blog/2024-11-28_transition_to_python/index.html#data-visualization-plotnine",
    "href": "blog/2024-11-28_transition_to_python/index.html#data-visualization-plotnine",
    "title": "Switching from R to Python: A Beginner’s Guide to Equivalent Tools",
    "section": "2.2 Data Visualization: plotnine",
    "text": "2.2 Data Visualization: plotnine\nggplot2 is beloved in the R community for its intuitive grammar of graphics, enabling users to create complex, layered visualizations with minimal effort. If you’ve relied on ggplot2 for your data storytelling, Python’s plotnine is your best friend.\nplotnine mirrors ggplot2’s syntax almost exactly. It supports layering plots with +, theming options, faceting for multi-panel plots, and customization of aesthetics. As a bonus, Python’s ecosystem integrates well with other visualization libraries, such as Matplotlib and Seaborn, for additional flexibility.\nDive into these plotnine tutorials to recreate your favorite ggplot2 visualizations in Python."
  },
  {
    "objectID": "blog/2024-11-28_transition_to_python/index.html#interactive-web-apps-shiny-for-python",
    "href": "blog/2024-11-28_transition_to_python/index.html#interactive-web-apps-shiny-for-python",
    "title": "Switching from R to Python: A Beginner’s Guide to Equivalent Tools",
    "section": "2.3 Interactive Web Apps: Shiny for Python",
    "text": "2.3 Interactive Web Apps: Shiny for Python\nShiny revolutionized how R users build interactive web applications with minimal code. The good news is that Shiny for Python brings the same simplicity to Python, letting you create interactive dashboards, data visualizations, and applications to showcase your work.\nShiny for Python follows a reactive programming paradigm, where outputs automatically update when inputs change. With Python’s robust backend options and Shiny’s UI capabilities, you can build powerful applications for both internal and external stakeholders. Whether you’re demonstrating a machine learning model or building a tool for non-technical audiences, Shiny has you covered.\nCheck out this Shiny for Python guide to start building your first app."
  },
  {
    "objectID": "blog/2024-11-28_transition_to_python/index.html#deploy-machine-learning-models-vetiver",
    "href": "blog/2024-11-28_transition_to_python/index.html#deploy-machine-learning-models-vetiver",
    "title": "Switching from R to Python: A Beginner’s Guide to Equivalent Tools",
    "section": "2.4 Deploy Machine Learning Models: Vetiver",
    "text": "2.4 Deploy Machine Learning Models: Vetiver\nDeploying machine learning models can often be complex and time-consuming. R’s vetiver package streamlines this process by creating APIs for your models, and Vetiver for Python does the same, making deployment accessible and consistent.\nWith Vetiver, you can deploy models built using scikit-learn, TensorFlow, PyTorch, or even custom algorithms. It generates prediction endpoints with minimal setup, allowing you to integrate your models into web applications, APIs, or automation workflows. This simplifies the journey from model development to production.\nLearn more about deploying models with vetiver in this comprehensive tutorial."
  },
  {
    "objectID": "blog/2024-11-28_transition_to_python/index.html#publishing-reports-quarto",
    "href": "blog/2024-11-28_transition_to_python/index.html#publishing-reports-quarto",
    "title": "Switching from R to Python: A Beginner’s Guide to Equivalent Tools",
    "section": "2.5 Publishing Reports: Quarto",
    "text": "2.5 Publishing Reports: Quarto\nR Markdown users transitioning to Python will be delighted to know that Quarto supports Python as well. Quarto extends the capabilities of R Markdown, enabling you to create HTML, PDF, and Word reports seamlessly. It even allows mixing code from Python, R, and Julia in the same document.\nQuarto offers numerous customization options, such as beautiful themes and dynamic content embedding, making it a versatile tool for technical reports, academic papers, and blog posts. As your projects grow, you can also use Quarto for building websites or books.\nExplore how to use Python with Quarto in this getting started guide."
  },
  {
    "objectID": "blog/2024-11-28_transition_to_python/index.html#an-ide-for-both-worlds-positron",
    "href": "blog/2024-11-28_transition_to_python/index.html#an-ide-for-both-worlds-positron",
    "title": "Switching from R to Python: A Beginner’s Guide to Equivalent Tools",
    "section": "2.6 An IDE for Both Worlds: Positron",
    "text": "2.6 An IDE for Both Worlds: Positron\nMany R users prefer RStudio for its clean, feature-rich interface. Fortunately, Positron, developed by Posit (formerly RStudio), provides a similar experience for Python. This integrated development environment (IDE) supports both R and Python, making it perfect for multi-language projects.\nWith Positron, you can enjoy a consistent environment for coding, debugging, and project management. Its features include a robust editor, version control integration, and support for Quarto documents. Download Positron from its GitHub repository and see how it complements your Python workflow."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Uppsala University\n\n\nPhD student Oct 2021 – Present\n\nPhD project: Analyzed the degree to which factors driving antimicrobial resistance (AMR) development and spread are addressed by AMR-specific policies and interventions versus broader societal approaches, particularly the Sustainable Development Goals (SDGs) agenda.\n\n\n\n\nPolicy gaps in integrating SDGs into AMR planning and implementation.\n\n\n\nINFLUX project (ERC grant No. 101039376): Contributed to research investigating and establishing database of drivers responsible for the emergence of emerging pests and pathogens, assessing governmental responses and their cascading social-ecological effects.\nSUSTAIN project: Conducted statistical analysis to evaluate the effectiveness of bundled healthcare interventions targeting health workers in Nepal, aimed at enhancing neonatal care quality\n\n\n\n\n\n\nApplied Epi\n\n\nInstructor, Coordinator Jun 2021 – Present\n\nInstructor: Provide R trainings for many epidemiologists worldwide. Provide public health services support including mentorship, outbreak response to AppliedEpi partners (i.e. Doctors Without Borders – MSF, Australian National University, WHO Philippines)\nTranslation Coordinator: Maintain translation packages and educational resources, including the Epi R Handbook (available in 10 languages), Epi Julia Handbook, slides, and tutorials\nCo-Author of Applied Epidemiology Manual (on-going project)\nLead translator the Vietnamese version of R for applied epidemiology and public health\n\n\n\n\nR training conducted by WHO for Philippines MoH.\n\n\n\n\n\n\n\nHanoi University of Public Health\n\n\nResearch Assistant, Center for Public Health and Ecosystem Research Jul 2018 – Jul 2020\nSupported three research projects on sanitation, food safety, and disease prevention in Vietnam, focusing on light-touch interventions in pig farms, parasite prevalence in ethnic communities, and assessing disease risk factors at the human-animal-environment interface, including biosecurity practices, animal movement, human-animal contact, and antimicrobial resistance\nMain responsibilities included:\n\nParticipated in the design, monitoring, and evaluation of interventions\nAssisted in collecting samples in field and preparing samples at laboratory\nCollected and analyzed data, wrote reports and manuscripts for publication\nConducted risk communication campaigns about food-borne diseases and food safety\nConducted project administrative tasks, i.e. contacting with local & international partners, applying ethical approval\n\n\n\n\nPresent research findings at Safepork conferece at Berlin 2019"
  },
  {
    "objectID": "cv.html#professional-experience",
    "href": "cv.html#professional-experience",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Uppsala University\n\n\nPhD student Oct 2021 – Present\n\nPhD project: Analyzed the degree to which factors driving antimicrobial resistance (AMR) development and spread are addressed by AMR-specific policies and interventions versus broader societal approaches, particularly the Sustainable Development Goals (SDGs) agenda.\n\n\n\n\nPolicy gaps in integrating SDGs into AMR planning and implementation.\n\n\n\nINFLUX project (ERC grant No. 101039376): Contributed to research investigating and establishing database of drivers responsible for the emergence of emerging pests and pathogens, assessing governmental responses and their cascading social-ecological effects.\nSUSTAIN project: Conducted statistical analysis to evaluate the effectiveness of bundled healthcare interventions targeting health workers in Nepal, aimed at enhancing neonatal care quality\n\n\n\n\n\n\nApplied Epi\n\n\nInstructor, Coordinator Jun 2021 – Present\n\nInstructor: Provide R trainings for many epidemiologists worldwide. Provide public health services support including mentorship, outbreak response to AppliedEpi partners (i.e. Doctors Without Borders – MSF, Australian National University, WHO Philippines)\nTranslation Coordinator: Maintain translation packages and educational resources, including the Epi R Handbook (available in 10 languages), Epi Julia Handbook, slides, and tutorials\nCo-Author of Applied Epidemiology Manual (on-going project)\nLead translator the Vietnamese version of R for applied epidemiology and public health\n\n\n\n\nR training conducted by WHO for Philippines MoH.\n\n\n\n\n\n\n\nHanoi University of Public Health\n\n\nResearch Assistant, Center for Public Health and Ecosystem Research Jul 2018 – Jul 2020\nSupported three research projects on sanitation, food safety, and disease prevention in Vietnam, focusing on light-touch interventions in pig farms, parasite prevalence in ethnic communities, and assessing disease risk factors at the human-animal-environment interface, including biosecurity practices, animal movement, human-animal contact, and antimicrobial resistance\nMain responsibilities included:\n\nParticipated in the design, monitoring, and evaluation of interventions\nAssisted in collecting samples in field and preparing samples at laboratory\nCollected and analyzed data, wrote reports and manuscripts for publication\nConducted risk communication campaigns about food-borne diseases and food safety\nConducted project administrative tasks, i.e. contacting with local & international partners, applying ethical approval\n\n\n\n\nPresent research findings at Safepork conferece at Berlin 2019"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": " Education",
    "text": "Education\n\n\n\n\n\nUppsala University\n\n\nDoctoral degree in Global Health Oct 2021 – Oct 2025\n\nThesis: Towards sustainable solutions for AMR: Targeting distal drivers through integrated SDG-policy and actions\n\n\n\n\n\n\nUniversity of Liège\n\n\nSpecialized Master in Integrated Management of Health Risks in the Global South Sep 2020 – Sep 2021\n\nGraduated with highest honor\nThesis: Operationalization and measurement of organizational adaptability: experiences from FRIENDSHIP, a non-governmental organization adapting to COVID-19 pandemic in Bangladesh\n\n\n\n\n\n\nHanoi Medical University\n\n\nBachelor of Public Health Sep 2013 – Sep 2017\n\nGraduated with high honor\nGraduation thesis: 9.5/10"
  },
  {
    "objectID": "cv.html#communities-and-organizations",
    "href": "cv.html#communities-and-organizations",
    "title": "Curriculum Vitae",
    "section": " Communities and Organizations",
    "text": "Communities and Organizations\n The Association of Vietnamese Science and Technology Experts in Sweden (AVISE)\n Applied Epi Community\n GlobeLife"
  },
  {
    "objectID": "cv.html#publication",
    "href": "cv.html#publication",
    "title": "Curriculum Vitae",
    "section": " Publication",
    "text": "Publication\nLink to my Google Scholar profile (14 peer-reviewed, h-index: 7, i10-index: 4, total number of citations: 110)\nSelected 5 articles:\n\nLuong NT, Didier W, Mats M, Tiscar G, Peter SJ (2024) Characterizing proximal and distal drivers of antimicrobial resistance: An umbrella review. Journal of Global Antimicrobial Resistance\nLuong NT, Didier W, Mats M, Peter SJ (2024) Characterizing proximal and distal drivers of antimicrobial resistance: An umbrella review. Journal of Global Antimicrobial Resistance\nPeter SJ, Luong NT and 10 co-authors (2024). Association between national action and trends in antibiotic resistance: an analysis of 73 countries from 2000 to 2023\nNgo HTH, Nguyen TL, and 6 co-authors (2021). Microbial contamination and associated risk factors in retailed pork from key value chains in Northern Vietnam. Food Microbiology\nLam S, Huyen NTT, Ngo HTH, Nguyen TL, and 6 co-authors (2021). Unpacking the Theory Behind One Health Food Safety Programs: A Vietnam Case Study. Frontiers in Veterinary Science"
  },
  {
    "objectID": "cv.html#academic-events-attendance",
    "href": "cv.html#academic-events-attendance",
    "title": "Curriculum Vitae",
    "section": " Academic events attendance",
    "text": "Academic events attendance\n\n2024 Oral presentation: The Nordic AMR Centre Conference, Tromsø, Norway\n2023 Poster presentation: EAR LTC-Sarea-ENLIGHT CONGRESS “Strengthening Antibiotic Resistance Networks”, Bordeaux, France\n2023 CIRCUS’ Interdisciplinary Summer School, Uppsala, Sweden\n2022 MIRAI 2.0 Research and Innovation week 2022, Fukuoka, Japan\n2022 NorDoc PhD Summit and summer school, Bergen Norway\n2022 SweDev PhD conference, Gothenburg, Sweden\n2019 Poster presentation: 13th SafePork 2019: One Health – Tear down interdisciplinary wall” conference, Berlin, Germany\n2019 Poster presentation: Regional symposium on research into smallholder pig production, and pork safety, Hanoi, Vietnam"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Curriculum Vitae",
    "section": " Skills",
    "text": "Skills\nProfessional skills\n\nStatistical & Machine Learning Frameworks: Extensive experience with generalized linear mixed models, time series analysis, spatial statistics, network analysis, clustering techniques, natural language processing, and large language models\nEpidemiological Methods: Skilled in conducting systematic reviews and meta-analyses, health policy analysis, household surveys, data collection (including in-depth interviews, focus group discussions), and risk communication.\nSoftware Engineering: Advanced proficiency in using programming language for statistical analysis (R, Python, STATA), product communication (R Markdown, Quarto, Shiny and Power BI), database management & query (SQL), version control (Git, Github Action), web framework (HTML, CSS, JS), package development (R, TypeScript).\nData Management: Proficiency in field/clinical data management using Open Data Kit (ODK) and RedCap\nLaboratory techniques: Familiar with laboratory microbiological and molecular techniques including Salmonella isolation, Total Bacteria Count, ELISA and PCR\nProject Management: Budget planning, M&E skills, proposal writing\n\nPersonal skills\n\nInterdisciplinary, adaptability, organized, independent, innovative, optimistic, constructive\n\nLanguages\n\nVietnamese, English, Swedish"
  },
  {
    "objectID": "projects/aetranslations.html",
    "href": "projects/aetranslations.html",
    "title": "{aetranslations}",
    "section": "",
    "text": "{pkgdown} site  Code  CRAN \nThis package hosts functions and guidelines for translating Applied Epi courses and resources. This includes courses materials (slides, exercises, tutorials), website (i.e. case studies) and books (i.e. the EpiRhandbook and the Applied Epi Manual). By leveraging AI services like DeepL or OpenAI, {aetranslations} package streamlines the process to make the Applied Epi materials available in multiple languages."
  },
  {
    "objectID": "projects/amr_trend.html",
    "href": "projects/amr_trend.html",
    "title": "AMR Global Trend",
    "section": "",
    "text": "App  Code\nThe ShinyApp is the supplementary materials for the article:\nAssociation between national policy and trends in antibiotic resistance: an analysis of 73 countries from 2000 to 2023\nPeter Søgaard Jørgensen1,2,3,, Luong Nguyen Thanh1,3, Ege Pehlivanoglu1, Franziska Klein1,#a, Didier Wernli4, Dusan Jasovsky5,#b, Athena Aktipis6, Rob R. Dunn7, Yrjo Gröhn8, Guillaume Lhermie8,#c, H. Morgan Scott9, Eili Y. Klein10,11\nAffiliations:\n1 Global Economic Dynamics and the Biosphere, The Royal Swedish Academy of Sciences, SE-114 18, Stockholm, Sweden\n2 Stockholm Resilience Centre, Stockholm University, SE-106 91, Stockholm, Sweden\n3 Uppsala Antibiotic Centre and Department of Women’s and Children’s Health, Uppsala University, SE-751 05, Uppsala, Sweden\n4 University of Geneva, Global Studies Institute, Transformative Governance Lab, CH-1211 Genève 4, Switzerland\n5 Uppsala University, ReAct Europe, SE-753 10, Uppsala, Sweden\n6 Arizona State University, Department of Psychology, Tempe, AZ 85281, USA\n7 North Carolina State University, Department of Applied Ecology, Raleigh, NC 27695-7617, USA\n8 Cornell University, College of Veterinary Medicine, Ithaca, NY 14853, USA\n9 Texas A&M University, College Station, TX 77843-4467, USA\n10 One Health Trust, Washington, D.C. 20015, USA\n11 Johns Hopkins School of Medicine, Department of Emergency Medicine, Baltimore, MD 21205, USA\n\n\n\nScreenshot from the App"
  },
  {
    "objectID": "talks/2024-11-17_avise_seminar_01/index.html",
    "href": "talks/2024-11-17_avise_seminar_01/index.html",
    "title": "How to keep information ‘fresh’",
    "section": "",
    "text": "Recorded\n\nDetails\n📆 November 17, 2024 // 2:00 pm - 3:00 pm CET\n🏨 Virtual\n🌠 The Association of Vietnamese Science and Technology Experts in Sweden\n\n\nAbstract\nWireless communication not only facilitates the transmission of information between mobile users but also serves as a stepping stone for delivering information to subsequent processing points in an intelligent system. In this context, the goal of communication extends beyond ensuring accuracy and continuity to maintaining the value of information. This value can be measured by the freshness of information.\nThe talk will introduce the concept of the “age of information”—a metric used to assess the freshness of information—and analyze this metric within a random multi-access system involving numerous devices. The content of the talk is designed for a general audience and does not require a technical background.\n\n\nScreenshot\n\n\n\n\n\nCitationBibTeX citation:@online{nguyen_thanh2024,\n  author = {Nguyen Thanh, Luong},\n  title = {How to Keep Information “Fresh”},\n  date = {2024-11-17},\n  url = {https://ntluong95.github.io/profile/talks/2024-11-17_avise_seminar_01/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nNguyen Thanh, Luong. 2024. “How to Keep Information\n‘Fresh’.” November 17, 2024. https://ntluong95.github.io/profile/talks/2024-11-17_avise_seminar_01/."
  }
]