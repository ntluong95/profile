[
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Upcoming\nNone at this time.\n\n\nPast\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to keep information â€˜youngâ€™\n\n\n\n\n\n\nCommunication\n\n\nTechnology\n\n\n\nThe talk will introduce the concept of the â€˜age of informationâ€™â€”a metric used to assess the freshness of informationâ€”and analyze this metric within a random multi-access system involving numerous devices\n\n\n\n\n\nNov 17, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "{aetranslations}\n\n\n\nR package\n\n\n\nThis package hosts functions and guidelines for translating Applied Epi courses and resources\n\n\n\nOct 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAMR Global Trend\n\n\n\nShiny app\n\n\n\nAMR Global Trend: A shiny app for exploring the global trend of AMR indicators from 73 countries\n\n\n\nAug 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAMR-SDGs interactions\n\n\n\nShiny app\n\n\n\nAMR-SDGs Explorer: A shiny app for exploring the interaction between AMR and SDGs agenda in national policies\n\n\n\nJun 20, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/amr_sdg.html",
    "href": "projects/amr_sdg.html",
    "title": "AMR-SDGs interactions",
    "section": "",
    "text": "App  Code\nThe ShinyApp is the supplementary materials for the article:\nâ€œWhen Global Health meets Global Goalsâ€: A comparative analysis the alignment between National Action Plans for AMR and Sustainable Development\nLuong Nguyen-Thanh1,2,4, Didier Wernli3, Mats MÃ¥lqvist1, Peter SÃ¸gaard JÃ¸rgensen1,4,5\nAffiliations:\n1 SWEDESD â€“ Sustainability Learning and Research Center, Department of Womenâ€™s and Childrenâ€™s Health, Uppsala University, Uppsala, Sweden\n2 Uppsala Antibiotic Centre (UAC), Uppsala University\n3 Global Studies Institute and Faculty of Science, University of Geneva, Geneva, Switzerland\n4 Global Economic Dynamics and the Biosphere, Royal Swedish Academy of Sciences, Stockholm, Sweden\n5 Stockholm Resilience Centre, Stockholm University, Stockholm, Sweden\n\n\n\nScreenshot from the App"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Luong Nguyen Thanh",
    "section": "",
    "text": "GitHub\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Research Profile\n  \n\n\n\n\n\n         \n\n\nHi there!\nWelcome to my website â€” Iâ€™m glad youâ€™re here!\nIâ€™m currently a PhD student at Uppsala University, following an academic path rooted in health and sustainability science, with degrees in Public Health (BPH) and One Health/Global Health (MSc).\nBeyond academia, I am passionate about programming, especially in R, with experience building packages, dashboards, Shiny apps, and working with large language models. My technical skills also extend to intermediate Python and foundational knowledge of SQL, HTML, CSS, and JavaScript. Iâ€™m especially interested in reproducibility and open science, and finding ways to streamline workflow.\nMy guiding principles in life are kindness, discipline, and integrity. I have found genuine fulfillment in working within the health and sustainability data science field, where I have the opportunity to engage with an inspiring community of like-minded professionals.\nIn my spare time, I love exploring history and diving into subjects related to religion and culture.\n\n\n\nGitHub Streak\n\n\n\n\n\n  \n\n\n  \n\n\n\nNote: Top languages is only a metric of the languages my public code consists of and doesnâ€™t reflect experience or skill level."
  },
  {
    "objectID": "index.html#hi-there",
    "href": "index.html#hi-there",
    "title": "Luong Nguyen Thanh",
    "section": "Hi there!",
    "text": "Hi there!\nWelcome to my website â€” Iâ€™m glad youâ€™re here!\nIâ€™m currently a PhD student at Uppsala University, following an academic path rooted in health and sustainability sciences, with degrees in Public Health (BPH) and One Health and Global Health (MSc).\nBeyond academia, I am passionate about programming, especially in R, with experience building packages, dashboards, Shiny apps, and working with large language models. My technical skills also extend to intermediate Python and foundational knowledge of SQL, HTML, CSS, and JavaScript. Iâ€™m especially interested in reproducibility and open science, and finding ways to streamline workflow.\nMy guiding principles in life are kindness, discipline, and integrity. I have found genuine fulfillment in working within the health and sustainability data science field, where I have the opportunity to engage with an inspiring community of like-minded professionals.\nIn my spare time, I love exploring history and diving into subjects related to religion and culture."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "The Data Alchemist",
    "section": "",
    "text": "â€œWelcome, adventurer! Iâ€™m thrilled youâ€™ve joined me on this journey through the world of data science and code. Here, Iâ€™ll share the discoveries Iâ€™m making, the projects Iâ€™m building, and the curiosities Iâ€™m exploringâ€”sprinkled with a little feline charm along the way. Letâ€™s learn and uncover insights together!â€\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPositron - a VSCode fork for Data Science\n\n\n\n\n\n\nPython\n\n\nR\n\n\nIDE\n\n\nTool reviews\n\n\n\nAt the end of June, the public beta version of Positron was released. Thatâ€™s almost 6 months ago, and the Positron team certainly hasnâ€™t been idle! So what happened over the last half year? And is it worth switching? ğŸ‘€ Some of my personal highlights about this IDE ğŸ‘‡ğŸ»\n\n\n\n\n\nDec 4, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nSwitching from R to Python: A Beginnerâ€™s Guide to Equivalent Tools\n\n\n\n\n\n\nPython\n\n\nR\n\n\nData Science\n\n\n\nLearn how to transition from R to Python seamlessly with tools that match your R favorites like dplyr, ggplot2, Shiny, and more.\n\n\n\n\n\nNov 28, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing a Python dataframe library as a dplyR useR\n\n\n\n\n\n\nR\n\n\nPython\n\n\n\n\n\n\n\n\n\nJan 25, 2023\n\n\n17 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html",
    "href": "blog/2024-11-01_dplyr_candidate/index.html",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "",
    "text": "Iâ€™m a long time R user and lately Iâ€™ve seen more and more signals that itâ€™s worth investing into Python. I use it for NLP with spaCy and to build functions on AWS Lambda. Further, there are many more data API libraries and machine learning libraries for Python than for R.\nAdopting Python means making choices on which libraries to invest time into learning. Manipulating data frames is one of the most common data science activities, so choosing the right library for it is key.\nMichael Chow, developer of siuba, a Python port of dplyr on top of pandas wrote describes the situation well:\nThe higher-level libraries he mentions come with a problem : Thereâ€™s no universal standard.\nIn a discussion of the polars library on Hacker News the user â€œcivilizedâ€ put the dplyr user perspective more bluntly:\nIâ€™m more willing to compromise though, so hereâ€™s a comparison of the strongest contenders."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#the-contenders",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#the-contenders",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "The contenders",
    "text": "The contenders\nThe database-like ops benchmark on H2Oai is a helpful performance comparison.\nIâ€™m considering these libraries:\n\nPandas: The most commonly used library and the one with the most tutorials and Stack Overflow answers available.\nsiuba: A port of dplyr to Python, built on top of pandas. Not in the benchmark. Performance probably similar to pandas or worse due to translation.\nPolars: The fastest library available. According to the benchmark, it runs 3-10x faster than Pandas.\nDuckdb: Use an in-memory OLAP database instead of a dataframe and write SQL. In R, this can also be queried via dbplyr.\nibis. Backend-agnostic wrapper for pandas and SQL engines.\n\nThere are more options. I excluded the others for these reasons:\n\nSlower than polars and not with a readability focus (dask, Arrow, Modin, pydatatable)\nRequires or is optmized for running on a remote server (Spark, ClickHouse and most other SQL databases).\nNot meant for OLAP (sqlite)\nNot in Python (DataFrames.jl)\nMeant for GPU (cuDF)"
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#github-stars-as-a-proxy-for-popularity",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#github-stars-as-a-proxy-for-popularity",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Github stars as a proxy for popularity",
    "text": "Github stars as a proxy for popularity\nThe benchmark provides a comparison of performance, but another important factor is popularity and maturity. A more mature library has a more stable API, better test coverage and there is more help available online, such as on StackOverflow. One way to measure popularity is the number of stars that the package repository has on Github.\n\nlibrary(ggplot2)\nlibs &lt;- data.frame(\n    library = c(\"pandas\", \"siuba\", \"polars\", \"duckdb\", \"dplyr\", \"data.table\", \"pydatatable\", \"dtplyr\", \"tidytable\", \"ibis\"),\n    language = c(\"Python\", \"Python\", \"Python\", \"SQL\", \"R\", \"R\", \"Python\", \"R\", \"R\", \"Python\"),\n    stars = c(32100, 732, 3900, 4100, 3900, 2900, 1400, 542, 285, 1600)\n)\n\nggplot(libs, aes(x = reorder(library, -stars), y = stars, fill = language)) +\n    geom_col() +\n    labs(\n        title = \"Pandas is by far the most popular choice\",\n        subtitle = \"Comparison of Github stars\",\n        fill = \"Language\",\n        x = \"Library\",\n        y = \"Github stars\"\n    )\n\n\n\n\n\n\n\n\nGithub stars are not a perfect proxy. For instance, dplyr is more mature than its star count suggests. Comparing the completeness of the documentation and tutorials for dplyr and polars reveals that itâ€™s a day and night difference.\nWith the quantitative comparison out of the way, hereâ€™s a qualitative comparison of the Python packages. Iâ€™m speaking of my personal opinion of these packages - not a general comparison. My reference is my current use of dplyr in R. When I need more performance, I use tidytable to get most of the speed of data.table with the grammar of dplyr and eager evaluation. Another alternative is dtplyr, which translates dplyr to data.table with lazy evaluation. I also use dbplyr, which translates dplyr to SQL.\nIâ€™ll compare the libraries by running a data transformation pipeline involving import from CSV, mutate, filter, sort, join, group by and summarize. Iâ€™ll use the nycflights13 dataset, which is featured in Hadley Wickhamâ€™s R for Data Science."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#dplyr-reference-in-r",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#dplyr-reference-in-r",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "dplyr: Reference in R",
    "text": "dplyr: Reference in R\nLetâ€™s start with a reference implementation in dplyr. The dataset is available as a package, so I skip the CSV import.\n\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(nycflights13)\nlibrary(reactable)\n\n# Take a look at the tables\nreactable(head(flights, 10))\n\n\n\n\nreactable(head(airlines, 10))\n\n\n\n\n\nThe flights tables has 336776 rows, one for each flight of an airplane. The airlines table has 16 rows, one for each airline mapping the full name of the company to a code.\nLetâ€™s find the airline with the highest arrival delays in January 2013.\n\nflights |&gt;\n    filter(year == 2013, month == 1, !is.na(arr_delay)) |&gt;\n    mutate(arr_delay = replace(arr_delay, arr_delay &lt; 0, 0)) |&gt;\n    left_join(airlines, by = \"carrier\") |&gt;\n    group_by(airline = name) |&gt;\n    summarise(flights = n(), mean_delay = mean(arr_delay)) |&gt;\n    arrange(desc(mean_delay))\n\n# A tibble: 16 Ã— 3\n   airline                     flights mean_delay\n   &lt;chr&gt;                         &lt;int&gt;      &lt;dbl&gt;\n 1 SkyWest Airlines Inc.             1     107   \n 2 Hawaiian Airlines Inc.           31      48.8 \n 3 ExpressJet Airlines Inc.       3964      29.6 \n 4 Frontier Airlines Inc.           59      23.9 \n 5 Mesa Airlines Inc.               39      20.4 \n 6 Endeavor Air Inc.              1480      19.3 \n 7 Alaska Airlines Inc.             62      17.6 \n 8 Envoy Air                      2203      14.3 \n 9 Southwest Airlines Co.          985      13.0 \n10 JetBlue Airways                4413      12.9 \n11 United Air Lines Inc.          4590      11.9 \n12 American Airlines Inc.         2724      11.0 \n13 AirTran Airways Corporation     324       9.95\n14 US Airways Inc.                1554       9.11\n15 Delta Air Lines Inc.           3655       8.07\n16 Virgin America                  314       3.17\n\n\nSome values in arr_delay are negative, indicating that the flight was faster than expected. I replaced these values with 0 because I donâ€™t want them to cancel out delays of other flights. I joined to the airlines table to get the full names of the airlines.\nI export the flights and airlines tables to CSV to hand them over to Python.\n\n# Write to temporary files\nflights_path &lt;- tempfile(fileext = \".csv\")\nairlines_path &lt;- tempfile(fileext = \".csv\")\n\ndata.table::fwrite(flights, flights_path, row.names = FALSE)\ndata.table::fwrite(airlines, airlines_path, row.names = FALSE)\n\nTo access the file from Python, the path is handed over:\n\n\n#| eval: false\n# Hand over the path from R\nflights_path = r[\"flights_path\"]\nairlines_path = r[\"airlines_path\"]\n\nFor more details on how this works with the reticulate package, check this documentation."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#pandas-most-popular",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#pandas-most-popular",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Pandas: Most popular",
    "text": "Pandas: Most popular\nThe following sections follow a pattern: read in from CSV, then build a query.\n\nimport pandas as pd\n\n# Import from CSV\nflights_pd = pd.read_csv(flights_path)\nairlines_pd = pd.read_csv(airlines_path)\n\npandas.read_csv reads the header and conveniently infers the column types.\n\n(\n    flights_pd.query(\"year == 2013 & month == 1 & arr_delay.notnull()\")\n    .assign(arr_delay=flights_pd.arr_delay.clip(lower=0))\n    .merge(airlines_pd, how=\"left\", on=\"carrier\")\n    .rename(columns={\"name\": \"airline\"})\n    .groupby(\"airline\")\n    .agg(flights=(\"airline\", \"count\"), mean_delay=(\"arr_delay\", \"mean\"))\n    .sort_values(by=\"mean_delay\", ascending=False)\n)\n\n                             flights  mean_delay\nairline                                         \nSkyWest Airlines Inc.              1  107.000000\nHawaiian Airlines Inc.            31   48.774194\nExpressJet Airlines Inc.        3964   29.642785\nFrontier Airlines Inc.            59   23.881356\nMesa Airlines Inc.                39   20.410256\nEndeavor Air Inc.               1480   19.321622\nAlaska Airlines Inc.              62   17.645161\nEnvoy Air                       2203   14.303677\nSouthwest Airlines Co.           985   12.964467\nJetBlue Airways                 4413   12.919329\nUnited Air Lines Inc.           4590   11.851852\nAmerican Airlines Inc.          2724   10.953377\nAirTran Airways Corporation      324    9.953704\nUS Airways Inc.                 1554    9.111326\nDelta Air Lines Inc.            3655    8.070315\nVirgin America                   314    3.165605\n\n\nI chose to use the pipeline syntax from pandas - another option is to modify the dataset in place. That has a lower memory footprint, but canâ€™t be run repeatedly for the same result, such as in interactive use in a notebook.\nHere, the query() function is slightly awkward with the long string argument. The groupby doesnâ€™t allow renaming on the fly like dplyr, though I donâ€™t consider that a real drawback. Perhaps itâ€™s clearer to rename explicitly anyway.\nPandas has the widest API, offering hundreds of functions for every conceivable manipulation. The clip function used here is one such example. One difference to dplyr is that pandas uses its own methods .mean(), rather than using external ones such as base::mean(). That means using custom functions instead carries a performance penalty.\nAs weâ€™ll see later, pandas is the backend for siuba and ibis, which boil down to pandas code.\nOne difference to all other discussed solutions is that pandas uses a row index. Base R also has this with row names, but the tidyverse and tibbles have largely removed them from common use. I never missed row names. At the times I had to work with them in pandas they were more confusing than helpful. The documentation of polars puts it more bluntly:\n\nNo index. They are not needed. Not having them makes things easier. Convince me otherwise\n\nThatâ€™s quite passive aggressive, but I do agree and wish pandas didnâ€™t have it."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#siuba-dplyr-in-python",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#siuba-dplyr-in-python",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "siuba: dplyr in Python",
    "text": "siuba: dplyr in Python\n\nimport siuba as si\n\n# Import from CSV\nflights_si = pd.read_csv(r[\"flights_path\"])\nairlines_si = pd.read_csv(r[\"airlines_path\"])\n\nAs siuba is just an alternative way of writing some pandas commands, we read the data just like in the pandas implementation.\n\n(\n    flights_si\n    &gt;&gt; si.filter(si._.year == 2013, si._.month == 1, si._.arr_delay.notnull())\n    &gt;&gt; si.mutate(arr_delay=si._.arr_delay.clip(lower=0))\n    &gt;&gt; si.left_join(si._, airlines_si, on=\"carrier\")\n    &gt;&gt; si.rename(airline=si._.name)\n    &gt;&gt; si.group_by(si._.airline)\n    &gt;&gt; si.summarize(flights=si._.airline.count(), mean_delay=si._.arr_delay.mean())\n    &gt;&gt; si.arrange(-si._.mean_delay)\n)\n\nI found siuba the easiest to work with. Once I understood the _ placeholder for a table of data, I could write it almost as fast as dplyr. Out of all the ways to refer to a column in a data frame, I found it to be the most convenient, because it doesnâ€™t require me to spell out the name of the data frame over and over. While not as elegant as dplyrâ€™s tidy evaluation (discussed at the end of the article), it avoids the ambivalence in dplyr where it can be unclear whether a name refers to a column or an outside object.\nItâ€™s always possible to drop into pandas, such as for the aggregation functions which use the mean() and count() methods of the pandas series. The &gt;&gt; is an easy replacement for the %&gt;% magrittr pipe or |&gt; base pipe in R.\nThe author advertises siuba like this (from the docs):\n\nSiuba is a library for quick, scrappy data analysis in Python. It is a port of dplyr, tidyr, and other R Tidyverse libraries.\n\nA way for dplyr users to quickly hack away at data analysis in Python, but not meant for unsupervised production use."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#polars-fastest",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#polars-fastest",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Polars: Fastest",
    "text": "Polars: Fastest\nPolars is written in Rust and also offers a Python API. It comes in two flavors: eager and lazy. Lazy evaluation is similar to how dbplyr and dtplyr work: until asked, nothing is evaluated. This enables performance gains by reordering the commands being executed. But itâ€™s a little less convenient for interactive analysis. Iâ€™ll use the eager API here.\n\nimport polars as pl\n\n# Import from CSV\nflights_pl = pl.read_csv(flights_path)\nairlines_pl = pl.read_csv(airlines_path)\n\n\n(\n    flights_pl.filter((pl.col(\"year\") == 2013) & (pl.col(\"month\") == 1))\n    .drop_nulls(\"arr_delay\")\n    .join(airlines_pl, on=\"carrier\", how=\"left\")\n    .with_columns(\n        [\n            pl.when(pl.col(\"arr_delay\") &gt; 0)\n            .then(pl.col(\"arr_delay\"))\n            .otherwise(0)\n            .alias(\"arr_delay\"),\n            pl.col(\"name\").alias(\"airline\"),\n        ]\n    )\n    .groupby(\"airline\")\n    .agg(\n        [pl.count(\"airline\").alias(\"flights\"), pl.mean(\"arr_delay\").alias(\"mean_delay\")]\n    )\n    .sort(\"mean_delay\", descending=True)\n)\n\nThe API is leaner than pandas, requiring to memorize fewer functions and patterns. Though this can also be seen as less feature-complete. Pandas, for example has a dedicated clip function.\nThere isnâ€™t nearly as much help available for problems with polars as for with pandas. While the documentation is good, it canâ€™t answer every question and lots of trial and error is needed.\nA comparison of polars and pandas is available in the polars documentation."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#duckdb-highly-compatible-and-easy-for-sql-users",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#duckdb-highly-compatible-and-easy-for-sql-users",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "DuckDB: Highly compatible and easy for SQL users",
    "text": "DuckDB: Highly compatible and easy for SQL users\n\nimport duckdb\n\ncon_duckdb = duckdb.connect(database=\":memory:\")\n\n# Import from CSV\ncon_duckdb.execute(\n    \"CREATE TABLE 'flights' AS \"\n    f\"SELECT * FROM read_csv_auto('{flights_path}', header = True);\"\n    \"CREATE TABLE 'airlines' AS \"\n    f\"SELECT * FROM read_csv_auto('{airlines_path}', header = True);\"\n)\n\n&lt;duckdb.duckdb.DuckDBPyConnection object at 0x0000014F23081730&gt;\n\n\nDuckDBâ€™s read_csv_auto() works just like the csv readers in Python.\n\ncon_duckdb.execute(\n    \"WITH flights_clipped AS ( \"\n    \"SELECT carrier, CASE WHEN arr_delay &gt; 0 THEN arr_delay ELSE 0 END AS arr_delay \"\n    \"FROM flights \"\n    \"WHERE year = 2013 AND month = 1 AND arr_delay IS NOT NULL\"\n    \")\"\n    \"SELECT name AS airline, COUNT(*) AS flights, AVG(arr_delay) AS mean_delay \"\n    \"FROM flights_clipped \"\n    \"LEFT JOIN airlines ON flights_clipped.carrier = airlines.carrier \"\n    \"GROUP BY name \"\n    \"ORDER BY mean_delay DESC \"\n).fetchdf()\n\n                        airline  flights  mean_delay\n0         SkyWest Airlines Inc.        1  107.000000\n1        Hawaiian Airlines Inc.       31   48.774194\n2      ExpressJet Airlines Inc.     3964   29.642785\n3        Frontier Airlines Inc.       59   23.881356\n4            Mesa Airlines Inc.       39   20.410256\n5             Endeavor Air Inc.     1480   19.321622\n6          Alaska Airlines Inc.       62   17.645161\n7                     Envoy Air     2203   14.303677\n8        Southwest Airlines Co.      985   12.964467\n9               JetBlue Airways     4413   12.919329\n10        United Air Lines Inc.     4590   11.851852\n11       American Airlines Inc.     2724   10.953377\n12  AirTran Airways Corporation      324    9.953704\n13              US Airways Inc.     1554    9.111326\n14         Delta Air Lines Inc.     3655    8.070315\n15               Virgin America      314    3.165605\n\n\nThe performance is closer to polars than to pandas. A big plus is the ability to handle larger than memory data.\nDuckDB can also operate directly on a pandas dataframe. The SQL code is portable to R, C, C++, Java and other programming languages the duckdb has APIs. Itâ€™s also portable when the logic is taken to a DB like Postgres, or Clickhouse, or is ported to an ETL framework like DBT.\nThis stands in contrast to polars and pandas code, which has to be rewritten from scratch. It also means that the skill gained in manipulating SQL translates well to other situations. SQL has been around for more than 50 years - learning SQL is future-proofing a career.\nWhile these are big plusses, duckdb isnâ€™t so convenient for interactive data exploration. SQL isnâ€™t as composeable. Composing SQL queries requires many common table expressions (CTEs, WITH x AS (SELECT ...)). Reusing them for other queries is not as easy as with Python. SQL is typically less expressive than Python. It lacks shorthands and itâ€™s awkward when there are many columns. Itâ€™s also harder to write custom functions in SQL than in R or Python. This is the motivation for using libraries like pandas and dplyr. But SQL can actually do a surprising amount of things, as database expert Haki Benita explained in a detailed article.\nOr in short, from the documentation of ibis:\n\nSQL is widely used and very convenient when writing simple queries. But as the complexity of operations grow, SQL can become very difficult to deal with.\n\nThen, thereâ€™s the issue of how to actually write the SQL code. Writing strings rather than actual Python is awkward and many editors donâ€™t provide syntax highlighting within the strings (Jetbrains editors like PyCharm and DataSpell do). The other option is writing .sql that have placeholders for parameters. Thatâ€™s cleaner and allows using a linter, but is inconvenient for interactive use.\nSQL is inherently lazily executed, because the query planner needs to take the whole query into account before starting computation. This enables performance gains. For interactive use, lazy evaluation is less convenient, because one canâ€™t see the intermediate results at each step. Speed of iteration is critical: the faster one can iterate, the more hypotheses about the data can be tested.\nThere is a programmatic way to construct queries for duckdb, designed to provide a dbplyr alternative in Python. Unfortunately its documentation is sparse.\nUsing duckdb without pandas doesnâ€™t seem feasible for exploratory data analysis, because graphing packages like seaborn and plotly expect a pandas data frame or similar as an input."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#ibis-lingua-franca-in-python",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#ibis-lingua-franca-in-python",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "ibis: Lingua franca in Python",
    "text": "ibis: Lingua franca in Python\nThe goal of ibis is to provide a universal language for working with data frames in Python, regardless of the backend that is used. Itâ€™s tagline is: Write your analytics code once, run in everywhere. This is similar to how dplyr can use SQL as a backend with dbplyr and data.table with dtplyr.\nAmong others, Ibis supports pandas, PostgreSQL and SQLite as backends. Unfortunately duckdb is not an available backend, because the authors of duckdb have decided against building on ibis.\nThe ibis project aims to bridge the gap between the needs of interactive data analysis and the capabilities of SQL, which I have detailed in the previous section on duckdb.\n\n\n\n\n\n\nNote\n\n\n\nUPDATE October 2023\n\nDuckdb is now a supported backend (along with many more). So performance is going to be very similar to duckdb.\nDirectly load/save data\njoin(), clip(), and case() are well-supported\nIbis is much more popular and now very actively maintained. There are more examples, better documentation, and community. Still definitely less than pandas, but perhaps comparable to polars.\n\nThanks to NickCrews for providing this update, including the following code example.\n\n\nFor the test drive, Iâ€™ll use the duckdb backend, meaning that the ibis code is translated to duckdb operations, similar to how siuba is translated to pandas. This gives ibis the blazing speed of duckdb.\n\nimport ibis\nfrom ibis import _\n\nflights_ib_csv = pd.read_csv(flights_path)\nairlines_ib_csv = pd.read_csv(airlines_path)\n\nibis.options.interactive = True\n\nflights_ib = ibis.read_csv(flights_path)\nairlines_ib = ibis.read_csv(airlines_path)\nflights_ib\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”\nâ”‚ year  â”‚ month â”‚ day   â”‚ dep_time â”‚ sched_dep_time â”‚ dep_delay â”‚ arr_time â”‚  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”¤\nâ”‚ int64 â”‚ int64 â”‚ int64 â”‚ int64    â”‚ int64          â”‚ int64     â”‚ int64    â”‚  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”¤\nâ”‚  2013 â”‚     1 â”‚     1 â”‚      517 â”‚            515 â”‚         2 â”‚      830 â”‚  â”‚\nâ”‚  2013 â”‚     1 â”‚     1 â”‚      533 â”‚            529 â”‚         4 â”‚      850 â”‚  â”‚\nâ”‚  2013 â”‚     1 â”‚     1 â”‚      542 â”‚            540 â”‚         2 â”‚      923 â”‚  â”‚\nâ”‚  2013 â”‚     1 â”‚     1 â”‚      544 â”‚            545 â”‚        -1 â”‚     1004 â”‚  â”‚\nâ”‚  2013 â”‚     1 â”‚     1 â”‚      554 â”‚            600 â”‚        -6 â”‚      812 â”‚  â”‚\nâ”‚  2013 â”‚     1 â”‚     1 â”‚      554 â”‚            558 â”‚        -4 â”‚      740 â”‚  â”‚\nâ”‚  2013 â”‚     1 â”‚     1 â”‚      555 â”‚            600 â”‚        -5 â”‚      913 â”‚  â”‚\nâ”‚  2013 â”‚     1 â”‚     1 â”‚      557 â”‚            600 â”‚        -3 â”‚      709 â”‚  â”‚\nâ”‚  2013 â”‚     1 â”‚     1 â”‚      557 â”‚            600 â”‚        -3 â”‚      838 â”‚  â”‚\nâ”‚  2013 â”‚     1 â”‚     1 â”‚      558 â”‚            600 â”‚        -2 â”‚      753 â”‚  â”‚\nâ”‚     â€¦ â”‚     â€¦ â”‚     â€¦ â”‚        â€¦ â”‚              â€¦ â”‚         â€¦ â”‚        â€¦ â”‚  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”˜\n\n\nNon-interactive ibis means that queries are evaluated lazily.\n\n(\n    flights_ib.filter(\n        [\n            _.year == 2013,\n            _.month == 1,\n            _.arr_delay.notnull(),\n        ]\n    )\n    .join(airlines_ib, \"carrier\", how=\"left\")\n    .select(arr_delay=_.arr_delay.clip(lower=0), airline=_.name)\n    .group_by(\"airline\")\n    .agg(flights=_.count(), mean_delay=_.arr_delay.mean())\n    .order_by(_.mean_delay.desc())\n)\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ airline                  â”‚ flights â”‚ mean_delay â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ string                   â”‚ int64   â”‚ float64    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ SkyWest Airlines Inc.    â”‚       1 â”‚ 107.000000 â”‚\nâ”‚ Hawaiian Airlines Inc.   â”‚      31 â”‚  48.774194 â”‚\nâ”‚ ExpressJet Airlines Inc. â”‚    3964 â”‚  29.642785 â”‚\nâ”‚ Frontier Airlines Inc.   â”‚      59 â”‚  23.881356 â”‚\nâ”‚ Mesa Airlines Inc.       â”‚      39 â”‚  20.410256 â”‚\nâ”‚ Endeavor Air Inc.        â”‚    1480 â”‚  19.321622 â”‚\nâ”‚ Alaska Airlines Inc.     â”‚      62 â”‚  17.645161 â”‚\nâ”‚ Envoy Air                â”‚    2203 â”‚  14.303677 â”‚\nâ”‚ Southwest Airlines Co.   â”‚     985 â”‚  12.964467 â”‚\nâ”‚ JetBlue Airways          â”‚    4413 â”‚  12.919329 â”‚\nâ”‚ â€¦                        â”‚       â€¦ â”‚          â€¦ â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nThe syntax looks quite similar to dplyr and the versatility of interchangeable backends is remarkable. In the first version of this article, ibis was lacking in documentation and had some rough edges in the API, but these were improved in the meantime."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#conclusion",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#conclusion",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Conclusion",
    "text": "Conclusion\nItâ€™s not a clear-cut choice. None of the options offer a syntax that is as convenient for interactive analysis as dplyr. siuba is the closest to it, but dplyr still has an edge with tidy evaluation, letting users refer to columns in a data frame by their names (colname) directly, without any wrappers. But Iâ€™ve also seen it be confusing for newbies to R that mix it up with base Râ€™s syntax. Itâ€™s also harder to program with, where itâ€™s necessary to use operators like { } and :=.\nMy appreciation for dplyr (and the closely associated tidyr) grew during this research. Not only is it a widely accepted standard like pandas, it can also be used as a translation layer for backends like SQL databases (including duckdb), data.table, and Spark. All while having the most elegant and flexible syntax available.\nPersonally, Iâ€™ll primarily leverage SQL and a OLAP database (such as Clickhouse or Snowflake) running on a server to do the heavy lifting. For steps that are better done locally, Iâ€™ll use pandas for maximum compatibility. I find the use of an index inconvenient, but thereâ€™s so much online help available on StackOverflow. Github Copilot also deserves a mention for making it easier to pick up. Other use cases can be very different, so I donâ€™t mean to say that my way is the best. For instance, if the data is not already on a server, fast local processing with polars may be best.\nMost data science work happens in a team. Choosing a library that all team members are familiar with is critical for collaboration. That is typically SQL, pandas or dplyr. The performance gains from using a less common library like polars have to be weighed against the effort spent learning the syntax as well as the increased likelihood of bugs, when beginners write in a new syntax.\nRelated articles:\n\nPolars: the fastest DataFrame library youâ€™ve never heard of\nWhat would it take to recreate dplyr in python?\nPandas has a hard job (and does it well)\ndplyr in Python? First impressions of the siuba module\nAn Overview of Pythonâ€™s Datatable package\nDiscussion of DuckDB on Hacker News\nDiscussion of Polars on Hacker News\nPractical SQL for Data Analysis"
  },
  {
    "objectID": "blog/2024-11-28_transition_to_python/index.html",
    "href": "blog/2024-11-28_transition_to_python/index.html",
    "title": "Switching from R to Python: A Beginnerâ€™s Guide to Equivalent Tools",
    "section": "",
    "text": "Transitioning from R to Python can feel like a daunting leap, especially if youâ€™ve grown comfortable with Râ€™s ecosystem. The good news? Python offers several tools and libraries that mimic the syntax and functionality of your favorite R packages. Letâ€™s explore these equivalents to ease your journey."
  },
  {
    "objectID": "blog/2024-11-28_transition_to_python/index.html#data-manipulation-ibis",
    "href": "blog/2024-11-28_transition_to_python/index.html#data-manipulation-ibis",
    "title": "Switching from R to Python: A Beginnerâ€™s Guide to Equivalent Tools",
    "section": "2.1 Data Manipulation: ibis",
    "text": "2.1 Data Manipulation: ibis\nIn R, dplyr and dbplyr are go-to packages for data manipulation, offering a clean, declarative syntax to filter, mutate, summarize, and join datasets. Pythonâ€™s ibis serves as an excellent alternative, providing a similar experience for working with structured data.\nWhat sets ibis apart is its performance optimization. It abstracts SQL-like operations and enables you to specify a backend engine, such as DuckDB, Polars, or Pandas. This allows for efficient in-memory data processing or seamless database querying without switching languages. Whether youâ€™re dealing with small data or large-scale analytics, ibis scales beautifully.\nTo get started, explore this dplyr-to-ibis tutorial, which maps your familiar R syntax to ibis equivalents."
  },
  {
    "objectID": "blog/2024-11-28_transition_to_python/index.html#data-visualization-plotnine",
    "href": "blog/2024-11-28_transition_to_python/index.html#data-visualization-plotnine",
    "title": "Switching from R to Python: A Beginnerâ€™s Guide to Equivalent Tools",
    "section": "2.2 Data Visualization: plotnine",
    "text": "2.2 Data Visualization: plotnine\nggplot2 is beloved in the R community for its intuitive grammar of graphics, enabling users to create complex, layered visualizations with minimal effort. If youâ€™ve relied on ggplot2 for your data storytelling, Pythonâ€™s plotnine is your best friend.\nplotnine mirrors ggplot2â€™s syntax almost exactly. It supports layering plots with +, theming options, faceting for multi-panel plots, and customization of aesthetics. As a bonus, Pythonâ€™s ecosystem integrates well with other visualization libraries, such as Matplotlib and Seaborn, for additional flexibility.\nDive into these plotnine tutorials to recreate your favorite ggplot2 visualizations in Python."
  },
  {
    "objectID": "blog/2024-11-28_transition_to_python/index.html#interactive-web-apps-shiny-for-python",
    "href": "blog/2024-11-28_transition_to_python/index.html#interactive-web-apps-shiny-for-python",
    "title": "Switching from R to Python: A Beginnerâ€™s Guide to Equivalent Tools",
    "section": "2.3 Interactive Web Apps: Shiny for Python",
    "text": "2.3 Interactive Web Apps: Shiny for Python\nShiny revolutionized how R users build interactive web applications with minimal code. The good news is that Shiny for Python brings the same simplicity to Python, letting you create interactive dashboards, data visualizations, and applications to showcase your work.\nShiny for Python follows a reactive programming paradigm, where outputs automatically update when inputs change. With Pythonâ€™s robust backend options and Shinyâ€™s UI capabilities, you can build powerful applications for both internal and external stakeholders. Whether youâ€™re demonstrating a machine learning model or building a tool for non-technical audiences, Shiny has you covered.\nCheck out this Shiny for Python guide to start building your first app."
  },
  {
    "objectID": "blog/2024-11-28_transition_to_python/index.html#deploy-machine-learning-models-vetiver",
    "href": "blog/2024-11-28_transition_to_python/index.html#deploy-machine-learning-models-vetiver",
    "title": "Switching from R to Python: A Beginnerâ€™s Guide to Equivalent Tools",
    "section": "2.4 Deploy Machine Learning Models: Vetiver",
    "text": "2.4 Deploy Machine Learning Models: Vetiver\nDeploying machine learning models can often be complex and time-consuming. Râ€™s vetiver package streamlines this process by creating APIs for your models, and Vetiver for Python does the same, making deployment accessible and consistent.\nWith Vetiver, you can deploy models built using scikit-learn, TensorFlow, PyTorch, or even custom algorithms. It generates prediction endpoints with minimal setup, allowing you to integrate your models into web applications, APIs, or automation workflows. This simplifies the journey from model development to production.\nLearn more about deploying models with vetiver in this comprehensive tutorial."
  },
  {
    "objectID": "blog/2024-11-28_transition_to_python/index.html#publishing-reports-quarto",
    "href": "blog/2024-11-28_transition_to_python/index.html#publishing-reports-quarto",
    "title": "Switching from R to Python: A Beginnerâ€™s Guide to Equivalent Tools",
    "section": "2.5 Publishing Reports: Quarto",
    "text": "2.5 Publishing Reports: Quarto\nR Markdown users transitioning to Python will be delighted to know that Quarto supports Python as well. Quarto extends the capabilities of R Markdown, enabling you to create HTML, PDF, and Word reports seamlessly. It even allows mixing code from Python, R, and Julia in the same document.\nQuarto offers numerous customization options, such as beautiful themes and dynamic content embedding, making it a versatile tool for technical reports, academic papers, and blog posts. As your projects grow, you can also use Quarto for building websites or books.\nExplore how to use Python with Quarto in this getting started guide."
  },
  {
    "objectID": "blog/2024-11-28_transition_to_python/index.html#an-ide-for-both-worlds-positron",
    "href": "blog/2024-11-28_transition_to_python/index.html#an-ide-for-both-worlds-positron",
    "title": "Switching from R to Python: A Beginnerâ€™s Guide to Equivalent Tools",
    "section": "2.6 An IDE for Both Worlds: Positron",
    "text": "2.6 An IDE for Both Worlds: Positron\nMany R users prefer RStudio for its clean, feature-rich interface. Fortunately, Positron, developed by Posit (formerly RStudio), provides a similar experience for Python. This integrated development environment (IDE) supports both R and Python, making it perfect for multi-language projects.\nWith Positron, you can enjoy a consistent environment for coding, debugging, and project management. Its features include a robust editor, version control integration, and support for Quarto documents. Download Positron from its GitHub repository and see how it complements your Python workflow."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Uppsala University\n\n\nPhD student Oct 2021 â€“ Present\n\nPhD project: Analyzed the degree to which factors driving antimicrobial resistance (AMR) development and spread are addressed by AMR-specific policies and interventions versus broader societal approaches, particularly the Sustainable Development Goals (SDGs) agenda.\n\n\n\n\n\n\n\n\n\n\n\n\nINFLUX project (ERC grant No.Â 101039376): Contributed to research investigating and establishing database of drivers responsible for the emergence of emerging pests and pathogens, assessing governmental responses and their cascading social-ecological effects.\nSUSTAIN project: Conducted statistical analysis to evaluate the effectiveness of bundled healthcare interventions targeting health workers in Nepal, aimed at enhancing neonatal care quality\n\n\n\n\n\n\nApplied Epi\n\n\nInstructor, Coordinator Jun 2021 â€“ Present\n\nInstructor: Provide R trainings for many epidemiologists worldwide. Provide public health services support including mentorship, outbreak response to AppliedEpi partners (i.e.Â Doctors Without Borders â€“ MSF, Australian National University, WHO Philippines)\nTranslation Coordinator: Maintain translation packages and educational resources, including the Epi R Handbook (available in 10 languages), Epi Julia Handbook, slides, and tutorials\nCo-Author of Applied Epidemiology Manual (on-going project)\nLead translator the Vietnamese version of R for applied epidemiology and public health\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHanoi University of Public Health\n\n\nResearch Assistant, Center for Public Health and Ecosystem Research Jul 2018 â€“ Jul 2020\nSupported three research projects on sanitation, food safety, and disease prevention in Vietnam, focusing on light-touch interventions in pig farms, parasite prevalence in ethnic communities, and assessing disease risk factors at the human-animal-environment interface, including biosecurity practices, animal movement, human-animal contact, and antimicrobial resistance\nMain responsibilities included:\n\nParticipated in the design, monitoring, and evaluation of interventions\nAssisted in collecting samples in field and preparing samples at laboratory\nCollected and analyzed data, wrote reports and manuscripts for publication\nConducted risk communication campaigns about food-borne diseases and food safety\nConducted project administrative tasks, i.e.Â contacting with local & international partners, applying ethical approval"
  },
  {
    "objectID": "cv.html#professional-experience",
    "href": "cv.html#professional-experience",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Uppsala University\n\n\nPhD student Oct 2021 â€“ Present\n\nPhD project: Analyzed the degree to which factors driving antimicrobial resistance (AMR) development and spread are addressed by AMR-specific policies and interventions versus broader societal approaches, particularly the Sustainable Development Goals (SDGs) agenda.\n\n\n\n\n\n\n\n\n\n\n\n\nINFLUX project (ERC grant No.Â 101039376): Contributed to research investigating and establishing database of drivers responsible for the emergence of emerging pests and pathogens, assessing governmental responses and their cascading social-ecological effects.\nSUSTAIN project: Conducted statistical analysis to evaluate the effectiveness of bundled healthcare interventions targeting health workers in Nepal, aimed at enhancing neonatal care quality\n\n\n\n\n\n\nApplied Epi\n\n\nInstructor, Coordinator Jun 2021 â€“ Present\n\nInstructor: Provide R trainings for many epidemiologists worldwide. Provide public health services support including mentorship, outbreak response to AppliedEpi partners (i.e.Â Doctors Without Borders â€“ MSF, Australian National University, WHO Philippines)\nTranslation Coordinator: Maintain translation packages and educational resources, including the Epi R Handbook (available in 10 languages), Epi Julia Handbook, slides, and tutorials\nCo-Author of Applied Epidemiology Manual (on-going project)\nLead translator the Vietnamese version of R for applied epidemiology and public health\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHanoi University of Public Health\n\n\nResearch Assistant, Center for Public Health and Ecosystem Research Jul 2018 â€“ Jul 2020\nSupported three research projects on sanitation, food safety, and disease prevention in Vietnam, focusing on light-touch interventions in pig farms, parasite prevalence in ethnic communities, and assessing disease risk factors at the human-animal-environment interface, including biosecurity practices, animal movement, human-animal contact, and antimicrobial resistance\nMain responsibilities included:\n\nParticipated in the design, monitoring, and evaluation of interventions\nAssisted in collecting samples in field and preparing samples at laboratory\nCollected and analyzed data, wrote reports and manuscripts for publication\nConducted risk communication campaigns about food-borne diseases and food safety\nConducted project administrative tasks, i.e.Â contacting with local & international partners, applying ethical approval"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": " Education",
    "text": "Education\n\n\n\n\n\nUppsala University\n\n\nDoctoral degree in Global Health Oct 2021 â€“ Oct 2025\n\nThesis: Towards sustainable solutions for AMR: Targeting distal drivers through integrated SDG-policy and actions\n\n\n\n\n\n\nUniversity of LiÃ¨ge\n\n\nSpecialized Master in Integrated Management of Health Risks in the Global South Sep 2020 â€“ Sep 2021\n\nGraduated with highest honor\nThesis: Operationalization and measurement of organizational adaptability: experiences from FRIENDSHIP, a non-governmental organization adapting to COVID-19 pandemic in Bangladesh\n\n\n\n\n\n\nHanoi Medical University\n\n\nBachelor of Public Health Sep 2013 â€“ Sep 2017\n\nGraduated with high honor\nGraduation thesis: 9.5/10"
  },
  {
    "objectID": "cv.html#communities-and-organizations",
    "href": "cv.html#communities-and-organizations",
    "title": "Curriculum Vitae",
    "section": " Communities and Organizations",
    "text": "Communities and Organizations\n The Association of Vietnamese Science and Technology Experts in Sweden (AVISE)\n Applied Epi Community\n GlobeLife"
  },
  {
    "objectID": "cv.html#publication",
    "href": "cv.html#publication",
    "title": "Curriculum Vitae",
    "section": " Publication",
    "text": "Publication\nLink to my Google Scholar profile (14 peer-reviewed, h-index: 7, i10-index: 4, total number of citations: 110)\nSelected 5 articles:\n\nLuong NT, Didier W, Mats M, Tiscar G, Peter SJ (2024) Characterizing proximal and distal drivers of antimicrobial resistance: An umbrella review. Journal of Global Antimicrobial Resistance\nPeter SJ, Luong NT and 10 co-authors (2024). Association between national action and trends in antibiotic resistance: an analysis of 73 countries from 2000 to 2023\nNgo HTH, Nguyen TL, and 6 co-authors (2021). Microbial contamination and associated risk factors in retailed pork from key value chains in Northern Vietnam. Food Microbiology\nLam S, Huyen NTT, Ngo HTH, Nguyen TL, and 6 co-authors (2021). Unpacking the Theory Behind One Health Food Safety Programs: A Vietnam Case Study. Frontiers in Veterinary Science\nMai HT, Le GM, Tran BX, Do HN, Latkin CA, Luong NT, and 6 co-authors (2018). Adherence to antiretroviral therapy among HIV/AIDS patients in the context of early treatment initiation in Vietnam. Patient Preference and Adherence"
  },
  {
    "objectID": "cv.html#academic-events-attendance",
    "href": "cv.html#academic-events-attendance",
    "title": "Curriculum Vitae",
    "section": " Academic events attendance",
    "text": "Academic events attendance\n\n2024 Oral presentation: The Nordic AMR Centre Conference, TromsÃ¸, Norway\n2023 Poster presentation: EAR LTC-Sarea-ENLIGHT CONGRESS â€œStrengthening Antibiotic Resistance Networksâ€, Bordeaux, France\n2023 CIRCUSâ€™ Interdisciplinary Summer School, Uppsala, Sweden\n2022 MIRAI 2.0 Research and Innovation week 2022, Fukuoka, Japan\n2022 NorDoc PhD Summit and summer school, Bergen Norway\n2022 SweDev PhD conference, Gothenburg, Sweden\n2019 Poster presentation: 13th SafePork 2019: One Health â€“ Tear down interdisciplinary wallâ€ conference, Berlin, Germany\n2019 Poster presentation: Regional symposium on research into smallholder pig production, and pork safety, Hanoi, Vietnam"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Curriculum Vitae",
    "section": " Skills",
    "text": "Skills\nProfessional skills\n\nStatistical & Machine Learning Frameworks: Extensive experience with generalized linear mixed models, time series analysis, spatial statistics, network analysis, clustering techniques, natural language processing, and large language models\nEpidemiological Methods: Skilled in conducting systematic reviews and meta-analyses, health policy analysis, household surveys, data collection (including in-depth interviews, focus group discussions), and risk communication.\nSoftware Engineering: Advanced proficiency in using programming language for statistical analysis (R, Python, STATA), product communication (R Markdown, Quarto, Shiny and Power BI), database management & query (SQL), version control (Git, Github Action), web framework (HTML, CSS, JS), package development (R, TypeScript).\nData Management: Proficiency in field/clinical data management using Open Data Kit (ODK) and RedCap\nLaboratory techniques: Familiar with laboratory microbiological and molecular techniques including Salmonella isolation, Total Bacteria Count, ELISA and PCR\nProject Management: Budget planning, M&E skills, proposal writing\n\nPersonal skills\n\nInterdisciplinary, adaptability, organized, independent, innovative, optimistic, constructive\n\nLanguages\n\nVietnamese, English, Swedish"
  },
  {
    "objectID": "projects/aetranslations.html",
    "href": "projects/aetranslations.html",
    "title": "{aetranslations}",
    "section": "",
    "text": "{pkgdown} site  Code  CRAN \nThis package hosts functions and guidelines for translating Applied Epi courses and resources. This includes courses materials (slides, exercises, tutorials), website (i.e.Â case studies) and books (i.e.Â the EpiRhandbook and the Applied Epi Manual). By leveraging AI services like DeepL or OpenAI, {aetranslations} package streamlines the process to make the Applied Epi materials available in multiple languages."
  },
  {
    "objectID": "projects/amr_trend.html",
    "href": "projects/amr_trend.html",
    "title": "AMR Global Trend",
    "section": "",
    "text": "App  Code\nThe ShinyApp is the supplementary materials for the article:\nAssociation between national policy and trends in antibiotic resistance: an analysis of 73 countries from 2000 to 2023\nPeter SÃ¸gaard JÃ¸rgensen1,2,3,, Luong Nguyen Thanh1,3, Ege Pehlivanoglu1, Franziska Klein1,#a, Didier Wernli4, Dusan Jasovsky5,#b, Athena Aktipis6, Rob R. Dunn7, Yrjo GrÃ¶hn8, Guillaume Lhermie8,#c, H. Morgan Scott9, Eili Y. Klein10,11\nAffiliations:\n1 Global Economic Dynamics and the Biosphere, The Royal Swedish Academy of Sciences, SE-114 18, Stockholm, Sweden\n2 Stockholm Resilience Centre, Stockholm University, SE-106 91, Stockholm, Sweden\n3 Uppsala Antibiotic Centre and Department of Womenâ€™s and Childrenâ€™s Health, Uppsala University, SE-751 05, Uppsala, Sweden\n4 University of Geneva, Global Studies Institute, Transformative Governance Lab, CH-1211 GenÃ¨ve 4, Switzerland\n5 Uppsala University, ReAct Europe, SE-753 10, Uppsala, Sweden\n6 Arizona State University, Department of Psychology, Tempe, AZ 85281, USA\n7 North Carolina State University, Department of Applied Ecology, Raleigh, NC 27695-7617, USA\n8 Cornell University, College of Veterinary Medicine, Ithaca, NY 14853, USA\n9 Texas A&M University, College Station, TX 77843-4467, USA\n10 One Health Trust, Washington, D.C. 20015, USA\n11 Johns Hopkins School of Medicine, Department of Emergency Medicine, Baltimore, MD 21205, USA\n\n\n\nScreenshot from the App"
  },
  {
    "objectID": "talks/2024-11-17_avise_seminar_01/index.html",
    "href": "talks/2024-11-17_avise_seminar_01/index.html",
    "title": "How to keep information â€˜youngâ€™",
    "section": "",
    "text": "Recorded\n\nDetails\nğŸ“† November 17, 2024 // 2:00 pm - 3:00 pm CET\nğŸ¨ Virtual\nğŸŒ  The Association of Vietnamese Science and Technology Experts in Sweden\n\n\nAbstract\nWireless communication not only facilitates the transmission of information between mobile users but also serves as a stepping stone for delivering information to subsequent processing points in an intelligent system. In this context, the goal of communication extends beyond ensuring accuracy and continuity to maintaining the value of information. This value can be measured by the freshness of information.\nThe talk will introduce the concept of the â€œage of informationâ€â€”a metric used to assess the freshness of informationâ€”and analyze this metric within a random multi-access system involving numerous devices. The content of the talk is designed for a general audience and does not require a technical background.\n\n\nScreenshot\n\n\n\n\n\nCitationBibTeX citation:@online{nguyen_thanh2024,\n  author = {Nguyen Thanh, Luong},\n  title = {How to Keep Information â€œYoungâ€},\n  date = {2024-11-17},\n  url = {https://ntluong95.github.io/profile/talks/2024-11-17_avise_seminar_01/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nNguyen Thanh, Luong. 2024. â€œHow to Keep Information\nâ€˜Youngâ€™.â€ November 17, 2024. https://ntluong95.github.io/profile/talks/2024-11-17_avise_seminar_01/."
  },
  {
    "objectID": "blog/2024-12-04_positron_review/index.html",
    "href": "blog/2024-12-04_positron_review/index.html",
    "title": "Positron - a VSCode fork for Data Science",
    "section": "",
    "text": "Key notes\n\n\n\n\nThe Data Explorer is a great way to inspect your data! You see things like the percentage of missing data or summary statistics per column. Thereâ€™s multi-sorting and filtering. Some things are known in RStudio as well, but this Data Explorer goes a few steps further.\nCode completions works out-of-the-box for both R and Python.\nHelp on hover: get some help when hovering over functions\nThe use of extensions: you can use anything from Open VSX and it really makes the IDE â€œyoursâ€. Some cool ones are: indent-rainbow, TODO highlight and GitLens.\nThe test explorer: a separate pane for R packages with testthat that gives you all kind of insights and actions related to testing."
  },
  {
    "objectID": "blog/2024-12-04_positron_review/index.html#hello-positron-ide-key-features-you-must-know",
    "href": "blog/2024-12-04_positron_review/index.html#hello-positron-ide-key-features-you-must-know",
    "title": "Positron - a VSCode fork for Data Science",
    "section": "1 Hello Positron IDE â€“ Key Features You Must Know",
    "text": "1 Hello Positron IDE â€“ Key Features You Must Know\nPositron is a next-generation data science IDE delivered by Posit. Itâ€™s still in active development, so itâ€™s expected to see some features not working properly (more on this later). But, as mentioned in the introduction, itâ€™s in public beta, which means youâ€™re free to take it for a spin!\nYou can download the latest Positron release from the official GitHub releases page.\nIn essence, Positron is a fork of a famous IDE â€“ Visual Studio Code. If youâ€™re familiar with it, Positron should feel right at home. It has some neat features delivered out of the box, but you could configure most of these through plugins on a fresh VSCode installation."
  },
  {
    "objectID": "blog/2024-12-04_positron_review/index.html#running-code-in-positron-ide-tips-and-tricks",
    "href": "blog/2024-12-04_positron_review/index.html#running-code-in-positron-ide-tips-and-tricks",
    "title": "Positron - a VSCode fork for Data Science",
    "section": "2 Running Code In Positron IDE â€“ Tips And Tricks",
    "text": "2 Running Code In Positron IDE â€“ Tips And Tricks\nPositron, being a mixture of RStudio and Visual Studio Code, combines the best features of both worlds. This section will take you through useful keyboard shortcuts and features for working with data.\n\nUseful Shortcuts\nWhile you donâ€™t necessarily need to use keyboard shortcuts, they significantly improve developer productivity. The following image illustrates what it takes to run a single cell without keyboard shortcuts:\nâ€\nImage 9 â€“ Running Jupyter cell without shortcuts Manually clicking on the play button will become pretty annoying after a couple of iterations. Instead, you can opt for one of the following:\nControl + Enter: Run the current cell without creating a new one Shift + Enter: Run the current cell and create a new cell for the same programming language While in a notebook environment, you might also find the following shortcuts useful:\nC: Copy cell X: Cut cell V: Paste cell D: Delete cell Command/Control + P: Navigate between files Note that they are different from the usual jupyter notebook keyboard shortcuts, but you can see the full list of available shortcuts in Settings â€“ Keyboard Shortcuts. Every shortcut is adjustable.\n\n\nDataFrame Viewer\nDataframes are the core of all data science workflows, so having an IDE that can display all relevant information about them is a must-have feature.\nPositron allows you to print the dataframe content to the R console by calling R-specific functions, such as head():\nâ€\nImage 10 â€“ Printing the top 6 rows of a dataframe But the more interesting feature is the dataframe viewer. Once your dataframe is declared, youâ€™ll see it in the Variables panel. You can expand the variable to view all columns and their respective values, or you can click on the table column to inspect the dataframe in an Excel-like fashion:\nâ€\nImage 11 â€“ Dataframe inspection As you can see, you can sort the values, apply filters, inspect missing values, and much more â€“ straight from the GUI.\n\n\nPlot Viewer\nAn amazing feature of RStudio is the plot viewer. You have a dedicated panel for visualizations, and you can easily cycle through multiple charts. Positron has the same feature, arguably with a somewhat updated interface:\nâ€\nImage 12 â€“ Plot inspection Creating a new chart wonâ€™t delete the old one, as you can easily navigate between them using the right-side panel:\nâ€\nImage 13 â€“ Plot inspection (2) Up next, letâ€™s discuss the powerful and improved variable inspector.\n\n\nVariable Inspector\nBeing able to inspect complex objects, such as plots, is an essential feature for debugging code and making sure everything works as expected. RStudio also has this feature, but Positron allows you to dig deeper and has a sleeker-looking user interface:\nâ€\nImage 14 â€“ Variable inspection As you can see, you can drill down into all the small pieces that are combined to make complex objects."
  },
  {
    "objectID": "README-THAM_KHAO.html",
    "href": "README-THAM_KHAO.html",
    "title": "Welcome to Luong Nguyenâ€™s Profile ğŸ‘‹",
    "section": "",
    "text": "Hey there! Iâ€™m a self-taught Full-Stack developer hailing from the beautiful country of Vietnam ğŸ‡»ğŸ‡³. When Iâ€™m not typing away at my keyboard, you can find me sipping on some coffee and munching on some delicious street food.\n\nğŸ”­ Currently, Iâ€™m embarking on a 365-day commit challenge, which means my GitHub chart is looking greener than a jungle! But letâ€™s be real, who doesnâ€™t love a green chart?\nğŸŒ± Iâ€™m all about React, Typescript, NextJS, and TailwindCSS, and Iâ€™m always looking for ways to create beautiful and speedy applications. Oh, and did I mention that Iâ€™m a proud Neovim user? Yeah, I like to live on the edge.\nğŸ‘‰ Check out my repo for some cool stuff I have been doing. Hit me up if you ever need a chat or have some ideas. My DMs are always open, well at least when Iâ€™m not stuck in Vim.\n\n\n\n\n           \n \n\n\n\n\n\n  Â Â  Â Â    Â Â    Â Â   \n\n\n\n\n\n\nGitHub Streak\n\n\n\n\n\n     Note: Top languages is only a metric of the languages my public code consists of and doesnâ€™t reflect experience or skill level."
  },
  {
    "objectID": "README-THAM_KHAO.html#streak-stats",
    "href": "README-THAM_KHAO.html#streak-stats",
    "title": "Welcome to Luong Nguyenâ€™s Profile ğŸ‘‹",
    "section": "",
    "text": "GitHub Streak"
  },
  {
    "objectID": "README-THAM_KHAO.html#github-stats",
    "href": "README-THAM_KHAO.html#github-stats",
    "title": "Welcome to Luong Nguyenâ€™s Profile ğŸ‘‹",
    "section": "",
    "text": "Note: Top languages is only a metric of the languages my public code consists of and doesnâ€™t reflect experience or skill level."
  },
  {
    "objectID": "index.html#streak-stats",
    "href": "index.html#streak-stats",
    "title": "Luong Nguyen Thanh",
    "section": "ğŸ”¥ Streak stats",
    "text": "ğŸ”¥ Streak stats\n\n\n\nGitHub Streak"
  },
  {
    "objectID": "blog/2024-12-04_positron_review/index.html#rstudio-meets-visual-studio-code",
    "href": "blog/2024-12-04_positron_review/index.html#rstudio-meets-visual-studio-code",
    "title": "Positron - a VSCode fork for Data Science",
    "section": "2 RStudio Meets Visual Studio Code",
    "text": "2 RStudio Meets Visual Studio Code\nHereâ€™s what youâ€™ll see when you first launch Positron:\n\n\n\n\n\n\n\n\n\nImage 1 â€“ Positron IDE welcome screen It certainly looks like a combination of RStudio and Visual Studio Code! Youâ€™ve got your familiar sidebar for navigation and extensions, but also your four-panel view for code, console, plots, and variables.\n\n\n\n\nThe top left panel allows you to start working on your data science projects â€“ either in R or Python, through a notebook or file. Positron automatically detects installed programming languages and their version, but also picks up any virtual environments youâ€™ve previously created:\n\n\n\n\n\n\n\n\n\nImage 2 â€“ File/project creation in Positron IDE Up next, letâ€™s explore this multi-language and multi-format support in more detail."
  },
  {
    "objectID": "blog/2024-12-04_positron_review/index.html#multi-language-support",
    "href": "blog/2024-12-04_positron_review/index.html#multi-language-support",
    "title": "Positron - a VSCode fork for Data Science",
    "section": "3 Multi-Language Support",
    "text": "3 Multi-Language Support\nThe big selling point of Positron IDE is that it comes configured for R and Python out of the box â€“ Jupyter Notebooks included. This means you donâ€™t have to set everything up from scratch, which in the case of R and Jupyter is not as easy as it sounds.\nTo create a new R script, click on the New File button on the welcome screen and select R File. Writing and running code works just like in RStudio â€“ Command/Control + Enter will run the cell on which your cursor is located:\n\n\n\n\n\n\n\n\n\nImage 3 â€“ Working with R files in Positron The same approach to writing and running code works in Python scripts â€“ write any code block you want and hit Command/Control + Enter to run it\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage 4 â€“ Working with Python files in Positron Still, we think Jupyter notebooks allow maximum flexibility. You can create a notebook with a default programming language profile (R or Python), but you can then change the language for each cell.\n\n\n\n\nBecause of this flexibility, you can also sprinkle text/markdown content between your cells to provide resources or explanations:\n\n\n\n\n\n\n\n\n\nImage 5 â€“ Working with Jupyter Notebooks in Positron And thatâ€™s the basics of programming language and format support in Positron. Up next, letâ€™s discuss some more advanced features.\n\n\n\n\nâ€"
  },
  {
    "objectID": "blog/2024-12-04_positron_review/index.html#dataframe-viewer",
    "href": "blog/2024-12-04_positron_review/index.html#dataframe-viewer",
    "title": "Positron - a VSCode fork for Data Science",
    "section": "4 DataFrame Viewer",
    "text": "4 DataFrame Viewer\nDataframes are the core of all data science workflows, so having an IDE that can display all relevant information about them is a must-have feature.\nPositron allows you to print the dataframe content to the R console by calling R-specific functions, such as head():\nâ€\n\n\n\n\n\n\n\n\n\nImage 6 â€“ Printing the top 6 rows of a dataframe But the more interesting feature is the dataframe viewer.\n\n\n\n\nâ€Once your dataframe is declared, youâ€™ll see it in the Variables panel. You can expand the variable to view all columns and their respective values, or you can click on the table column to inspect the dataframe in an Excel-like fashion\n\n\n\n\n\n\n\n\n\nImage 7 â€“ Dataframe inspection As you can see, you can sort the values, apply filters, inspect missing values, and much more â€“ straight from the GUI.\n\n\n\n\nThe Data Explorer has three primary components, discussed in greater detail in the sections below:\n\nData grid: Spreadsheet-like display of the individual cells and columns, as well as sorting\nSummary panel: Column name, type and missing data percentage for each column\nFilter bar: Ephemeral filters for specific columns\n\n\n\n\n\n\n\n\n\n\nImage 8 â€“ Data Explorer three main components."
  },
  {
    "objectID": "blog/2024-12-04_positron_review/index.html#plot-viewer",
    "href": "blog/2024-12-04_positron_review/index.html#plot-viewer",
    "title": "Positron - a VSCode fork for Data Science",
    "section": "5 Plot Viewer",
    "text": "5 Plot Viewer\nAn amazing feature of RStudio is the plot viewer. You have a dedicated panel for visualizations, and you can easily cycle through multiple charts. Positron has the same feature, arguably with a somewhat updated interface. Creating a new chart wonâ€™t delete the old one, as you can easily navigate between them using the right-side panel â€\n\n\n\n\n\n\n\n\n\nImage 9 â€“ Plot inspection"
  },
  {
    "objectID": "blog/2024-12-04_positron_review/index.html#variable-inspector",
    "href": "blog/2024-12-04_positron_review/index.html#variable-inspector",
    "title": "Positron - a VSCode fork for Data Science",
    "section": "6 Variable Inspector",
    "text": "6 Variable Inspector\nBeing able to inspect complex objects, such as plots, is an essential feature for debugging code and making sure everything works as expected. RStudio also has this feature, but Positron allows you to dig deeper and has a sleeker-looking user interface. As you can see, you can drill down into all the small pieces that are combined to make complex objects.\n\n\n\n\n\n\n\n\n\nImage 10 â€“ Variable inspection"
  }
]