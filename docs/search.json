[
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Upcoming\nNone at this time.\n\n\nPast\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom musical practice to artistic reseaarch\n\n\n\nArtistic Research\n\n\n\nIn this talk, Dr.¬†Nguy·ªÖn Thanh Th·ªßy will share about the journey from her personal questions in music practice to the experiments in her artistic research.\n\n\n\n\n\nMar 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAI in Global Health - Applications & Ethical dilemmas\n\n\n\nGlobal Health\n\nArtificial Intelligence\n\n\n\nHow is AI changing the future of healthcare‚Äîand your role in it? Join us for an interactive session with three experts leading the way in AI-driven diagnostics, research, and ethics. Discover real-world applications, debate ethical dilemmas, and explore how AI could shape your career in global health.\n\n\n\n\n\nFeb 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding your future career in Global Health\n\n\n\nGlobal Health\n\nInternship\n\n\n\nThe GlobeLife Students Section ONLINE event on ‚ÄòInsights and Internships: Building your future career in Global Health‚Äô aims to provide insider tips and share about the value of internships from decent speakers who have successfully navigated this path.\n\n\n\n\n\nDec 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHow to keep information ‚Äòyoung‚Äô\n\n\n\nCommunication\n\nTechnology\n\n\n\nThe talk will introduce the concept of the ‚Äòage of information‚Äô‚Äîa metric used to assess the freshness of information‚Äîand analyze this metric within a random multi-access system involving numerous devices\n\n\n\n\n\nNov 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGlobalLife Annual Networking Event\n\n\n\nGlobal Health\n\nNetworking\n\n\n\nThe event was held to connect alumni and current MSc and PhD students in Global Health between Uppsala University and Karolinska Institutet.\n\n\n\n\n\nFeb 2, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/2025-02-14_globelife_05/index.html",
    "href": "talks/2025-02-14_globelife_05/index.html",
    "title": "AI in Global Health - Applications & Ethical dilemmas",
    "section": "",
    "text": "Recording\n\nDetails\nüìÜ February 14, 2025 // 1:30 pm - 2:30 pm CET\nüè® Virtual\nüå† GlobeLife\n\n\nAbstract\nHow is AI changing the future of healthcare‚Äîand your role in it? Join us for an interactive session with three experts leading the way in AI-driven diagnostics, research, and ethics. Discover real-world applications, debate ethical dilemmas, and explore how AI could shape your career in global health.\nNina Linder: Discover a groundbreaking AI-powered method for cervical cancer screening. Nina will showcase how mobile diagnostics, enhanced by wireless connectivity and advanced AI analysis, are transforming access to quality care in low-resource settings.\nJohan Lagerros: Explore the applications of AI in study and research. Johan will discuss how AI tools are reshaping study methodologies and unlocking new insights, driving forward the frontier of scientific research.\nLovisa H√•kansson: Engage in an interactive discussion on the ethical use of AI in study and research. Lovisa will lead a collaborative dialogue, encouraging participants to share insights and explore responsible practices for integrating AI technologies in research environments.\n\n\n\n\nCitationBibTeX citation:@online{nguyen_thanh2025,\n  author = {Nguyen Thanh, Luong},\n  title = {AI in {Global} {Health} - {Applications} \\& {Ethical}\n    Dilemmas},\n  date = {2025-02-14},\n  url = {https://ntluong95.github.io/profile/talks/2025-02-14_globelife_05/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nNguyen Thanh, Luong. 2025. ‚ÄúAI in Global Health - Applications\n& Ethical Dilemmas.‚Äù February 14, 2025. https://ntluong95.github.io/profile/talks/2025-02-14_globelife_05/."
  },
  {
    "objectID": "talks/2024-11-17_avise_seminar_01/index.html",
    "href": "talks/2024-11-17_avise_seminar_01/index.html",
    "title": "How to keep information ‚Äòyoung‚Äô",
    "section": "",
    "text": "Recording\n\nDetails\nüìÜ November 17, 2024 // 2:00 pm - 3:00 pm CET\nüè® Virtual\nüå† The Association of Vietnamese Science and Technology Experts in Sweden\n\n\nAbstract\nWireless communication not only facilitates the transmission of information between mobile users but also serves as a stepping stone for delivering information to subsequent processing points in an intelligent system. In this context, the goal of communication extends beyond ensuring accuracy and continuity to maintaining the value of information. This value can be measured by the freshness of information.\nThe talk will introduce the concept of the ‚Äúage of information‚Äù‚Äîa metric used to assess the freshness of information‚Äîand analyze this metric within a random multi-access system involving numerous devices. The content of the talk is designed for a general audience and does not require a technical background.\n\n\nScreenshot\n\n\n\n\n\nCitationBibTeX citation:@online{nguyen_thanh2024,\n  author = {Nguyen Thanh, Luong},\n  title = {How to Keep Information ‚ÄúYoung‚Äù},\n  date = {2024-11-17},\n  url = {https://ntluong95.github.io/profile/talks/2024-11-17_avise_seminar_01/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nNguyen Thanh, Luong. 2024. ‚ÄúHow to Keep Information\n‚ÄòYoung‚Äô.‚Äù November 17, 2024. https://ntluong95.github.io/profile/talks/2024-11-17_avise_seminar_01/."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Sahlgrenska Global Health Hackathon\n\n\n\nMobile App\n\nGlobal Health\n\nData Science\n\n\n\nCareBuddy is a mobile app designed to empower individuals managing chronic diseases. Our solution not only enhanced patient care but also secured a Top 3 finish at the‚Ä¶\n\n\n\nFeb 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n{aetranslations}\n\n\n\nR package\n\n\n\nThis package hosts functions and guidelines for translating Applied Epi courses and resources\n\n\n\nOct 10, 2024\n\n\n\n\n\n\n\n\n\n\n\nAMR Global Trend\n\n\n\nShiny app\n\n\n\nAMR Global Trend: A shiny app for exploring the global trend of AMR indicators from 73 countries\n\n\n\nAug 6, 2024\n\n\n\n\n\n\n\n\n\n\n\nAMR-SDGs interactions\n\n\n\nShiny app\n\n\n\nAMR-SDGs Explorer: A shiny app for exploring the interaction between AMR and SDGs agenda in national policies\n\n\n\nJun 20, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/amr_trend.html",
    "href": "projects/amr_trend.html",
    "title": "AMR Global Trend",
    "section": "",
    "text": "App  Code\nThe ShinyApp is the supplementary materials for the article:\nAssociation between national policy and trends in antibiotic resistance: an analysis of 73 countries from 2000 to 2023. PLOS Global Public Health\nPeter S√∏gaard J√∏rgensen1,2,3,, Luong Nguyen Thanh1,3, Ege Pehlivanoglu1, Franziska Klein1,#a, Didier Wernli4, Dusan Jasovsky5,#b, Athena Aktipis6, Rob R. Dunn7, Yrjo Gr√∂hn8, Guillaume Lhermie8,#c, H. Morgan Scott9, Eili Y. Klein10,11\nAffiliations:\n1 Global Economic Dynamics and the Biosphere, The Royal Swedish Academy of Sciences, SE-114 18, Stockholm, Sweden\n2 Stockholm Resilience Centre, Stockholm University, SE-106 91, Stockholm, Sweden\n3 Uppsala Antibiotic Centre and Department of Women‚Äôs and Children‚Äôs Health, Uppsala University, SE-751 05, Uppsala, Sweden\n4 University of Geneva, Global Studies Institute, Transformative Governance Lab, CH-1211 Gen√®ve 4, Switzerland\n5 Uppsala University, ReAct Europe, SE-753 10, Uppsala, Sweden\n6 Arizona State University, Department of Psychology, Tempe, AZ 85281, USA\n7 North Carolina State University, Department of Applied Ecology, Raleigh, NC 27695-7617, USA\n8 Cornell University, College of Veterinary Medicine, Ithaca, NY 14853, USA\n9 Texas A&M University, College Station, TX 77843-4467, USA\n10 One Health Trust, Washington, D.C. 20015, USA\n11 Johns Hopkins School of Medicine, Department of Emergency Medicine, Baltimore, MD 21205, USA\n\n\n\nScreenshot from the App"
  },
  {
    "objectID": "projects/aetranslations.html",
    "href": "projects/aetranslations.html",
    "title": "{aetranslations}",
    "section": "",
    "text": "{pkgdown} site  Code  CRAN \nThis package hosts functions and guidelines for translating Applied Epi courses and resources. This includes courses materials (slides, exercises, tutorials), website (i.e.¬†case studies) and books (i.e.¬†the EpiRhandbook and the Applied Epi Manual). By leveraging AI services like DeepL or OpenAI, {aetranslations} package streamlines the process to make the Applied Epi materials available in multiple languages."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Uppsala University\n\n\nDoctoral degree in Global Health Oct 2021 ‚Äì Oct 2025\n\nDoctoral degree in Global Health\nThesis: Bridging the gap: Leveraging the sustainability agenda to inform policies and actions on drivers of antimicrobial resistance\n\n\n\n\n\n\nUniversity of Li√®ge\n\n\nSpecialized Master in Integrated Management of Health Risks in the Global South Sep 2020 ‚Äì Sep 2021\n\nGraduated with highest honor (summa cum laude)\nThesis: Operationalization and measurement of organizational adaptability: experiences from FRIENDSHIP, a non-governmental organization adapting to COVID-19 pandemic in Bangladesh\n\n\n\n\n\n\nHanoi Medical University\n\n\nBachelor of Public Health Sep 2013 ‚Äì Sep 2017\n\nGraduated with high honor\nGraduation thesis: 9.5/10"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Uppsala University\n\n\nDoctoral degree in Global Health Oct 2021 ‚Äì Oct 2025\n\nDoctoral degree in Global Health\nThesis: Bridging the gap: Leveraging the sustainability agenda to inform policies and actions on drivers of antimicrobial resistance\n\n\n\n\n\n\nUniversity of Li√®ge\n\n\nSpecialized Master in Integrated Management of Health Risks in the Global South Sep 2020 ‚Äì Sep 2021\n\nGraduated with highest honor (summa cum laude)\nThesis: Operationalization and measurement of organizational adaptability: experiences from FRIENDSHIP, a non-governmental organization adapting to COVID-19 pandemic in Bangladesh\n\n\n\n\n\n\nHanoi Medical University\n\n\nBachelor of Public Health Sep 2013 ‚Äì Sep 2017\n\nGraduated with high honor\nGraduation thesis: 9.5/10"
  },
  {
    "objectID": "cv.html#professional-experience",
    "href": "cv.html#professional-experience",
    "title": "Curriculum Vitae",
    "section": "\n Professional Experience",
    "text": "Professional Experience\n\n\n\n\n\nUppsala University\n\n\nPhD student Oct 2021 ‚Äì Oct 2025\n\n\nPhD project: Analyzed the degree to which factors driving antimicrobial resistance (AMR) development and spread are addressed by AMR-specific policies and interventions versus broader societal approaches, particularly the Sustainable Development Goals (SDGs) agenda.\n\n\n\n\n\n\n\n\n\n\nINFLUX project (ERC grant No.¬†101039376): Contributed to research investigating and establishing database of drivers responsible for the emergence of emerging pests and pathogens, assessing governmental responses and their cascading social-ecological effects.\nSUSTAIN project: Conducted statistical analysis to evaluate the effectiveness of bundled healthcare interventions targeting health workers in Nepal, aimed at enhancing neonatal care quality\nTeaching: Delivered a lecture on the ‚ÄúPlanetary Health‚Äù course for the Master‚Äôs Program in Implementation, Transformative Learning and Sustainability at Uppsala University and presented a guest lecture on R programming for assessing climate impact on health outcomes at the University of Gothenburg.\nSupervision: Co-supervised an MSc student at Karolinska Institutet on the project: National Indicators of Temporal Trends in the Complex Network of Antibiotic Resistance Drivers: Assessing Priorities for Action\n\n\n\n\n\n\nEpistat AB\n\n\nIntern Apr 2025 ‚Äì Present\n\n\nCardio project: Contributing to the development of the Statistical Analysis Plan (SAP), designing mock shells, and supporting data analysis\n\n\n\n\n\n\nApplied Epi\n\n\nInstructor, Coordinator Jun 2021 ‚Äì Present\n\nInstructor: Provide R trainings for many epidemiologists worldwide. Provide public health services support including mentorship, outbreak response to AppliedEpi partners (i.e.¬†Doctors Without Borders ‚Äì MSF, Australian National University, WHO Philippines)\nTranslation Coordinator: Maintain translation packages and educational resources, including the Epi R Handbook (available in 10 languages), Epi Julia Handbook, slides, and tutorials\nCo-Author of Applied Epidemiology Manual (on-going project)\nLead translator the Vietnamese version of R for applied epidemiology and public health\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHanoi University of Public Health\n\n\nResearch Assistant, Center for Public Health and Ecosystem Research Jul 2018 ‚Äì Jul 2020\nSupported three research projects on sanitation, food safety, and disease prevention in Vietnam, focusing on light-touch interventions in pig farms, parasite prevalence in ethnic communities, and assessing disease risk factors at the human-animal-environment interface, including biosecurity practices, animal movement, human-animal contact, and antimicrobial resistance\nMain responsibilities included:\n\nParticipated in the design, monitoring, and evaluation of interventions\nAssisted in collecting samples in field and preparing samples at laboratory\nCollected and analyzed data, wrote reports and manuscripts for publication\nConducted risk communication campaigns about food-borne diseases and food safety\nConducted project administrative tasks, i.e.¬†contacting with local & international partners, applying ethical approval"
  },
  {
    "objectID": "cv.html#communities-and-organizations",
    "href": "cv.html#communities-and-organizations",
    "title": "Curriculum Vitae",
    "section": "\n Communities and Organizations",
    "text": "Communities and Organizations\nAVSE Global - Medical Translational Research Network\nThe Association of Vietnamese Science and Technology Experts in Sweden (AVISE)\nApplied Epi Community\nGlobeLife"
  },
  {
    "objectID": "cv.html#publication",
    "href": "cv.html#publication",
    "title": "Curriculum Vitae",
    "section": "\n Publication",
    "text": "Publication\nLink to my Google Scholar profile (14 peer-reviewed, h-index: 7, i10-index: 4, total number of citations: 110)\nSelected 5 articles:\n\nLuong NT, Didier W, Mats M, Peter SJ (2025). ‚ÄòWhen global health meets global goals‚Äô: assessing the alignment between antimicrobial resistance and sustainable development policies in 10 African and Asian countries. BMJ Global Health\nPeter SJ, Luong NT and 10 co-authors (2024). Association between national action and trends in antibiotic resistance: an analysis of 73 countries from 2000 to 2023. PLOS Global Public Health\nLuong NT, Didier W, Mats M, Tiscar G, Peter SJ (2024) Characterizing proximal and distal drivers of antimicrobial resistance: An umbrella review. Journal of Global Antimicrobial Resistance\nNgo HTH, Nguyen TL, and 6 co-authors (2021). Microbial contamination and associated risk factors in retailed pork from key value chains in Northern Vietnam. Food Microbiology \nLam S, Huyen NTT, Ngo HTH, Nguyen TL, and 6 co-authors (2021). Unpacking the Theory Behind One Health Food Safety Programs: A Vietnam Case Study. Frontiers in Veterinary Science"
  },
  {
    "objectID": "cv.html#academic-events-attendance",
    "href": "cv.html#academic-events-attendance",
    "title": "Curriculum Vitae",
    "section": "\n Academic events attendance",
    "text": "Academic events attendance\n\n\n2024 Oral presentation: The Nordic AMR Centre Conference, Troms√∏, Norway\n\n2023 Poster presentation: EAR LTC-Sarea-ENLIGHT CONGRESS ‚ÄúStrengthening Antibiotic Resistance Networks‚Äù, Bordeaux, France\n\n2023 CIRCUS‚Äô Interdisciplinary Summer School, Uppsala, Sweden\n\n2022 MIRAI 2.0 Research and Innovation week 2022, Fukuoka, Japan\n\n2022 NorDoc PhD Summit and summer school, Bergen Norway\n\n2022 SweDev PhD conference, Gothenburg, Sweden\n\n2019 Poster presentation: 13th SafePork 2019: One Health ‚Äì Tear down interdisciplinary wall‚Äù conference, Berlin, Germany\n\n2019 Poster presentation: Regional symposium on research into smallholder pig production, and pork safety, Hanoi, Vietnam"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Curriculum Vitae",
    "section": "\n Skills",
    "text": "Skills\nProfessional skills\n\n\nGlobal Health & Sustainability research methodologies: Solid expertise in evidence synthesis and system scoping, with experience applying LLMs to systematic reviews and meta-analyses, health policy analysis, qualitative content analysis, comparative case study analysis, participatory modelling, knowledge co-production, household surveys, data collection (including in-depth interviews, focus group discussions), and risk communication.\n\nStatistical & Machine Learning frameworks: Extensive experience with generalized regression models, time series analysis, spatial statistics and data visualization, clustering techniques, network analysis, natural language processing and large language models applications. Familiar with advanced statistical models (GAMs, SEMs, Bayesian statistics, High-performance computing).\n\nLaboratory techniques & Field Data Management: Familiar with laboratory microbiological and molecular techniques including Salmonella isolation, Total Bacteria Count, ELISA and PCR. Proficiency in field/clinical data management using Open Data Kit (ODK) and RedCap\n\nProgramming & Data Engineering: Advanced proficiency in using programming language for statistical analysis (R, Python, STATA), product communication (R Markdown, Quarto, Shiny and Power BI), database management & query (SQL), version control (Git, Github Action, Docker), package development, web framework (HTML, CSS, JS, WordPress).\n\nProject Management: Budget planning, M&E skills, proposal writing, and interdisciplinary project coordination.\n\nPersonal skills\n\nInterdisciplinary, adaptability, organized, independent, innovative, optimistic, constructive\n\nLanguages\n\nVietnamese, English, Swedish"
  },
  {
    "objectID": "blog/2024-12-04_positron_review/index.html",
    "href": "blog/2024-12-04_positron_review/index.html",
    "title": "Positron - a VSCode fork for Data Science",
    "section": "",
    "text": "Key notes\n\n\n\n\nThe Data Explorer is a great way to inspect your data! You see things like the percentage of missing data or summary statistics per column. There‚Äôs multi-sorting and filtering. Some things are known in RStudio as well, but this Data Explorer goes a few steps further.\nCode completions works out-of-the-box for both R and Python.\nHelp on hover: get some help when hovering over functions\nThe use of extensions: you can use anything from Open VSX and it really makes the IDE ‚Äúyours‚Äù. Some cool ones are: indent-rainbow, TODO highlight and GitLens.\nThe test explorer: a separate pane for R packages with testthat that gives you all kind of insights and actions related to testing."
  },
  {
    "objectID": "blog/2024-12-04_positron_review/index.html#hello-positron-ide-key-features-you-must-know",
    "href": "blog/2024-12-04_positron_review/index.html#hello-positron-ide-key-features-you-must-know",
    "title": "Positron - a VSCode fork for Data Science",
    "section": "\n1 Hello Positron IDE ‚Äì Key Features You Must Know",
    "text": "1 Hello Positron IDE ‚Äì Key Features You Must Know\nPositron is a next-generation data science IDE delivered by Posit. It‚Äôs still in active development, so it‚Äôs expected to see some features not working properly (more on this later). But, as mentioned in the introduction, it‚Äôs in public beta, which means you‚Äôre free to take it for a spin!\nYou can download the latest Positron release from the official GitHub releases page.\nIn essence, Positron is a fork of a famous IDE ‚Äì Visual Studio Code. If you‚Äôre familiar with it, Positron should feel right at home. It has some neat features delivered out of the box, but you could configure most of these through plugins on a fresh VSCode installation."
  },
  {
    "objectID": "blog/2024-12-04_positron_review/index.html#rstudio-meets-visual-studio-code",
    "href": "blog/2024-12-04_positron_review/index.html#rstudio-meets-visual-studio-code",
    "title": "Positron - a VSCode fork for Data Science",
    "section": "\n2 RStudio Meets Visual Studio Code",
    "text": "2 RStudio Meets Visual Studio Code\nHere‚Äôs what you‚Äôll see when you first launch Positron:\n\n\n\n\n\n\n\nImage 1 ‚Äì Positron IDE welcome screen It certainly looks like a combination of RStudio and Visual Studio Code! You‚Äôve got your familiar sidebar for navigation and extensions, but also your four-panel view for code, console, plots, and variables.\n\n\n\nThe top left panel allows you to start working on your data science projects ‚Äì either in R or Python, through a notebook or file. Positron automatically detects installed programming languages and their version, but also picks up any virtual environments you‚Äôve previously created:\n\n\n\n\n\n\n\nImage 2 ‚Äì File/project creation in Positron IDE Up next, let‚Äôs explore this multi-language and multi-format support in more detail."
  },
  {
    "objectID": "blog/2024-12-04_positron_review/index.html#multi-language-support",
    "href": "blog/2024-12-04_positron_review/index.html#multi-language-support",
    "title": "Positron - a VSCode fork for Data Science",
    "section": "\n3 Multi-Language Support",
    "text": "3 Multi-Language Support\nThe big selling point of Positron IDE is that it comes configured for R and Python out of the box ‚Äì Jupyter Notebooks included. This means you don‚Äôt have to set everything up from scratch, which in the case of R and Jupyter is not as easy as it sounds.\nTo create a new R script, click on the New File button on the welcome screen and select R File. Writing and running code works just like in RStudio ‚Äì Command/Control + Enter will run the cell on which your cursor is located:\n\n\n\n\n\n\n\nImage 3 ‚Äì Working with R files in Positron The same approach to writing and running code works in Python scripts ‚Äì write any code block you want and hit Command/Control + Enter to run it\n\n\n\n\n\n\n\n\n\n\nImage 4 ‚Äì Working with Python files in Positron Still, we think Jupyter notebooks allow maximum flexibility. You can create a notebook with a default programming language profile (R or Python), but you can then change the language for each cell.\n\n\n\nBecause of this flexibility, you can also sprinkle text/markdown content between your cells to provide resources or explanations:\n\n\n\n\n\n\n\nImage 5 ‚Äì Working with Jupyter Notebooks in Positron And that‚Äôs the basics of programming language and format support in Positron. Up next, let‚Äôs discuss some more advanced features.\n\n\n\n‚Äç"
  },
  {
    "objectID": "blog/2024-12-04_positron_review/index.html#dataframe-viewer",
    "href": "blog/2024-12-04_positron_review/index.html#dataframe-viewer",
    "title": "Positron - a VSCode fork for Data Science",
    "section": "\n4 DataFrame Viewer",
    "text": "4 DataFrame Viewer\nDataframes are the core of all data science workflows, so having an IDE that can display all relevant information about them is a must-have feature.\nPositron allows you to print the dataframe content to the R console by calling R-specific functions, such as head():\n‚Äç\n\n\n\n\n\n\n\nImage 6 ‚Äì Printing the top 6 rows of a dataframe But the more interesting feature is the dataframe viewer.\n\n\n\n‚ÄçOnce your dataframe is declared, you‚Äôll see it in the Variables panel. You can expand the variable to view all columns and their respective values, or you can click on the table column to inspect the dataframe in an Excel-like fashion\n\n\n\n\n\n\n\nImage 7 ‚Äì Dataframe inspection As you can see, you can sort the values, apply filters, inspect missing values, and much more ‚Äì straight from the GUI.\n\n\n\nThe Data Explorer has three primary components, discussed in greater detail in the sections below:\n\nData grid: Spreadsheet-like display of the individual cells and columns, as well as sorting\nSummary panel: Column name, type and missing data percentage for each column\nFilter bar: Ephemeral filters for specific columns\n\n\n\n\n\n\n\n\nImage 8 ‚Äì Data Explorer three main components."
  },
  {
    "objectID": "blog/2024-12-04_positron_review/index.html#plot-viewer",
    "href": "blog/2024-12-04_positron_review/index.html#plot-viewer",
    "title": "Positron - a VSCode fork for Data Science",
    "section": "\n5 Plot Viewer",
    "text": "5 Plot Viewer\nAn amazing feature of RStudio is the plot viewer. You have a dedicated panel for visualizations, and you can easily cycle through multiple charts. Positron has the same feature, arguably with a somewhat updated interface. Creating a new chart won‚Äôt delete the old one, as you can easily navigate between them using the right-side panel ‚Äç\n\n\n\n\n\n\n\nImage 9 ‚Äì Plot inspection"
  },
  {
    "objectID": "blog/2024-12-04_positron_review/index.html#variable-inspector",
    "href": "blog/2024-12-04_positron_review/index.html#variable-inspector",
    "title": "Positron - a VSCode fork for Data Science",
    "section": "\n6 Variable Inspector",
    "text": "6 Variable Inspector\nBeing able to inspect complex objects, such as plots, is an essential feature for debugging code and making sure everything works as expected. RStudio also has this feature, but Positron allows you to dig deeper and has a sleeker-looking user interface. As you can see, you can drill down into all the small pieces that are combined to make complex objects.\n\n\n\n\n\n\n\nImage 10 ‚Äì Variable inspection"
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html",
    "href": "blog/2024-11-01_dplyr_candidate/index.html",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "",
    "text": "I‚Äôm a long time R user and lately I‚Äôve seen more and more signals that it‚Äôs worth investing into Python. I use it for NLP with spaCy and to build functions on AWS Lambda. Further, there are many more data API libraries and machine learning libraries for Python than for R.\nAdopting Python means making choices on which libraries to invest time into learning. Manipulating data frames is one of the most common data science activities, so choosing the right library for it is key.\nMichael Chow, developer of siuba, a Python port of dplyr on top of pandas wrote describes the situation well:\nThe higher-level libraries he mentions come with a problem : There‚Äôs no universal standard.\nIn a discussion of the polars library on Hacker News the user ‚Äúcivilized‚Äù put the dplyr user perspective more bluntly:\nI‚Äôm more willing to compromise though, so here‚Äôs a comparison of the strongest contenders."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#the-contenders",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#the-contenders",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "The contenders",
    "text": "The contenders\nThe database-like ops benchmark on H2Oai is a helpful performance comparison.\nI‚Äôm considering these libraries:\n\n\nPandas: The most commonly used library and the one with the most tutorials and Stack Overflow answers available.\n\nsiuba: A port of dplyr to Python, built on top of pandas. Not in the benchmark. Performance probably similar to pandas or worse due to translation.\n\nPolars: The fastest library available. According to the benchmark, it runs 3-10x faster than Pandas.\n\nDuckdb: Use an in-memory OLAP database instead of a dataframe and write SQL. In R, this can also be queried via dbplyr.\n\nibis. Backend-agnostic wrapper for pandas and SQL engines.\n\nThere are more options. I excluded the others for these reasons:\n\nSlower than polars and not with a readability focus (dask, Arrow, Modin, pydatatable)\nRequires or is optmized for running on a remote server (Spark, ClickHouse and most other SQL databases).\nNot meant for OLAP (sqlite)\nNot in Python (DataFrames.jl)\nMeant for GPU (cuDF)"
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#github-stars-as-a-proxy-for-popularity",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#github-stars-as-a-proxy-for-popularity",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Github stars as a proxy for popularity",
    "text": "Github stars as a proxy for popularity\nThe benchmark provides a comparison of performance, but another important factor is popularity and maturity. A more mature library has a more stable API, better test coverage and there is more help available online, such as on StackOverflow. One way to measure popularity is the number of stars that the package repository has on Github.\n\nlibrary(ggplot2)\nlibs &lt;- data.frame(\n    library = c(\"pandas\", \"siuba\", \"polars\", \"duckdb\", \"dplyr\", \"data.table\", \"pydatatable\", \"dtplyr\", \"tidytable\", \"ibis\"),\n    language = c(\"Python\", \"Python\", \"Python\", \"SQL\", \"R\", \"R\", \"Python\", \"R\", \"R\", \"Python\"),\n    stars = c(32100, 732, 3900, 4100, 3900, 2900, 1400, 542, 285, 1600)\n)\n\nggplot(libs, aes(x = reorder(library, -stars), y = stars, fill = language)) +\n    geom_col() +\n    labs(\n        title = \"Pandas is by far the most popular choice\",\n        subtitle = \"Comparison of Github stars\",\n        fill = \"Language\",\n        x = \"Library\",\n        y = \"Github stars\"\n    )\n\n\n\n\n\n\n\nGithub stars are not a perfect proxy. For instance, dplyr is more mature than its star count suggests. Comparing the completeness of the documentation and tutorials for dplyr and polars reveals that it‚Äôs a day and night difference.\nWith the quantitative comparison out of the way, here‚Äôs a qualitative comparison of the Python packages. I‚Äôm speaking of my personal opinion of these packages - not a general comparison. My reference is my current use of dplyr in R. When I need more performance, I use tidytable to get most of the speed of data.table with the grammar of dplyr and eager evaluation. Another alternative is dtplyr, which translates dplyr to data.table with lazy evaluation. I also use dbplyr, which translates dplyr to SQL.\nI‚Äôll compare the libraries by running a data transformation pipeline involving import from CSV, mutate, filter, sort, join, group by and summarize. I‚Äôll use the nycflights13 dataset, which is featured in Hadley Wickham‚Äôs R for Data Science."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#dplyr-reference-in-r",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#dplyr-reference-in-r",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "dplyr: Reference in R",
    "text": "dplyr: Reference in R\nLet‚Äôs start with a reference implementation in dplyr. The dataset is available as a package, so I skip the CSV import.\n\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(nycflights13)\nlibrary(reactable)\n\n# Take a look at the tables\nreactable(head(flights, 10))\n\n\n\n\nreactable(head(airlines, 10))\n\n\n\n\n\nThe flights tables has 336776 rows, one for each flight of an airplane. The airlines table has 16 rows, one for each airline mapping the full name of the company to a code.\nLet‚Äôs find the airline with the highest arrival delays in January 2013.\n\nflights |&gt;\n    filter(year == 2013, month == 1, !is.na(arr_delay)) |&gt;\n    mutate(arr_delay = replace(arr_delay, arr_delay &lt; 0, 0)) |&gt;\n    left_join(airlines, by = \"carrier\") |&gt;\n    group_by(airline = name) |&gt;\n    summarise(flights = n(), mean_delay = mean(arr_delay)) |&gt;\n    arrange(desc(mean_delay))\n\n# A tibble: 16 √ó 3\n   airline                     flights mean_delay\n   &lt;chr&gt;                         &lt;int&gt;      &lt;dbl&gt;\n 1 SkyWest Airlines Inc.             1     107   \n 2 Hawaiian Airlines Inc.           31      48.8 \n 3 ExpressJet Airlines Inc.       3964      29.6 \n 4 Frontier Airlines Inc.           59      23.9 \n 5 Mesa Airlines Inc.               39      20.4 \n 6 Endeavor Air Inc.              1480      19.3 \n 7 Alaska Airlines Inc.             62      17.6 \n 8 Envoy Air                      2203      14.3 \n 9 Southwest Airlines Co.          985      13.0 \n10 JetBlue Airways                4413      12.9 \n11 United Air Lines Inc.          4590      11.9 \n12 American Airlines Inc.         2724      11.0 \n13 AirTran Airways Corporation     324       9.95\n14 US Airways Inc.                1554       9.11\n15 Delta Air Lines Inc.           3655       8.07\n16 Virgin America                  314       3.17\n\n\nSome values in arr_delay are negative, indicating that the flight was faster than expected. I replaced these values with 0 because I don‚Äôt want them to cancel out delays of other flights. I joined to the airlines table to get the full names of the airlines.\nI export the flights and airlines tables to CSV to hand them over to Python.\n\n# Write to temporary files\nflights_path &lt;- tempfile(fileext = \".csv\")\nairlines_path &lt;- tempfile(fileext = \".csv\")\n\ndata.table::fwrite(flights, flights_path, row.names = FALSE)\ndata.table::fwrite(airlines, airlines_path, row.names = FALSE)\n\nTo access the file from Python, the path is handed over:\n\n\n#| eval: false\n# Hand over the path from R\nflights_path = r[\"flights_path\"]\nairlines_path = r[\"airlines_path\"]\n\nFor more details on how this works with the reticulate package, check this documentation."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#pandas-most-popular",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#pandas-most-popular",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Pandas: Most popular",
    "text": "Pandas: Most popular\nThe following sections follow a pattern: read in from CSV, then build a query.\n\nimport pandas as pd\n\n# Import from CSV\nflights_pd = pd.read_csv(flights_path)\nairlines_pd = pd.read_csv(airlines_path)\n\npandas.read_csv reads the header and conveniently infers the column types.\n\n(\n    flights_pd.query(\"year == 2013 & month == 1 & arr_delay.notnull()\")\n    .assign(arr_delay=flights_pd.arr_delay.clip(lower=0))\n    .merge(airlines_pd, how=\"left\", on=\"carrier\")\n    .rename(columns={\"name\": \"airline\"})\n    .groupby(\"airline\")\n    .agg(flights=(\"airline\", \"count\"), mean_delay=(\"arr_delay\", \"mean\"))\n    .sort_values(by=\"mean_delay\", ascending=False)\n)\n\n                             flights  mean_delay\nairline                                         \nSkyWest Airlines Inc.              1  107.000000\nHawaiian Airlines Inc.            31   48.774194\nExpressJet Airlines Inc.        3964   29.642785\nFrontier Airlines Inc.            59   23.881356\nMesa Airlines Inc.                39   20.410256\nEndeavor Air Inc.               1480   19.321622\nAlaska Airlines Inc.              62   17.645161\nEnvoy Air                       2203   14.303677\nSouthwest Airlines Co.           985   12.964467\nJetBlue Airways                 4413   12.919329\nUnited Air Lines Inc.           4590   11.851852\nAmerican Airlines Inc.          2724   10.953377\nAirTran Airways Corporation      324    9.953704\nUS Airways Inc.                 1554    9.111326\nDelta Air Lines Inc.            3655    8.070315\nVirgin America                   314    3.165605\n\n\nI chose to use the pipeline syntax from pandas - another option is to modify the dataset in place. That has a lower memory footprint, but can‚Äôt be run repeatedly for the same result, such as in interactive use in a notebook.\nHere, the query() function is slightly awkward with the long string argument. The groupby doesn‚Äôt allow renaming on the fly like dplyr, though I don‚Äôt consider that a real drawback. Perhaps it‚Äôs clearer to rename explicitly anyway.\nPandas has the widest API, offering hundreds of functions for every conceivable manipulation. The clip function used here is one such example. One difference to dplyr is that pandas uses its own methods .mean(), rather than using external ones such as base::mean(). That means using custom functions instead carries a performance penalty.\nAs we‚Äôll see later, pandas is the backend for siuba and ibis, which boil down to pandas code.\nOne difference to all other discussed solutions is that pandas uses a row index. Base R also has this with row names, but the tidyverse and tibbles have largely removed them from common use. I never missed row names. At the times I had to work with them in pandas they were more confusing than helpful. The documentation of polars puts it more bluntly:\n\nNo index. They are not needed. Not having them makes things easier. Convince me otherwise\n\nThat‚Äôs quite passive aggressive, but I do agree and wish pandas didn‚Äôt have it."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#siuba-dplyr-in-python",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#siuba-dplyr-in-python",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "siuba: dplyr in Python",
    "text": "siuba: dplyr in Python\n\nimport siuba as si\n\n# Import from CSV\nflights_si = pd.read_csv(r[\"flights_path\"])\nairlines_si = pd.read_csv(r[\"airlines_path\"])\n\nAs siuba is just an alternative way of writing some pandas commands, we read the data just like in the pandas implementation.\n\n(\n    flights_si\n    &gt;&gt; si.filter(si._.year == 2013, si._.month == 1, si._.arr_delay.notnull())\n    &gt;&gt; si.mutate(arr_delay=si._.arr_delay.clip(lower=0))\n    &gt;&gt; si.left_join(si._, airlines_si, on=\"carrier\")\n    &gt;&gt; si.rename(airline=si._.name)\n    &gt;&gt; si.group_by(si._.airline)\n    &gt;&gt; si.summarize(flights=si._.airline.count(), mean_delay=si._.arr_delay.mean())\n    &gt;&gt; si.arrange(-si._.mean_delay)\n)\n\nI found siuba the easiest to work with. Once I understood the _ placeholder for a table of data, I could write it almost as fast as dplyr. Out of all the ways to refer to a column in a data frame, I found it to be the most convenient, because it doesn‚Äôt require me to spell out the name of the data frame over and over. While not as elegant as dplyr‚Äôs tidy evaluation (discussed at the end of the article), it avoids the ambivalence in dplyr where it can be unclear whether a name refers to a column or an outside object.\nIt‚Äôs always possible to drop into pandas, such as for the aggregation functions which use the mean() and count() methods of the pandas series. The &gt;&gt; is an easy replacement for the %&gt;% magrittr pipe or |&gt; base pipe in R.\nThe author advertises siuba like this (from the docs):\n\nSiuba is a library for quick, scrappy data analysis in Python. It is a port of dplyr, tidyr, and other R Tidyverse libraries.\n\nA way for dplyr users to quickly hack away at data analysis in Python, but not meant for unsupervised production use."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#polars-fastest",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#polars-fastest",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Polars: Fastest",
    "text": "Polars: Fastest\nPolars is written in Rust and also offers a Python API. It comes in two flavors: eager and lazy. Lazy evaluation is similar to how dbplyr and dtplyr work: until asked, nothing is evaluated. This enables performance gains by reordering the commands being executed. But it‚Äôs a little less convenient for interactive analysis. I‚Äôll use the eager API here.\n\nimport polars as pl\n\n# Import from CSV\nflights_pl = pl.read_csv(flights_path)\nairlines_pl = pl.read_csv(airlines_path)\n\n\n(\n    flights_pl.filter((pl.col(\"year\") == 2013) & (pl.col(\"month\") == 1))\n    .drop_nulls(\"arr_delay\")\n    .join(airlines_pl, on=\"carrier\", how=\"left\")\n    .with_columns(\n        [\n            pl.when(pl.col(\"arr_delay\") &gt; 0)\n            .then(pl.col(\"arr_delay\"))\n            .otherwise(0)\n            .alias(\"arr_delay\"),\n            pl.col(\"name\").alias(\"airline\"),\n        ]\n    )\n    .groupby(\"airline\")\n    .agg(\n        [pl.count(\"airline\").alias(\"flights\"), pl.mean(\"arr_delay\").alias(\"mean_delay\")]\n    )\n    .sort(\"mean_delay\", descending=True)\n)\n\nThe API is leaner than pandas, requiring to memorize fewer functions and patterns. Though this can also be seen as less feature-complete. Pandas, for example has a dedicated clip function.\nThere isn‚Äôt nearly as much help available for problems with polars as for with pandas. While the documentation is good, it can‚Äôt answer every question and lots of trial and error is needed.\nA comparison of polars and pandas is available in the polars documentation."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#duckdb-highly-compatible-and-easy-for-sql-users",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#duckdb-highly-compatible-and-easy-for-sql-users",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "DuckDB: Highly compatible and easy for SQL users",
    "text": "DuckDB: Highly compatible and easy for SQL users\n\nimport duckdb\n\ncon_duckdb = duckdb.connect(database=\":memory:\")\n\n# Import from CSV\ncon_duckdb.execute(\n    \"CREATE TABLE 'flights' AS \"\n    f\"SELECT * FROM read_csv_auto('{flights_path}', header = True);\"\n    \"CREATE TABLE 'airlines' AS \"\n    f\"SELECT * FROM read_csv_auto('{airlines_path}', header = True);\"\n)\n\n&lt;duckdb.duckdb.DuckDBPyConnection object at 0x0000014F23081730&gt;\n\n\nDuckDB‚Äôs read_csv_auto() works just like the csv readers in Python.\n\ncon_duckdb.execute(\n    \"WITH flights_clipped AS ( \"\n    \"SELECT carrier, CASE WHEN arr_delay &gt; 0 THEN arr_delay ELSE 0 END AS arr_delay \"\n    \"FROM flights \"\n    \"WHERE year = 2013 AND month = 1 AND arr_delay IS NOT NULL\"\n    \")\"\n    \"SELECT name AS airline, COUNT(*) AS flights, AVG(arr_delay) AS mean_delay \"\n    \"FROM flights_clipped \"\n    \"LEFT JOIN airlines ON flights_clipped.carrier = airlines.carrier \"\n    \"GROUP BY name \"\n    \"ORDER BY mean_delay DESC \"\n).fetchdf()\n\n                        airline  flights  mean_delay\n0         SkyWest Airlines Inc.        1  107.000000\n1        Hawaiian Airlines Inc.       31   48.774194\n2      ExpressJet Airlines Inc.     3964   29.642785\n3        Frontier Airlines Inc.       59   23.881356\n4            Mesa Airlines Inc.       39   20.410256\n5             Endeavor Air Inc.     1480   19.321622\n6          Alaska Airlines Inc.       62   17.645161\n7                     Envoy Air     2203   14.303677\n8        Southwest Airlines Co.      985   12.964467\n9               JetBlue Airways     4413   12.919329\n10        United Air Lines Inc.     4590   11.851852\n11       American Airlines Inc.     2724   10.953377\n12  AirTran Airways Corporation      324    9.953704\n13              US Airways Inc.     1554    9.111326\n14         Delta Air Lines Inc.     3655    8.070315\n15               Virgin America      314    3.165605\n\n\nThe performance is closer to polars than to pandas. A big plus is the ability to handle larger than memory data.\nDuckDB can also operate directly on a pandas dataframe. The SQL code is portable to R, C, C++, Java and other programming languages the duckdb has APIs. It‚Äôs also portable when the logic is taken to a DB like Postgres, or Clickhouse, or is ported to an ETL framework like DBT.\nThis stands in contrast to polars and pandas code, which has to be rewritten from scratch. It also means that the skill gained in manipulating SQL translates well to other situations. SQL has been around for more than 50 years - learning SQL is future-proofing a career.\nWhile these are big plusses, duckdb isn‚Äôt so convenient for interactive data exploration. SQL isn‚Äôt as composeable. Composing SQL queries requires many common table expressions (CTEs, WITH x AS (SELECT ...)). Reusing them for other queries is not as easy as with Python. SQL is typically less expressive than Python. It lacks shorthands and it‚Äôs awkward when there are many columns. It‚Äôs also harder to write custom functions in SQL than in R or Python. This is the motivation for using libraries like pandas and dplyr. But SQL can actually do a surprising amount of things, as database expert Haki Benita explained in a detailed article.\nOr in short, from the documentation of ibis:\n\nSQL is widely used and very convenient when writing simple queries. But as the complexity of operations grow, SQL can become very difficult to deal with.\n\nThen, there‚Äôs the issue of how to actually write the SQL code. Writing strings rather than actual Python is awkward and many editors don‚Äôt provide syntax highlighting within the strings (Jetbrains editors like PyCharm and DataSpell do). The other option is writing .sql that have placeholders for parameters. That‚Äôs cleaner and allows using a linter, but is inconvenient for interactive use.\nSQL is inherently lazily executed, because the query planner needs to take the whole query into account before starting computation. This enables performance gains. For interactive use, lazy evaluation is less convenient, because one can‚Äôt see the intermediate results at each step. Speed of iteration is critical: the faster one can iterate, the more hypotheses about the data can be tested.\nThere is a programmatic way to construct queries for duckdb, designed to provide a dbplyr alternative in Python. Unfortunately its documentation is sparse.\nUsing duckdb without pandas doesn‚Äôt seem feasible for exploratory data analysis, because graphing packages like seaborn and plotly expect a pandas data frame or similar as an input."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#ibis-lingua-franca-in-python",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#ibis-lingua-franca-in-python",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "ibis: Lingua franca in Python",
    "text": "ibis: Lingua franca in Python\nThe goal of ibis is to provide a universal language for working with data frames in Python, regardless of the backend that is used. It‚Äôs tagline is: Write your analytics code once, run in everywhere. This is similar to how dplyr can use SQL as a backend with dbplyr and data.table with dtplyr.\nAmong others, Ibis supports pandas, PostgreSQL and SQLite as backends. Unfortunately duckdb is not an available backend, because the authors of duckdb have decided against building on ibis.\nThe ibis project aims to bridge the gap between the needs of interactive data analysis and the capabilities of SQL, which I have detailed in the previous section on duckdb.\n\n\n\n\n\n\nNote\n\n\n\nUPDATE October 2023\n\nDuckdb is now a supported backend (along with many more). So performance is going to be very similar to duckdb.\nDirectly load/save data\n\njoin(), clip(), and case() are well-supported\nIbis is much more popular and now very actively maintained. There are more examples, better documentation, and community. Still definitely less than pandas, but perhaps comparable to polars.\n\nThanks to NickCrews for providing this update, including the following code example.\n\n\nFor the test drive, I‚Äôll use the duckdb backend, meaning that the ibis code is translated to duckdb operations, similar to how siuba is translated to pandas. This gives ibis the blazing speed of duckdb.\n\nimport ibis\nfrom ibis import _\n\nflights_ib_csv = pd.read_csv(flights_path)\nairlines_ib_csv = pd.read_csv(airlines_path)\n\nibis.options.interactive = True\n\nflights_ib = ibis.read_csv(flights_path)\nairlines_ib = ibis.read_csv(airlines_path)\nflights_ib\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îê\n‚îÇ year  ‚îÇ month ‚îÇ day   ‚îÇ dep_time ‚îÇ sched_dep_time ‚îÇ dep_delay ‚îÇ arr_time ‚îÇ  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚î§\n‚îÇ int64 ‚îÇ int64 ‚îÇ int64 ‚îÇ int64    ‚îÇ int64          ‚îÇ int64     ‚îÇ int64    ‚îÇ  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚î§\n‚îÇ  2013 ‚îÇ     1 ‚îÇ     1 ‚îÇ      517 ‚îÇ            515 ‚îÇ         2 ‚îÇ      830 ‚îÇ  ‚îÇ\n‚îÇ  2013 ‚îÇ     1 ‚îÇ     1 ‚îÇ      533 ‚îÇ            529 ‚îÇ         4 ‚îÇ      850 ‚îÇ  ‚îÇ\n‚îÇ  2013 ‚îÇ     1 ‚îÇ     1 ‚îÇ      542 ‚îÇ            540 ‚îÇ         2 ‚îÇ      923 ‚îÇ  ‚îÇ\n‚îÇ  2013 ‚îÇ     1 ‚îÇ     1 ‚îÇ      544 ‚îÇ            545 ‚îÇ        -1 ‚îÇ     1004 ‚îÇ  ‚îÇ\n‚îÇ  2013 ‚îÇ     1 ‚îÇ     1 ‚îÇ      554 ‚îÇ            600 ‚îÇ        -6 ‚îÇ      812 ‚îÇ  ‚îÇ\n‚îÇ  2013 ‚îÇ     1 ‚îÇ     1 ‚îÇ      554 ‚îÇ            558 ‚îÇ        -4 ‚îÇ      740 ‚îÇ  ‚îÇ\n‚îÇ  2013 ‚îÇ     1 ‚îÇ     1 ‚îÇ      555 ‚îÇ            600 ‚îÇ        -5 ‚îÇ      913 ‚îÇ  ‚îÇ\n‚îÇ  2013 ‚îÇ     1 ‚îÇ     1 ‚îÇ      557 ‚îÇ            600 ‚îÇ        -3 ‚îÇ      709 ‚îÇ  ‚îÇ\n‚îÇ  2013 ‚îÇ     1 ‚îÇ     1 ‚îÇ      557 ‚îÇ            600 ‚îÇ        -3 ‚îÇ      838 ‚îÇ  ‚îÇ\n‚îÇ  2013 ‚îÇ     1 ‚îÇ     1 ‚îÇ      558 ‚îÇ            600 ‚îÇ        -2 ‚îÇ      753 ‚îÇ  ‚îÇ\n‚îÇ     ‚Ä¶ ‚îÇ     ‚Ä¶ ‚îÇ     ‚Ä¶ ‚îÇ        ‚Ä¶ ‚îÇ              ‚Ä¶ ‚îÇ         ‚Ä¶ ‚îÇ        ‚Ä¶ ‚îÇ  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îò\n\n\nNon-interactive ibis means that queries are evaluated lazily.\n\n(\n    flights_ib.filter(\n        [\n            _.year == 2013,\n            _.month == 1,\n            _.arr_delay.notnull(),\n        ]\n    )\n    .join(airlines_ib, \"carrier\", how=\"left\")\n    .select(arr_delay=_.arr_delay.clip(lower=0), airline=_.name)\n    .group_by(\"airline\")\n    .agg(flights=_.count(), mean_delay=_.arr_delay.mean())\n    .order_by(_.mean_delay.desc())\n)\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ airline                  ‚îÇ flights ‚îÇ mean_delay ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ string                   ‚îÇ int64   ‚îÇ float64    ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ SkyWest Airlines Inc.    ‚îÇ       1 ‚îÇ 107.000000 ‚îÇ\n‚îÇ Hawaiian Airlines Inc.   ‚îÇ      31 ‚îÇ  48.774194 ‚îÇ\n‚îÇ ExpressJet Airlines Inc. ‚îÇ    3964 ‚îÇ  29.642785 ‚îÇ\n‚îÇ Frontier Airlines Inc.   ‚îÇ      59 ‚îÇ  23.881356 ‚îÇ\n‚îÇ Mesa Airlines Inc.       ‚îÇ      39 ‚îÇ  20.410256 ‚îÇ\n‚îÇ Endeavor Air Inc.        ‚îÇ    1480 ‚îÇ  19.321622 ‚îÇ\n‚îÇ Alaska Airlines Inc.     ‚îÇ      62 ‚îÇ  17.645161 ‚îÇ\n‚îÇ Envoy Air                ‚îÇ    2203 ‚îÇ  14.303677 ‚îÇ\n‚îÇ Southwest Airlines Co.   ‚îÇ     985 ‚îÇ  12.964467 ‚îÇ\n‚îÇ JetBlue Airways          ‚îÇ    4413 ‚îÇ  12.919329 ‚îÇ\n‚îÇ ‚Ä¶                        ‚îÇ       ‚Ä¶ ‚îÇ          ‚Ä¶ ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\nThe syntax looks quite similar to dplyr and the versatility of interchangeable backends is remarkable. In the first version of this article, ibis was lacking in documentation and had some rough edges in the API, but these were improved in the meantime."
  },
  {
    "objectID": "blog/2024-11-01_dplyr_candidate/index.html#conclusion",
    "href": "blog/2024-11-01_dplyr_candidate/index.html#conclusion",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Conclusion",
    "text": "Conclusion\nIt‚Äôs not a clear-cut choice. None of the options offer a syntax that is as convenient for interactive analysis as dplyr. siuba is the closest to it, but dplyr still has an edge with tidy evaluation, letting users refer to columns in a data frame by their names (colname) directly, without any wrappers. But I‚Äôve also seen it be confusing for newbies to R that mix it up with base R‚Äôs syntax. It‚Äôs also harder to program with, where it‚Äôs necessary to use operators like { } and :=.\nMy appreciation for dplyr (and the closely associated tidyr) grew during this research. Not only is it a widely accepted standard like pandas, it can also be used as a translation layer for backends like SQL databases (including duckdb), data.table, and Spark. All while having the most elegant and flexible syntax available.\nPersonally, I‚Äôll primarily leverage SQL and a OLAP database (such as Clickhouse or Snowflake) running on a server to do the heavy lifting. For steps that are better done locally, I‚Äôll use pandas for maximum compatibility. I find the use of an index inconvenient, but there‚Äôs so much online help available on StackOverflow. Github Copilot also deserves a mention for making it easier to pick up. Other use cases can be very different, so I don‚Äôt mean to say that my way is the best. For instance, if the data is not already on a server, fast local processing with polars may be best.\nMost data science work happens in a team. Choosing a library that all team members are familiar with is critical for collaboration. That is typically SQL, pandas or dplyr. The performance gains from using a less common library like polars have to be weighed against the effort spent learning the syntax as well as the increased likelihood of bugs, when beginners write in a new syntax.\nRelated articles:\n\nPolars: the fastest DataFrame library you‚Äôve never heard of\nWhat would it take to recreate dplyr in python?\nPandas has a hard job (and does it well)\ndplyr in Python? First impressions of the siuba module\nAn Overview of Python‚Äôs Datatable package\nDiscussion of DuckDB on Hacker News\nDiscussion of Polars on Hacker News\nPractical SQL for Data Analysis"
  },
  {
    "objectID": "blog/2024-11-28_transition_to_python/index.html",
    "href": "blog/2024-11-28_transition_to_python/index.html",
    "title": "Switching from R to Python: A Beginner‚Äôs Guide to Equivalent Tools",
    "section": "",
    "text": "Transitioning from R to Python can feel like a daunting leap, especially if you‚Äôve grown comfortable with R‚Äôs ecosystem. The good news? Python offers several tools and libraries that mimic the syntax and functionality of your favorite R packages. Let‚Äôs explore these equivalents to ease your journey."
  },
  {
    "objectID": "blog/2024-11-28_transition_to_python/index.html#data-manipulation-ibis",
    "href": "blog/2024-11-28_transition_to_python/index.html#data-manipulation-ibis",
    "title": "Switching from R to Python: A Beginner‚Äôs Guide to Equivalent Tools",
    "section": "2.1 Data Manipulation: ibis",
    "text": "2.1 Data Manipulation: ibis\nIn R, dplyr and dbplyr are go-to packages for data manipulation, offering a clean, declarative syntax to filter, mutate, summarize, and join datasets. Python‚Äôs ibis serves as an excellent alternative, providing a similar experience for working with structured data.\nWhat sets ibis apart is its performance optimization. It abstracts SQL-like operations and enables you to specify a backend engine, such as DuckDB, Polars, or Pandas. This allows for efficient in-memory data processing or seamless database querying without switching languages. Whether you‚Äôre dealing with small data or large-scale analytics, ibis scales beautifully.\nTo get started, explore this dplyr-to-ibis tutorial, which maps your familiar R syntax to ibis equivalents."
  },
  {
    "objectID": "blog/2024-11-28_transition_to_python/index.html#data-visualization-plotnine",
    "href": "blog/2024-11-28_transition_to_python/index.html#data-visualization-plotnine",
    "title": "Switching from R to Python: A Beginner‚Äôs Guide to Equivalent Tools",
    "section": "2.2 Data Visualization: plotnine",
    "text": "2.2 Data Visualization: plotnine\nggplot2 is beloved in the R community for its intuitive grammar of graphics, enabling users to create complex, layered visualizations with minimal effort. If you‚Äôve relied on ggplot2 for your data storytelling, Python‚Äôs plotnine is your best friend.\nplotnine mirrors ggplot2‚Äôs syntax almost exactly. It supports layering plots with +, theming options, faceting for multi-panel plots, and customization of aesthetics. As a bonus, Python‚Äôs ecosystem integrates well with other visualization libraries, such as Matplotlib and Seaborn, for additional flexibility.\nDive into these plotnine tutorials to recreate your favorite ggplot2 visualizations in Python."
  },
  {
    "objectID": "blog/2024-11-28_transition_to_python/index.html#interactive-web-apps-shiny-for-python",
    "href": "blog/2024-11-28_transition_to_python/index.html#interactive-web-apps-shiny-for-python",
    "title": "Switching from R to Python: A Beginner‚Äôs Guide to Equivalent Tools",
    "section": "2.3 Interactive Web Apps: Shiny for Python",
    "text": "2.3 Interactive Web Apps: Shiny for Python\nShiny revolutionized how R users build interactive web applications with minimal code. The good news is that Shiny for Python brings the same simplicity to Python, letting you create interactive dashboards, data visualizations, and applications to showcase your work.\nShiny for Python follows a reactive programming paradigm, where outputs automatically update when inputs change. With Python‚Äôs robust backend options and Shiny‚Äôs UI capabilities, you can build powerful applications for both internal and external stakeholders. Whether you‚Äôre demonstrating a machine learning model or building a tool for non-technical audiences, Shiny has you covered.\nCheck out this Shiny for Python guide to start building your first app."
  },
  {
    "objectID": "blog/2024-11-28_transition_to_python/index.html#deploy-machine-learning-models-vetiver",
    "href": "blog/2024-11-28_transition_to_python/index.html#deploy-machine-learning-models-vetiver",
    "title": "Switching from R to Python: A Beginner‚Äôs Guide to Equivalent Tools",
    "section": "2.4 Deploy Machine Learning Models: Vetiver",
    "text": "2.4 Deploy Machine Learning Models: Vetiver\nDeploying machine learning models can often be complex and time-consuming. R‚Äôs vetiver package streamlines this process by creating APIs for your models, and Vetiver for Python does the same, making deployment accessible and consistent.\nWith Vetiver, you can deploy models built using scikit-learn, TensorFlow, PyTorch, or even custom algorithms. It generates prediction endpoints with minimal setup, allowing you to integrate your models into web applications, APIs, or automation workflows. This simplifies the journey from model development to production.\nLearn more about deploying models with vetiver in this comprehensive tutorial."
  },
  {
    "objectID": "blog/2024-11-28_transition_to_python/index.html#publishing-reports-quarto",
    "href": "blog/2024-11-28_transition_to_python/index.html#publishing-reports-quarto",
    "title": "Switching from R to Python: A Beginner‚Äôs Guide to Equivalent Tools",
    "section": "2.5 Publishing Reports: Quarto",
    "text": "2.5 Publishing Reports: Quarto\nR Markdown users transitioning to Python will be delighted to know that Quarto supports Python as well. Quarto extends the capabilities of R Markdown, enabling you to create HTML, PDF, and Word reports seamlessly. It even allows mixing code from Python, R, and Julia in the same document.\nQuarto offers numerous customization options, such as beautiful themes and dynamic content embedding, making it a versatile tool for technical reports, academic papers, and blog posts. As your projects grow, you can also use Quarto for building websites or books.\nExplore how to use Python with Quarto in this getting started guide."
  },
  {
    "objectID": "blog/2024-11-28_transition_to_python/index.html#an-ide-for-both-worlds-positron",
    "href": "blog/2024-11-28_transition_to_python/index.html#an-ide-for-both-worlds-positron",
    "title": "Switching from R to Python: A Beginner‚Äôs Guide to Equivalent Tools",
    "section": "2.6 An IDE for Both Worlds: Positron",
    "text": "2.6 An IDE for Both Worlds: Positron\nMany R users prefer RStudio for its clean, feature-rich interface. Fortunately, Positron, developed by Posit (formerly RStudio), provides a similar experience for Python. This integrated development environment (IDE) supports both R and Python, making it perfect for multi-language projects.\nWith Positron, you can enjoy a consistent environment for coding, debugging, and project management. Its features include a robust editor, version control integration, and support for Quarto documents. Download Positron from its GitHub repository and see how it complements your Python workflow."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "The Data Alchemist",
    "section": "",
    "text": "‚ÄúWelcome, adventurer! I‚Äôm thrilled you‚Äôve joined me on this journey through the world of data science and code. Here, I‚Äôll share the discoveries I‚Äôm making, the projects I‚Äôm building, and the curiosities I‚Äôm exploring‚Äîsprinkled with a little feline charm along the way. Let‚Äôs learn and uncover insights together!‚Äù\n\n\n\n\n\n\n\n\n\n\n\n\n\nPositron - a VSCode fork for Data Science\n\n\n\nPython\n\nR\n\nIDE\n\nTool reviews\n\n\n\nAt the end of June, the public beta version of Positron was released. That‚Äôs almost 6 months ago, and the Positron team certainly hasn‚Äôt been idle! So what happened over the last half year? And is it worth switching? üëÄ Some of my personal highlights about this IDE üëáüèª\n\n\n\n\n\nDec 4, 2024\n\n5 min\n\n\n\n\n\n\n\n\n\nSwitching from R to Python: A Beginner‚Äôs Guide to Equivalent Tools\n\n\n\nPython\n\nR\n\nData Science\n\n\n\nLearn how to transition from R to Python seamlessly with tools that match your R favorites like dplyr, ggplot2, Shiny, and more.\n\n\n\n\n\nNov 28, 2024\n\n5 min\n\n\n\n\n\n\n\n\n\nChoosing a Python dataframe library as a dplyR useR\n\n\n\nR\n\nPython\n\n\n\n\n\n\n\n\n\nJan 25, 2023\n\n17 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Luong Nguyen Thanh",
    "section": "",
    "text": "GitHub\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Research Profile\n  \n\n\n\n\n\n         \n\n\nHi there!\nWelcome to my website ‚Äî I‚Äôm glad you‚Äôre here!\nI‚Äôm currently a PhD student at Uppsala University, following an academic path rooted in health and sustainability science, with and One Health/Global Health (MSc).\nBeyond academia, I am passionate about programming, especially in R, with experience building packages, dashboards, Shiny apps, and working with large language models. My technical skills also extend to intermediate Python and foundational knowledge of SQL, HTML, CSS, and JavaScript. I‚Äôm especially interested in reproducibility and open science, and finding ways to streamline workflow.\nMy guiding principles in life are kindness, discipline, and integrity. I have found genuine fulfillment in working within the health and sustainability data science field, where I have the opportunity to engage with an inspiring community of like-minded professionals. I am also cautiously optimistic about AI. With a keen eye on the interplay between human actions and technological advancements, I aim to understand and leverage this relationship for societal and health betterment, seeking to uncover novel ways in which technology can be harnessed to shape positive health outcomes.\nIn my spare time, I love exploring history and diving into subjects related to religion and culture.\n\n\n\nGitHub Streak\n\n\n\n\n\n  \n\n\n  \n\n\n\nNote: Top languages is only a metric of the languages my public code consists of and doesn‚Äôt reflect experience or skill level."
  },
  {
    "objectID": "projects/amr_sdg.html",
    "href": "projects/amr_sdg.html",
    "title": "AMR-SDGs interactions",
    "section": "",
    "text": "App  Code\nThe ShinyApp is the supplementary materials for the article:\n‚ÄúWhen global health meets global goals‚Äô: assessing the alignment between antimicrobial resistance and sustainable development policies in 10 African and Asian countries. BMJ Global Health\nLuong Nguyen-Thanh1,2,4, Didier Wernli3, Mats M√•lqvist1, Peter S√∏gaard J√∏rgensen1,4,5\nAffiliations:\n1 SWEDESD ‚Äì Sustainability Learning and Research Center, Department of Women‚Äôs and Children‚Äôs Health, Uppsala University, Uppsala, Sweden\n2 Uppsala Antibiotic Centre (UAC), Uppsala University\n3 Global Studies Institute and Faculty of Science, University of Geneva, Geneva, Switzerland\n4 Global Economic Dynamics and the Biosphere, Royal Swedish Academy of Sciences, Stockholm, Sweden\n5 Stockholm Resilience Centre, Stockholm University, Stockholm, Sweden\n\n\n\nScreenshot from the App"
  },
  {
    "objectID": "projects/hackathon.html",
    "href": "projects/hackathon.html",
    "title": "Sahlgrenska Global Health Hackathon",
    "section": "",
    "text": "Hackathon website  App Demo\n\nPROBLEM STATEMENT:\nChronic diseases place a significant burden on global health, affecting millions and overwhelming healthcare systems. Effective management of these conditions requires ongoing care, much of which often occurs outside healthcare settings with a focus on preventing complications and managing risk factors. Early intervention and continuous monitoring are essential to improving outcomes, especially in cases where diseases can worsen unpredictably. Epilepsy serves as a prime example of such a condition, illustrating the challenges faced in managing chronic illnesses.\nEpilepsy affects approximately 5 million people globally each year (WHO, 2024) and is one of the most common neurological disorders. With the right treatment, up to 70% of people with epilepsy can live seizure-free, yet the unpredictable nature of seizures creates ongoing challenges. Seizures can strike suddenly, causing immediate physical harm and long-term psychological stress for both patients and caregivers. Managing epilepsy requires a personalized approach, where treatment is adjusted to the unique needs of each patient, ensuring that care remains effective over time.\nHowever, the management of epilepsy is hindered by several key obstacles. Fragmented data from clinical records, wearable devices, and lifestyle tracking complicate the ability to monitor patients effectively. Seizures, by their very nature, are unpredictable and often occur without warning, heightening anxiety and making it difficult to manage the disease in real-time. Medication non-adherence, a common issue in chronic disease management, leads to inconsistent control and increased risk of complications. Furthermore, healthcare providers also face challenges in delivering personalized care, with increasing workloads and time constraints making it difficult to offer the necessary attention to each patient.\nBy overcoming data fragmentation, improving personalized care, and leveraging technology, we can significantly enhance the management of epilepsy and other chronic conditions.\n\n\n\nINNOVATION AT A GLANCE\n\nObjective: To create a one-stop platform for chronic disease management, starting with epilepsy, that empowers patients to monitor and manage their condition outside medical settings. The platform aims to reduce the burden on patients and caregivers while enabling healthcare providers to deliver more personalized and effective interventions. Success in its initial implementation will pave the way for the platform to support more neurological disorders and other common chronic diseases.\nTarget Audience: Patients and Caregivers: Offering free access to monitoring tools and full control over their data. Healthcare Providers: Equipping neurologists, epilepsy specialists, and hospitals with predictive analytics and data-driven insights.\nKey Features:\n\n\nData Aggregation From Clinical, Behavioral, and Wearable Sources: By unifying data from multiple sources, this feature eliminates fragmentation, creating a comprehensive platform for monitoring and management. Enhanced by AI, it streamlines the continuous collection of data while ensuring researchers have access to high-quality, anonymized datasets, enabling advancements in treatment and understanding. Most importantly, patients maintain full control over their data.\nAI-Powered Prediction of Seizures and Critical Events: The platform uses collected data to issue reliable alerts (with confidence level) about the potential occurrence of seizures within a specified time frame. This feature addresses the unpredictability of seizures, helping patients and caregivers anticipate and prepare for critical episodes. It also aids in diagnosing epilepsy by identifying patterns in diverse symptoms, ensuring more accurate and timely interventions.\nPersonalized Insights Based on Predictive Analytics: By leveraging AI-driven predictive analytics, this feature provides healthcare providers with systematic insights into patient data, enabling them to optimize and personalize treatment plans for epilepsy patients. By presenting a comprehensive view of a patient‚Äôs condition, the platform empowers doctors to make informed decisions and design tailored therapeutic approaches.\nMedication Adherence Tools (Reminders and Tracking): These tools promote consistent medication use, directly addressing the issue of non-adherence. By ensuring patients follow prescribed treatments, this feature contributes to better health outcomes.\nRemote Patient Management:: Through AI-driven platforms, healthcare providers can monitor epilepsy patients remotely. This not only reduces the burden of frequent in-person visits but also improves access to care for patients in remote or underserved areas\n\n\n\nHealth Focus:\n\n\nEnhancing health monitoring and management outside of medical settings.\nImproving chronic diseases‚Äô early diagnosis and personalized treatment, therefore improving health outcomes and quality of life for patients\n\n\nTechnology Focus:\n\n\nA Multi-Agent AI Model, a cutting-edge system designed to revolutionize chronic disease management, starting with epilepsy. This model coordinates multiple specialized AI agents, each tailored to perform critical tasks such as seizure prediction, real-time monitoring, medication management, reporting and warning, and patient education.\nIoT devices for patient health monitoring.\nCloud-based infrastructure to ensure scalability and sustainability.\n\n\n\nADDED VALUES FOR CHRONIC DISEASE MANAGEMENT\n\nIntegration Across Diverse Data Sources: Our platform consolidates data from clinical records, IoT devices, lifestyle metrics, and genomics to create a comprehensive health profile. This integration enables more accurate predictions and a deeper understanding of chronic disease management.\nPatient-Centric Data Privacy and Control: Our platform gives patients full ownership of their data, with granular control over what is shared and for what purpose.\nSeamless Integration with Healthcare Systems: Our platform integrates effortlessly with Electronic Health Records (EHRs), streamlining workflows and reducing administrative burdens for healthcare providers. This ensures a unified, up-to-date view of patient data, enabling more efficient, coordinated, and personalized care without disrupting existing processes."
  },
  {
    "objectID": "talks/2024-02-02_globelife_02/index.html",
    "href": "talks/2024-02-02_globelife_02/index.html",
    "title": "GlobalLife Annual Networking Event",
    "section": "",
    "text": "Recording\n\nDetails\nüìÜ February 02, 2024 // 5:00 pm - 8:00 pm CET\nüè® Virtual\nüå† GlobeLife\n\n\nAbstract\nOn Friday, 2/2/2024! The GlobeLife Student Section is thrilled to announce our inaugural ‚ÄúAnnual Networking Event,‚Äù fostering connections between alumni and current MSc and PhD students in Global Health from Uppsala University and Karolinska Institutet.\nü§ù Join us for an evening of networking, where you can meet like-minded individuals and expand your professional network within the Global Health community.\nüìÖ Date: Friday, 2/2/2024 üìç Location: KI Solna campus üö® Limited seats available ‚Äì first come, first served! We‚Äôre also providing transportation support between Uppsala University and Karolinska Institute.\nInvited speakers are: * Abdel El Manira, The Nobel committee for Physiology or Medicine * Eva Garmendia, Communication Expert and Project Coordinator, Uppsala Antibiotic Center.\nüîó Ready to secure your spot? Register now by scanning the QR code in the flyer below or by clicking on this link: https://lnkd.in/dS9nf_4G\n\n\nScreenshot\n\n\n\n\n\nCitationBibTeX citation:@online{nguyen_thanh2024,\n  author = {Nguyen Thanh, Luong},\n  title = {GlobalLife {Annual} {Networking} {Event}},\n  date = {2024-02-02},\n  url = {https://ntluong95.github.io/profile/talks/2024-02-02_globelife_02/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nNguyen Thanh, Luong. 2024. ‚ÄúGlobalLife Annual Networking\nEvent.‚Äù February 2, 2024. https://ntluong95.github.io/profile/talks/2024-02-02_globelife_02/."
  },
  {
    "objectID": "talks/2024-12-05_globelife_04/index.html",
    "href": "talks/2024-12-05_globelife_04/index.html",
    "title": "Building your future career in Global Health",
    "section": "",
    "text": "Recording\n\nDetails\nüìÜ December 05, 2024 // 1:30 pm - 3:00 pm CET\nüè® Virtual\nüå† GlobeLife\n\n\nAbstract\nInternships are essential for gaining work experience and often a requirement for many jobs. Applying for competitive positions, like those at the WHO, can sometimes feel like a lottery or shooting an arrow into a black box, hoping to hit the target.\nJoin us to receive insider tips and learn about the value of internships from those who have successfully navigated this path.\n\n\nScreenshot\n\n\n\n\n\nCitationBibTeX citation:@online{nguyen_thanh2024,\n  author = {Nguyen Thanh, Luong},\n  title = {Building Your Future Career in {Global} {Health}},\n  date = {2024-12-05},\n  url = {https://ntluong95.github.io/profile/talks/2024-12-05_globelife_04/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nNguyen Thanh, Luong. 2024. ‚ÄúBuilding Your Future Career in Global\nHealth.‚Äù December 5, 2024. https://ntluong95.github.io/profile/talks/2024-12-05_globelife_04/."
  },
  {
    "objectID": "talks/2025-03-22_avise_seminar_02/index.html",
    "href": "talks/2025-03-22_avise_seminar_02/index.html",
    "title": "From musical practice to artistic reseaarch",
    "section": "",
    "text": "Recording\n\nDetails\nüìÜ March 22, 2023 // 2:00 pm - 3:00 pm CET\nüè® Virtual\nüå† The Association of Vietnamese Science and Technology Experts in Sweden\n\n\nAbstract\nXin ch√†o c√°c b·∫°n, AVISE xin h√¢n h·∫°nh gi·ªõi thi·ªáu Ch∆∞∆°ng tr√¨nh Seminar khoa h·ªçc th∆∞·ªùng k·ª≥. Seminar s·ªë 2 s·∫Ω ƒë∆∞·ª£c t·ªï ch·ª©c v√†o ng√†y Th·ª© b·∫£y 22/3/2025 v·ªõi ch·ªß ƒë·ªÅ ‚ÄúT·ª´ th·ª±c h√†nh √¢m nh·∫°c ƒë·∫øn nghi√™n c·ª©u ngh·ªá thu·∫≠t‚Äù (From musical practice to artistic research)\nDi·ªÖn gi·∫£ l·∫ßn n√†y, Ti·∫øn sƒ©, Ngh·ªá sƒ© ƒë√†n tranh Nguy·ªÖn Thanh Th·ªßy s·∫Ω chia s·∫ª qu√° tr√¨nh t·ª´ nh·ªØng c√¢u h·ªèi c√° nh√¢n trong th·ª±c h√†nh √¢m nh·∫°c d·∫´n ƒë·∫øn nh·ªØng th·ª≠ nghi·ªám trong nghi√™n c·ª©u ngh·ªá thu·∫≠t c·ªßa ch·ªã.\nƒê·ªÉ kh√¥ng b·ªè l·ª°, h√£y ƒëƒÉng k√Ω ngay t·∫°i link https://lnkd.in/d-ZatXaJ ho·∫∑c scan m√£ QR ·ªü poster ƒë√≠nh k√®m!\nN·∫øu b·∫°n mong mu·ªën c·∫≠p nh·∫≠t c√°c s·ª± ki·ªán ti·∫øp theo c·ªßa AVISE, ho·∫∑c quan t√¢m tr·ªü th√†nh th√†nh vi√™n ch√≠nh th·ª©c c·ªßa H·ªôi, h√£y theo d√µi ch√∫ng t√¥i tr√™n group LinkedIn n√†y ho·∫∑c Facebook t·∫°i ƒë∆∞·ªùng link https://lnkd.in/dB4hdh4a C·∫£m ∆°n c√°c b·∫°n ƒë√£ quan t√¢m. H·∫πn g·∫∑p c√°c b·∫°n v√†o ng√†y 22/3 t·ªõi! Tr√¢n tr·ªçng, AVISE\n\n\n\n\nCitationBibTeX citation:@online{nguyen_thanh2025,\n  author = {Nguyen Thanh, Luong},\n  title = {From Musical Practice to Artistic Reseaarch},\n  date = {2025-03-22},\n  url = {https://ntluong95.github.io/profile/talks/2025-03-22_avise_seminar_02/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nNguyen Thanh, Luong. 2025. ‚ÄúFrom Musical Practice to Artistic\nReseaarch.‚Äù March 22, 2025. https://ntluong95.github.io/profile/talks/2025-03-22_avise_seminar_02/."
  }
]