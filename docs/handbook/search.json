[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Health Researcher’s Roadmap: An A-Z Handbook for Career Success",
    "section": "",
    "text": "Preface\nWelcome to “Feature Engineering A-Z”! This book is written to be used as a reference guide to nearly all feature engineering methods you will encounter. This is reflected in the chapter structure. Any question a practitioner is having should be answered by looking at the index and finding the right chapter.\nEach section tries to be as comprehensive as possible with the number of different methods and solutions that are presented. A section on dimensionality reduction should list all the practical methods that could be used, as well as a comparison between the methods to help the reader decide what would be most appropriate. This does not mean that all methods are recommended to use. A number of these methods have little and narrow use cases. Methods that are deemed too domain-specific have been excluded from this book.\nEach chapter will cover a specific method or small group of methods. This will include motivations and explanations for the method. Whenever possible each method will be accompanied by mathematical formulas and visualizations to illustrate the mechanics of the method. A small pros and cons list is provided for each method. Lastly, each section will include code snippets showcasing how to implement the methods. This is done in R and Python, using tidymodels and scikit-learn respectively. This book is a methods book first, and a coding book second.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#preface-1",
    "href": "index.html#preface-1",
    "title": "The Health Researcher’s Roadmap: An A-Z Handbook for Career Success",
    "section": "Preface",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-does-this-book-not-cover",
    "href": "index.html#what-does-this-book-not-cover",
    "title": "The Health Researcher’s Roadmap: An A-Z Handbook for Career Success",
    "section": "What does this book not cover?",
    "text": "What does this book not cover?\nTo keep the scope of this book as focused as possible, the following topics will not be covered in this book:\n\nwhole process modeling\ncase studies\ndeployment details\ndomain-specific methods\n\nFor whole process modeling see instead “Hands-On Machine Learning with Scikit-learn, Keras & Tensorflow” (2017), “Tidy modeling with R” (2022), “Approaching (almost) any machine learning problem” (2020) and “Applied Predictive Modeling” (2013) are all great resources. For feature engineering books that tell more of a story by going through case studies, I recommended: “Python Feature Engineering Cookbook” (2020), “Feature Engineering Bookcamp” (2022) And “Feature Engineering and Selection” (2019). I have found that books on deployment domain-specific methods are highly related to the field and stack that you are using and am not able to give broad advice.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-is-this-book-for",
    "href": "index.html#who-is-this-book-for",
    "title": "The Health Researcher’s Roadmap: An A-Z Handbook for Career Success",
    "section": "Who is this book for?",
    "text": "Who is this book for?\nThis book is designed to be used by people involved in the modeling of data. These can include but are not limited to data scientists, students, professors, data analysts and machine learning engineers. The reference style nature of the book makes it useful for beginners and seasoned professionals. A background in the basics of modeling, statistics and machine learning would be helpful. Feature engineering as a practice is tightly connected to the rest of the machine learning pipeline so knowledge of the other components is key.\nMany educational resources skip over the finer details of feature engineering methods, which is where this book tries to fill the gap.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "The Health Researcher’s Roadmap: An A-Z Handbook for Career Success",
    "section": "License",
    "text": "License\nThis book is licensed to you under Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#rendering-details",
    "href": "index.html#rendering-details",
    "title": "The Health Researcher’s Roadmap: An A-Z Handbook for Career Success",
    "section": "Rendering details",
    "text": "Rendering details\nThis book is rendered using quarto (1.9.0), R (4.5.1) and Python (3.12.10). The website source is hosted on Github.\nThe following R packages are used to render the book, with tidymodels, recipes, embed, themis, textrecipes and timetk being the main packages.\nThe following Python libraries are used to render the book, with scikit-learn and feature-engine being the main ones.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#can-i-contribute",
    "href": "index.html#can-i-contribute",
    "title": "The Health Researcher’s Roadmap: An A-Z Handbook for Career Success",
    "section": "Can I contribute?",
    "text": "Can I contribute?\nPlease feel free to improve the quality of this content by submitting pull requests. A merged PR will make you appear in the contributor list. It will, however, be considered a donation of your work to this project. You are still bound by the conditions of the license, meaning that you are not considered an author, copyright holder, or owner of the content once it has been merged in.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "The Health Researcher’s Roadmap: An A-Z Handbook for Career Success",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI’m so thankful for the contributions, help, and perspectives of people who have supported us in this project. There are several I would like to thank in particular.\nI would like to thank my Posit colleagues on the tidymodels team (Hannah Frick, Max Kuhn, and Simon Couch) as well as the rest of our coworkers on the Posit open-source team. I also thank Javier Orraca-Deatcu, Matt Dancho and Mike Mahoney for looking over some of the chapters before the first release.\n\n\n\n\nGalli, S. 2020. Python Feature Engineering Cookbook: Over 70 Recipes for Creating, Engineering, and Transforming Features to Build Machine Learning Models. Packt Publishing. https://books.google.com/books?id=2c_LDwAAQBAJ.\n\n\nGéron, Aurélien. 2017. Hands-on Machine Learning with Scikit-Learn and TensorFlow : Concepts, Tools, and Techniques to Build Intelligent Systems. Sebastopol, CA: O’Reilly Media.\n\n\nKuhn, M., and K. Johnson. 2013. Applied Predictive Modeling. SpringerLink : Bücher. Springer New York. https://books.google.com/books?id=xYRDAAAAQBAJ.\n\n\n———. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. Chapman & Hall/CRC Data Science Series. CRC Press. https://books.google.com/books?id=q5alDwAAQBAJ.\n\n\nKuhn, M., and J. Silge. 2022. Tidy Modeling with r. O’Reilly Media. https://books.google.com/books?id=98J6EAAAQBAJ.\n\n\nOzdemir, S. 2022. Feature Engineering Bookcamp. Manning. https://books.google.com/books?id=3n6HEAAAQBAJ.\n\n\nThakur, A. 2020. Approaching (Almost) Any Machine Learning Problem. Amazon Digital Services LLC - Kdp. https://books.google.com/books?id=ZbgAEAAAQBAJ.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nIt is commonly said that feature engineering, much like machine learning, is an art rather than a science. The purpose of this book is to reinforce this perspective through the exploration of feature engineering tools and techniques. This foundational understanding will make encountering future methods less intimidating.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#how-to-deal-with",
    "href": "introduction.html#how-to-deal-with",
    "title": "Introduction",
    "section": "How to Deal With …",
    "text": "How to Deal With …\nThis book is structured according to the types of data and problems you will encounter. Each section specifies a type of data or problem, and each chapter details a method or group of methods that can be useful in dealing with that type. So for example the Numeric section contains methods that deal with numeric variables such as Logarithms and and Max Abs Scaling, and the Categorical section contains methods that deal with categorical variables such as Dummy Encoding and Hashing Encoding. There should be sections and chapters for most methods you will find in practice that aren’t too domain-specific.\nIt is because of this structure that this book is most suited as reference material, each time you encounter some data you are unsure how to deal with, you find the corresponding section and study the methods listed to see which would be best for your use case. This isn’t to say that you can’t read this book from end to end. The sections have been ordered roughly such that earlier chapters are broadly useful and later chapters touch on less used data types and problems.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#sec-modeling",
    "href": "introduction.html#sec-modeling",
    "title": "Introduction",
    "section": "Where does feature engineering fit into the modeling workflow?",
    "text": "Where does feature engineering fit into the modeling workflow?\nWhen we talk about the modeling workflow, it starts at the data source and ends with a fitted model. The fitted model in this instance should be created such that it can be used for the downstream task, be it inference or prediction.\n\n\n\n\n\n\nNoteInference and Prediction\n\n\n\nIn some data science organizations, the term “inference” is frequently used interchangeably with “prediction,” denoting the process of generating prediction outputs from a trained model. However, in this book, we will use “inference” specifically to refer to statistical inference, which involves drawing conclusions about populations or scientific truths from data analysis.\n\n\nWe want to make sure that the feature engineering methods we are applying are done correctly to avoid problems with the modeling. Things we especially want to avoid are data leakage, overfitting, and high computational cost.\n\n\n\n\n\n\nCautionTODO\n\n\n\nAdd diagram of modeling workflow from data source to model\n\n\nWhen applying feature engineering methods, we need to think about trained and untrained methods. Trained methods will perform a calculation doing the training of the method, and then using the extracted values to perform the transformation again. We see this in the Normalization chapter, where we explore centering and scaling. To implement centering, we adjust each variable by subtracting its mean value, computed using the training dataset. Since this value needs to be calculated, it becomes a trained method. Examples of untrained methods are logarithmic transformation and datetime value extraction. These methods are static in nature, meaning their execution can be applied at the observation-level without parameter-level inferences.\nIn practice, this means that untrained methods can be applied before the data-splitting procedure, as it would give the same results regardless of when it was done. Trained methods have to be performed after the data-splitting to ensure you don’t have data leakage. The wrinkle to this is that untrained methods applied to variables that have already been transformed by a trained method will have to also be done after the data-splitting.\n\n\n\n\n\n\nCautionTODO\n\n\n\nadd a diagram for untrained/trained rule\n\n\nSome untrained methods have a high computational cost, such as BERT. If you are unsure about when to apply a feature engineering method, a general rule of thumb that errs on the side of caution is to apply the method after the data-splitting procedure.\nIn the examples of this book, we will show how to perform methods and techniques using {recipes} on the R side, as they can be used together with the rest of tidymodels to make sure the calculations are done correctly. On the Python side, we show the methods by using transformers, that should then be used inside a sklearn.pipeline.Pipeline() to make sure the calculations are done correctly.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#why-do-we-use-thresholds",
    "href": "introduction.html#why-do-we-use-thresholds",
    "title": "Introduction",
    "section": "Why do we use thresholds?",
    "text": "Why do we use thresholds?\nOftentimes, when we use a method that selects something with a quantity, we end up doing it with a threshold instead of counting directly. The answer to this is purely practical, as it leaves less ambiguity. When selecting these features to keep in a feature selection routine as seen in the Too Many variables section is a good example. It is easier to write the code that selects every feature that has more than X amount of variability. On the other hand, if we said “Give me the 25 most useful features”, we might have 4 variables tied for 25th place. Now we have another problem. Does it keep all of them in, leaving 28 variables? If we do that, we violate our request of 25 variables. What if we select the first? Then we arbitrarily give a bias towards variables early in the data set. What if we randomly select among the ties? Then we introduce randomness into the method.\nIt is for the above reasons that many methods in feature engineering and machine learning use thresholds instead of precise numbers.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#sec-terminology",
    "href": "introduction.html#sec-terminology",
    "title": "Introduction",
    "section": "Terminology",
    "text": "Terminology\nBelow are some terminology clarifications since the term usage in this book may differ from other books. When a method is known by multiple names, the additional name(s) will be listed at the beginning of each chapter. The index will likewise point you to the right chapter regardless of which name you use.\n\nEDA\nOtherwise known as exploratory data analysis is the part of the modeling process where you look at the data very carefully. This will be done in part using descriptive statistics and visualization. This should be done after splitting the data, and only on the training data set. A project may be scrapped at this stage due to the limited usefulness of the data. Spending a lot of time in EDA is almost always fruitful as you gain insight into how to use and transform the data best with feature engineering methods.\n\n\nObservations\nThis book will mostly be working with rectangular data. In this context, each observation is defined as a row, with the columns holding the specific characteristics for each observation.\nThe observational unit can change depending on the data. Consider the following examples consisting of restaurants:\n\nIf we were looking at a data set of restaurant health code inspections, you are likely to see the data with one row per inspection\nIf your data set represented general business information about each restaurant, each observation may represent one unique restaurant\nIf you were a restaurant owner planning future schedules, you could think of each day/week as an observation\n\nReading this book will not tell you how to think about your data; You alone possess the subject matter expertise specific to your data set and problem statement. However, once your data is in the right format and order, we can expose you to possible feature engineering methods.\n\n\nLearned\nSome methods require information to be transformed that we are not able to supply beforehand. In the case of centering of numeric variables described in the normalization chapter, you need to know the mean value of the training data set to apply this transformation. This means is the sufficient information needed to perform the calculations and is the reason why the method is a learned method.\nIn contrast, taking the square root of a variable as described in the Square root chapter isn’t a learned method as there isn’t any sufficient information needed. The method can be applied immediately.\n\n\nSupervised / Unsupervised\nSome methods use the outcome to guide the calculations. If the outcome is used, the method is said to be supervised. Most methods are unsupervised.\n\n\nLevels\nVariables that contain non-numeric information are typically called qualitative or categorical variables. These can be things such as eye color, street names, names, grades, car models and subscription types. Where there is a finite known set of values a categorical variable can take, we call these values the levels of that variable. So the levels of the variables containing weekdays are “Monday”, “Tuesday”, “Wednesday”, “Thursday”, “Friday”, “Saturday”, and “Sunday”. But the names of our subscribers don’t have levels as we don’t know all of them.\nWe will sometimes bend this definition, as it is sometimes useful to pretend that a variable has a finite known set of values, even if it doesn’t.\n\n\nLinear models\nWe talk about linear models as models that are specified as a linear combination of features. These models tend to be simple, and fast to use, but having the limitation of “linear combination of features” means that they struggle when non-linear effects exist in the data set.\n\n\nEmbedding\nThe word embedding is frequently utilized in machine learning and artificial intelligence documentation, however, we will use it to refer to the numeric transformation of data point. We see this often in text embeddings, where a free-from-text field is turned into a fixed-length numerical vector.\nSomething being an embedding doesn’t mean that it is useful. However, with care and signal, useful representations of the data can be created. The reason why we have embeddings in the first place is that most machine learning models require numerical features for the models to work.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "pages/numeric.html",
    "href": "pages/numeric.html",
    "title": "1  Numeric Overview",
    "section": "",
    "text": "1.1 Numeric Overview\nData can come in all shapes and sizes, but the most common one is the numeric variable. These are values such as age, height, deficit, price, and quantity and we call these quantitative variables. They are plentiful in most data sets and immediately usable in all statistical and machine learning models. That being said, this does not imply that certain transformations wouldn’t improve model performance.\nThe following chapters will focus on different methods that follow 1-to-1 or 1-to-more transformations. These methods will mostly be applied one at a time to each variable. Methods that take many variables and return fewer variables such as dimensionality reduction methods will be covered in the Too Many variables section.\nWhen working with a single variable at a time, there are several problems we can encounter. Understanding the problem and how each method addresses it is key to getting the most out of numeric variables.\nThe four main types of problems we deal with when working with individual numeric variables are:",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Numeric Overview</span>"
    ]
  },
  {
    "objectID": "pages/numeric.html#distributional-problems",
    "href": "pages/numeric.html#distributional-problems",
    "title": "1  Numeric Overview",
    "section": "1.2 Distributional problems",
    "text": "1.2 Distributional problems\nTODO: add more explanation to this section\n\nlogarithm\nsqrt\nBoxCox\nYeo-Johnson\nPercentile",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Numeric Overview</span>"
    ]
  },
  {
    "objectID": "pages/numeric.html#sec-numeric-scaling-issues",
    "href": "pages/numeric.html#sec-numeric-scaling-issues",
    "title": "1  Numeric Overview",
    "section": "1.3 Scaling issues",
    "text": "1.3 Scaling issues\nThe topic of feature scaling is important and used widely in all of machine learning. This chapter will go over what feature scaling is and why we want to use it. The following chapters will each go over a different method of feature scaling.\n\n\n\n\n\n\nNote\n\n\n\nThere is some disagreement about the naming of these topics. These types of methods are called feature scaling and scaling in different fields. This book will call this general class of methods feature scaling and will make notes for each specific method and what other names they go by.\n\n\nIn this book, we will define feature scaling as an operation that modifies variables using multiplication and addition. While broadly defined, the methods typically reduce to the following form:\n\\[\nX_{scaled} = \\dfrac{X - a}{b}\n\\tag{1.1}\\]\nThe main difference between the methods is how \\(a\\) and \\(b\\) are calculated. These are learned transformation methods. We use the training data to derive the right values of \\(a\\) and \\(b\\), and then these values are used to perform the transformations when applied to new data. The different methods might differ on what property is desired for the transformed variables, same range or same spread, but they never change the distribution itself. The power transformations we saw in the Box-Cox and Yeo-Johnson chapters, distort the transformations, where these feature scalings essentially perform a “zooming” effect.\n\n\n\nTable 1.1: All feature scaling methods\n\n\n\n\n\n\n\n\n\nMethod\nDefinition\n\n\n\n\nCentering\n\\(X_{scaled} = X - \\text{mean}(X)\\)\n\n\nScaling\n\\(X_{scaled} = \\dfrac{X}{\\text{sd}(X)}\\)\n\n\nMax-Abs\n\\(X_{scaled} = \\dfrac{X}{\\text{max}(\\text{abs}(X))}\\)\n\n\nNormalization\n\\(X_{scaled} = \\dfrac{X - \\text{mean}(X)}{\\text{sd}(X)}\\)\n\n\nMin-Max\n\\(X_{scaled} = \\dfrac{X - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)}\\)\n\n\nRobust\n\\(X_{scaled} = \\dfrac{X - \\text{median}(X)}{\\text{Q3}(X) - \\text{Q1}(X)}\\)\n\n\n\n\n\n\nWe see here that all the methods in Table 1.1 follow Equation 1.1. Sometimes \\(a\\) and \\(b\\) take a value of 0, which is perfectly fine. Centering and scaling when used together is equal to normalization. They are kept separate in the table since they are sometimes used independently. Centering, scaling, and normalization will all be discussed in the Normalization chapter.\nThere are two main reasons why we want to perform feature scaling. Firstly, many different types of models take the magnitude of the variables into account when fitting the models, so having variables on different scales can be disadvantageous because some variables have high priorities. In turn, we get that the other variables have low priority. Models that work using Euclidean distances like KNN models are affected by this change. Regularized models such as lasso and ridge regression also need to be scaled since the regularization depends on the magnitude of the estimates. Secondly, some algorithms converge much faster when all the variables are on the same scale. These types of models produce the same fit, just at a slower pace than if you don’t scale the variables. Any algorithms using Gradient Descent fit into this category.\n\n\n\n\n\n\nCautionTODO\n\n\n\nHave a KNN diagram show why this is important List which types of models need feature scaling. Should be a 2 column list. Left=name, right=comment %in% c(no effect, different fit, slow down)",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Numeric Overview</span>"
    ]
  },
  {
    "objectID": "pages/numeric.html#non-linear-effect",
    "href": "pages/numeric.html#non-linear-effect",
    "title": "1  Numeric Overview",
    "section": "1.4 Non-linear effect",
    "text": "1.4 Non-linear effect\n\nbinning\nsplines\npolynomial\n\n\n\n\n\n\n\nCautionTODO\n\n\n\nShow different distributions, and how well the different methods do at dealing with them",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Numeric Overview</span>"
    ]
  },
  {
    "objectID": "pages/numeric.html#sec-numeric-outliers-issues",
    "href": "pages/numeric.html#sec-numeric-outliers-issues",
    "title": "1  Numeric Overview",
    "section": "1.5 Outliers",
    "text": "1.5 Outliers\n\n\n\n\n\n\nCautionTODO\n\n\n\nFill in some text here, and list issues\nadd chapters that can handle outliers",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Numeric Overview</span>"
    ]
  },
  {
    "objectID": "pages/numeric.html#other",
    "href": "pages/numeric.html#other",
    "title": "1  Numeric Overview",
    "section": "1.6 Other",
    "text": "1.6 Other\nThere are any number of transformations we can apply to numeric data, other functions include:\n\nhyperbolic\nRelu\ninverse\ninverse logit\nlogit",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Numeric Overview</span>"
    ]
  },
  {
    "objectID": "pages/summary.html",
    "href": "pages/summary.html",
    "title": "2  🏗️ Summary",
    "section": "",
    "text": "2.1 Summary\nWIP",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>🏗️ Summary</span>"
    ]
  },
  {
    "objectID": "pages/references.html",
    "href": "pages/references.html",
    "title": "References",
    "section": "",
    "text": "Galli, S. 2020. Python Feature Engineering Cookbook: Over 70 Recipes\nfor Creating, Engineering, and Transforming Features to Build Machine\nLearning Models. Packt Publishing. https://books.google.com/books?id=2c_LDwAAQBAJ.\n\n\nGéron, Aurélien. 2017. Hands-on Machine Learning with Scikit-Learn\nand TensorFlow : Concepts, Tools, and Techniques to Build Intelligent\nSystems. Sebastopol, CA: O’Reilly Media.\n\n\nKuhn, M., and K. Johnson. 2013. Applied Predictive Modeling.\nSpringerLink : Bücher. Springer New York. https://books.google.com/books?id=xYRDAAAAQBAJ.\n\n\n———. 2019. Feature Engineering and Selection: A Practical Approach\nfor Predictive Models. Chapman & Hall/CRC Data Science Series.\nCRC Press. https://books.google.com/books?id=q5alDwAAQBAJ.\n\n\nKuhn, M., and J. Silge. 2022. Tidy Modeling with r. O’Reilly\nMedia. https://books.google.com/books?id=98J6EAAAQBAJ.\n\n\nOzdemir, S. 2022. Feature Engineering Bookcamp. Manning. https://books.google.com/books?id=3n6HEAAAQBAJ.\n\n\nThakur, A. 2020. Approaching (Almost) Any Machine Learning\nProblem. Amazon Digital Services LLC - Kdp. https://books.google.com/books?id=ZbgAEAAAQBAJ.",
    "crumbs": [
      "References"
    ]
  }
]